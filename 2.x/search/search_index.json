{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Welcome \u00b6 Percona Monitoring and Management (PMM) is an open source database monitoring, management, and observability solution for MySQL, PostgreSQL, and MongoDB. It allows you to observe the health of your database systems, explore new patterns in their behavior, troubleshoot them and perform database management operations no matter where they are located on-prem or in the cloud. PMM collects thousands of out-of-the-box performance metrics from databases and their hosts. The PMM web UI visualizes data in dashboards . Additional features include advisors for database health assessments . This is the documentation for the latest release, PMM 2.31.0 ( Release Notes ). Here\u2019s how the home page looks on our free, live demo system . PMM runs in the cloud, on-prem, or across hybrid platforms. It\u2019s supported by our legendary expertise in open source databases, and by a vibrant developer and user community . A minimal PMM set-up comprises one server and a client agent on every system you want to monitor. Start here \u00b6 An easy install script, which you download, make executable and run. The script installs Docker and runs PMM Server as a container. The Quickstart install guide shows how to run PMM Server as a Docker container, and how to install PMM Client on Ubuntu or Red Hat Linux hosts. Setting Up explains in detail how to set up PMM Server, clients, and how to add services. Read more \u00b6 Links to popular sections \u00b6 For System Administrators \u00b6 Setting up How to configure How to upgrade pmm-admin Architecture For Users \u00b6 User interface Using Query Analytics Using Integrated Alerting Using DBaaS Dashboards reference Full section map (click to show/hide) Commands Dashboards API pmm-admin pmm-agent Architecture User Interface components VictoriaMetrics Glossary Details Render dashboard images Annotate Optimize Secure Upgrade Configure Manage Users Extend metrics Troubleshoot How to DBaaS Advisors Query Analytics Backup and Restore Integrated Alerting User Interface Using MySQL MongoDB PostgreSQL ProxySQL Amazon RDS Microsoft Azure Linux External Services HA Proxy Remote Instances Google Cloud Platform Client Server Network Docker Virtual appliance AWS Marketplace Easy-install script DBaaS Setting up Welcome","title":"Welcome"},{"location":"index.html#welcome","text":"Percona Monitoring and Management (PMM) is an open source database monitoring, management, and observability solution for MySQL, PostgreSQL, and MongoDB. It allows you to observe the health of your database systems, explore new patterns in their behavior, troubleshoot them and perform database management operations no matter where they are located on-prem or in the cloud. PMM collects thousands of out-of-the-box performance metrics from databases and their hosts. The PMM web UI visualizes data in dashboards . Additional features include advisors for database health assessments . This is the documentation for the latest release, PMM 2.31.0 ( Release Notes ). Here\u2019s how the home page looks on our free, live demo system . PMM runs in the cloud, on-prem, or across hybrid platforms. It\u2019s supported by our legendary expertise in open source databases, and by a vibrant developer and user community . A minimal PMM set-up comprises one server and a client agent on every system you want to monitor.","title":"Welcome"},{"location":"index.html#start-here","text":"An easy install script, which you download, make executable and run. The script installs Docker and runs PMM Server as a container. The Quickstart install guide shows how to run PMM Server as a Docker container, and how to install PMM Client on Ubuntu or Red Hat Linux hosts. Setting Up explains in detail how to set up PMM Server, clients, and how to add services.","title":"Start here"},{"location":"index.html#read-more","text":"","title":"Read more"},{"location":"index.html#links-to-popular-sections","text":"","title":"Links to popular sections"},{"location":"index.html#for-system-administrators","text":"Setting up How to configure How to upgrade pmm-admin Architecture","title":"For System Administrators"},{"location":"index.html#for-users","text":"User interface Using Query Analytics Using Integrated Alerting Using DBaaS Dashboards reference Full section map (click to show/hide) Commands Dashboards API pmm-admin pmm-agent Architecture User Interface components VictoriaMetrics Glossary Details Render dashboard images Annotate Optimize Secure Upgrade Configure Manage Users Extend metrics Troubleshoot How to DBaaS Advisors Query Analytics Backup and Restore Integrated Alerting User Interface Using MySQL MongoDB PostgreSQL ProxySQL Amazon RDS Microsoft Azure Linux External Services HA Proxy Remote Instances Google Cloud Platform Client Server Network Docker Virtual appliance AWS Marketplace Easy-install script DBaaS Setting up Welcome","title":"For Users"},{"location":"faq.html","text":"FAQ \u00b6 How can I contact the developers? \u00b6 Community forum . Discord chat . PMM project in JIRA . What are the minimum system requirements? \u00b6 Server: Disk: 1 GB per monitored database (1 week data retention) Memory: 2 GB per monitored database CPU: Supports SSE4.2 Client: Disk: 100 MB See also Setting up PMM Server Setting up PMM Client How can I upgrade from version 1? \u00b6 There is no direct software upgrade path. You must set up PMM 2 and connect your existing clients to it. When all data is registered in PMM2 and expired in PMM1, decommission your PMM1 instance. See also Upgrade from PMM1 Percona blog: Running PMM1 and PMM2 Clients on the Same Host How to control data retention? \u00b6 Go to Configuration \u2192 Settings \u2192 Advanced Settings \u2192 Data retention to adjust the value in days. See also Configure data retention How are PMM Server logs rotated? \u00b6 PMM Server embeds multiple components, like Victoria Metrics, Query Analytics, Grafana, managed , PostgreSQL, ClickHouse, etc. (components). All PMM Server component logs are rotated by supervisord . The components\u2019 log rotation settings are stored in *.ini files within the /etc/supervisord.d directory. Those settings define both the maximum size of a log file and the number of log files to keep. The log rotation takes place once the log file reaches its maximum size. What privileges are required to monitor a MySQL instance? \u00b6 SELECT , PROCESS , SUPER , REPLICATION CLIENT , RELOAD See also Setting Up/Client/MySQL . Can I monitor multiple service instances? \u00b6 Yes. You can add multiple instances of MySQL or any other service to be monitored from the same PMM Client. To do this, you provide a unique port and IP address, or a socket for each instance, and specify a unique name for each. (If a name is not provided, PMM uses the name of the PMM Client host.) For example, to add MySQL monitoring for two local MySQL servers: pmm-admin add mysql --username root --password root instance-01 127 .0.0.1:3001 pmm-admin add mysql --username root --password root instance-02 127 .0.0.1:3002 See also pmm-admin add mysql Can I rename instances? \u00b6 Yes, by removing and re-adding with a different name. When you remove a monitoring service, previously collected data remains available in Grafana. However, the metrics are tied to the instance name. So if you add the same instance back with a different name, it will be considered a new instance with a new set of metrics. So if you are re-adding an instance and want to keep its previous data, add it with the same name. Can I add an AWS RDS MySQL or Aurora MySQL instance from a non-default AWS partition? \u00b6 By default, the RDS discovery works with the default aws partition. But you can switch to special regions, like the GovCloud one, with the alternative AWS partitions (e.g. aws-us-gov ) adding them to the Settings via the PMM Server API . To specify other than the default value, or to use several, use the JSON Array syntax: [\"aws\", \"aws-cn\"] . What resolution is used for metrics? \u00b6 The default values (in seconds): Preset Low Medium High Rare 300 180 60 Standard 60 10 5 Frequent 30 5 1 Custom (defaults) 60 10 5 See also Metrics resolution How do I set up Alerting? \u00b6 When a monitored service metric reaches a defined threshold, PMM Server can trigger alerts for it using embedded Grafana Alerting functionality. For this, you must configure alerting rules that define conditions under which an alert should be triggered, and the contact points used to send the alert (e.g. email). Percona templated alerts enable you to create alerts based on built-in or custom templates to simplify the alert setup process. Grafana managed alerts allows attaching rules to your dashboard panel and enables you to create more sophisticated alerting rules. In addition, it can be easier to manage installations with a large number of hosts. This additional flexibility comes at the expense of simplicity. See also Grafana Alerting How do I use a custom Prometheus configuration file? \u00b6 Normally, PMM Server fully manages the Prometheus configuration file . However, some users may want to change the generated configuration to add additional scrape jobs, configure remote storage, etc. From version 2.4.0, when pmm-managed starts the Prometheus file generation process, it tries to load the /srv/prometheus/prometheus.base.yml file first, to use it as a base for the prometheus.yml file. The prometheus.yml file can be regenerated by restarting the PMM Server container, or by using the SetSettings API call with an empty body. See also API Percona blog: Extending PMM\u2019s Prometheus Configuration How to troubleshoot an Update? \u00b6 See Troubleshoot update . What are my login credentials when I try to connect to a Prometheus Exporter? \u00b6 User name: pmm Password: Agent ID PMM protects an exporter\u2019s output from unauthorized access by adding an authorization layer. To access an exporter you can use pmm as a user name and the Agent ID as a password. You can find the Agent ID corresponding to a given exporter by running pmm-admin list . See also pmm-admin list How to provision PMM Server with non-default admin password? \u00b6 Currently there is no API available to change the admin password. If you\u2019re deploying through Docker you can use the following code snippet to change the password after starting the Docker container: PMM_PASSWORD = \"mypassword\" echo \"Waiting for PMM to initialize to set password...\" until [ \"`docker inspect -f {{.State.Health.Status}} pmm-server`\" = \"healthy\" ] ; do sleep 1 ; done docker exec -t pmm-server bash -c \"grafana-cli --homepath /usr/share/grafana admin reset-admin-password $PMM_PASSWORD \" (This example assumes your Docker container is named pmm-server .) How to change the PMM password for a default admin user? \u00b6 If you\u2019re deploying through Docker you can change the default password for an admin user after starting the Docker container as follows: For PMM versions 2.27.0 and later: docker exec -t pmm-server change-admin-password <new_password> For PMM versions prior to 2.27.0: docker exec -t pmm-server bash -c 'grafana-cli --homepath /usr/share/grafana --configOverrides cfg:default.paths.data=/srv/grafana admin reset-admin-password newpass' How to use a non-default listen-port for pmm-admin? \u00b6 If you configure the PMM agent to use a non-default listen-port, for pmm-admin to communicate with the agent, use the global flag --pmm-agent-listen-port=LISTEN_PORT . --pmm-agent-listen-port = LISTEN_PORT Example: To use the listen-port 8000 pmm-admin --pmm-agent-listen-port = 8000 add postgresql --username = pmm-agent --password = pmm-agent-password --query-source = pgstatmonitor nameofpostgres If you are using OVF/AMI, you can change the default password through SSH by using the following command: change-admin-password <new_password>","title":"FAQ"},{"location":"faq.html#faq","text":"","title":"FAQ"},{"location":"faq.html#how-can-i-contact-the-developers","text":"Community forum . Discord chat . PMM project in JIRA .","title":"How can I contact the developers?"},{"location":"faq.html#what-are-the-minimum-system-requirements","text":"Server: Disk: 1 GB per monitored database (1 week data retention) Memory: 2 GB per monitored database CPU: Supports SSE4.2 Client: Disk: 100 MB See also Setting up PMM Server Setting up PMM Client","title":"What are the minimum system requirements?"},{"location":"faq.html#how-can-i-upgrade-from-version-1","text":"There is no direct software upgrade path. You must set up PMM 2 and connect your existing clients to it. When all data is registered in PMM2 and expired in PMM1, decommission your PMM1 instance. See also Upgrade from PMM1 Percona blog: Running PMM1 and PMM2 Clients on the Same Host","title":"How can I upgrade from version 1?"},{"location":"faq.html#retention","text":"Go to Configuration \u2192 Settings \u2192 Advanced Settings \u2192 Data retention to adjust the value in days. See also Configure data retention","title":"How to control data retention?"},{"location":"faq.html#how-are-pmm-server-logs-rotated","text":"PMM Server embeds multiple components, like Victoria Metrics, Query Analytics, Grafana, managed , PostgreSQL, ClickHouse, etc. (components). All PMM Server component logs are rotated by supervisord . The components\u2019 log rotation settings are stored in *.ini files within the /etc/supervisord.d directory. Those settings define both the maximum size of a log file and the number of log files to keep. The log rotation takes place once the log file reaches its maximum size.","title":"How are PMM Server logs rotated?"},{"location":"faq.html#what-privileges-are-required-to-monitor-a-mysql-instance","text":"SELECT , PROCESS , SUPER , REPLICATION CLIENT , RELOAD See also Setting Up/Client/MySQL .","title":"What privileges are required to monitor a MySQL instance?"},{"location":"faq.html#can-i-monitor-multiple-service-instances","text":"Yes. You can add multiple instances of MySQL or any other service to be monitored from the same PMM Client. To do this, you provide a unique port and IP address, or a socket for each instance, and specify a unique name for each. (If a name is not provided, PMM uses the name of the PMM Client host.) For example, to add MySQL monitoring for two local MySQL servers: pmm-admin add mysql --username root --password root instance-01 127 .0.0.1:3001 pmm-admin add mysql --username root --password root instance-02 127 .0.0.1:3002 See also pmm-admin add mysql","title":"Can I monitor multiple service instances?"},{"location":"faq.html#can-i-rename-instances","text":"Yes, by removing and re-adding with a different name. When you remove a monitoring service, previously collected data remains available in Grafana. However, the metrics are tied to the instance name. So if you add the same instance back with a different name, it will be considered a new instance with a new set of metrics. So if you are re-adding an instance and want to keep its previous data, add it with the same name.","title":"Can I rename instances?"},{"location":"faq.html#can-i-add-an-aws-rds-mysql-or-aurora-mysql-instance-from-a-non-default-aws-partition","text":"By default, the RDS discovery works with the default aws partition. But you can switch to special regions, like the GovCloud one, with the alternative AWS partitions (e.g. aws-us-gov ) adding them to the Settings via the PMM Server API . To specify other than the default value, or to use several, use the JSON Array syntax: [\"aws\", \"aws-cn\"] .","title":"Can I add an AWS RDS MySQL or Aurora MySQL instance from a non-default AWS partition?"},{"location":"faq.html#what-resolution-is-used-for-metrics","text":"The default values (in seconds): Preset Low Medium High Rare 300 180 60 Standard 60 10 5 Frequent 30 5 1 Custom (defaults) 60 10 5 See also Metrics resolution","title":"What resolution is used for metrics?"},{"location":"faq.html#how-do-i-set-up-alerting","text":"When a monitored service metric reaches a defined threshold, PMM Server can trigger alerts for it using embedded Grafana Alerting functionality. For this, you must configure alerting rules that define conditions under which an alert should be triggered, and the contact points used to send the alert (e.g. email). Percona templated alerts enable you to create alerts based on built-in or custom templates to simplify the alert setup process. Grafana managed alerts allows attaching rules to your dashboard panel and enables you to create more sophisticated alerting rules. In addition, it can be easier to manage installations with a large number of hosts. This additional flexibility comes at the expense of simplicity. See also Grafana Alerting","title":"How do I set up Alerting?"},{"location":"faq.html#how-do-i-use-a-custom-prometheus-configuration-file","text":"Normally, PMM Server fully manages the Prometheus configuration file . However, some users may want to change the generated configuration to add additional scrape jobs, configure remote storage, etc. From version 2.4.0, when pmm-managed starts the Prometheus file generation process, it tries to load the /srv/prometheus/prometheus.base.yml file first, to use it as a base for the prometheus.yml file. The prometheus.yml file can be regenerated by restarting the PMM Server container, or by using the SetSettings API call with an empty body. See also API Percona blog: Extending PMM\u2019s Prometheus Configuration","title":"How do I use a custom Prometheus configuration file?"},{"location":"faq.html#how-to-troubleshoot-an-update","text":"See Troubleshoot update .","title":"How to troubleshoot an Update?"},{"location":"faq.html#what-are-my-login-credentials-when-i-try-to-connect-to-a-prometheus-exporter","text":"User name: pmm Password: Agent ID PMM protects an exporter\u2019s output from unauthorized access by adding an authorization layer. To access an exporter you can use pmm as a user name and the Agent ID as a password. You can find the Agent ID corresponding to a given exporter by running pmm-admin list . See also pmm-admin list","title":"What are my login credentials when I try to connect to a Prometheus Exporter?"},{"location":"faq.html#how-to-provision-pmm-server-with-non-default-admin-password","text":"Currently there is no API available to change the admin password. If you\u2019re deploying through Docker you can use the following code snippet to change the password after starting the Docker container: PMM_PASSWORD = \"mypassword\" echo \"Waiting for PMM to initialize to set password...\" until [ \"`docker inspect -f {{.State.Health.Status}} pmm-server`\" = \"healthy\" ] ; do sleep 1 ; done docker exec -t pmm-server bash -c \"grafana-cli --homepath /usr/share/grafana admin reset-admin-password $PMM_PASSWORD \" (This example assumes your Docker container is named pmm-server .)","title":"How to provision PMM Server with non-default admin password?"},{"location":"faq.html#how-to-change-the-pmm-password-for-a-default-admin-user","text":"If you\u2019re deploying through Docker you can change the default password for an admin user after starting the Docker container as follows: For PMM versions 2.27.0 and later: docker exec -t pmm-server change-admin-password <new_password> For PMM versions prior to 2.27.0: docker exec -t pmm-server bash -c 'grafana-cli --homepath /usr/share/grafana --configOverrides cfg:default.paths.data=/srv/grafana admin reset-admin-password newpass'","title":"How to change the PMM password for a default admin user?"},{"location":"faq.html#how-to-use-a-non-default-listen-port-for-pmm-admin","text":"If you configure the PMM agent to use a non-default listen-port, for pmm-admin to communicate with the agent, use the global flag --pmm-agent-listen-port=LISTEN_PORT . --pmm-agent-listen-port = LISTEN_PORT Example: To use the listen-port 8000 pmm-admin --pmm-agent-listen-port = 8000 add postgresql --username = pmm-agent --password = pmm-agent-password --query-source = pgstatmonitor nameofpostgres If you are using OVF/AMI, you can change the default password through SSH by using the following command: change-admin-password <new_password>","title":"How to use a non-default listen-port for pmm-admin?"},{"location":"details/index.html","text":"Details \u00b6 Architecture : High-level architecture and main components. User interface components : Descriptions of the main menus and icons. PMM components and versions : PMM components and their version used in PMM. Developing Advisor checks : Database health assessments. Dashboards reference : A complete list of dashboards by category, with screenshots. Commands: pmm-admin : The manual page for the PMM administration tool. pmm-agent : The manual page for the PMM Client agent program. API : How to access the Swagger API. VictoriaMetrics : The monitoring solution and time-series database that replaced Prometheus in PMM 2.12.0 . ClickHouse : A third-party column-oriented database management system (DBMS) that facilitates the Query Analytics functionality. Glossary : A list of obscure terms and definitions.","title":"Details"},{"location":"details/index.html#details","text":"Architecture : High-level architecture and main components. User interface components : Descriptions of the main menus and icons. PMM components and versions : PMM components and their version used in PMM. Developing Advisor checks : Database health assessments. Dashboards reference : A complete list of dashboards by category, with screenshots. Commands: pmm-admin : The manual page for the PMM administration tool. pmm-agent : The manual page for the PMM Client agent program. API : How to access the Swagger API. VictoriaMetrics : The monitoring solution and time-series database that replaced Prometheus in PMM 2.12.0 . ClickHouse : A third-party column-oriented database management system (DBMS) that facilitates the Query Analytics functionality. Glossary : A list of obscure terms and definitions.","title":"Details"},{"location":"details/api.html","text":"API \u00b6 PMM Server lets you visually interact with API resources representing all objects within PMM. You can browse the API using the Swagger UI, accessible at the /swagger/ endpoint URL: Clicking an object lets you examine objects and execute requests on them: The objects visible are nodes, services, and agents: A Node represents a bare metal server, a virtual machine, a Docker container, or a more specific type such as an Amazon RDS Node. A node runs zero or more Services and Agents, and has zero or more Agents providing insights for it. A Service represents something useful running on the Node: Amazon Aurora MySQL, MySQL, MongoDB, etc. It runs on zero (Amazon Aurora Serverless), single (MySQL), or several (Percona XtraDB Cluster) Nodes. It also has zero or more Agents providing insights for it. An Agent represents something that runs on the Node which is not useful in itself but instead provides insights (metrics, query performance data, etc) about Nodes and/or Services. An agent always runs on the single Node (except External Exporters), and provides insights for zero or more Services and Nodes. Nodes, Services, and Agents have Types which define specific their properties, and their specific logic. Nodes and Services are external by nature \u2013 we do not manage them (create, destroy), but merely maintain a list of them (add to inventory, remove from inventory) in pmm-managed . Most Agents are started and stopped by pmm-agent . One exception is the External Exporter Type which is started externally. API Keys and authentication \u00b6 API keys are used to control access to the PMM server components and resources. With an API key, you are authenticated to the PMM server, have access to PMM server components and resources, and perform various actions on them. You can use API keys as a replacement for basic authentication. Generate API keys \u00b6 PMM uses the Grafana API keys for authentication. Following are the steps to generate the API keys: Login to PMM. From the side menu, click Configuration \u2192 API keys . On the Configuration page, click Add API Key . Add API key dialog box opens. Enter the following to generate an API key: key name (you can give any desired name) Select the Role from the dropdown Enter a value in the Time to live text box (hover on the tooltip for more information). Click Add. API Key Created window displays your newly created key. Make sure to copy your key and keep it secure. Authenticate \u00b6 You can authenticate your request using the HTTPS header. Important Use the -k or --insecure parameter to force cURL to ignore invalid and self-signed SSL certificate errors. The option will skip the SSL verification process, and you can bypass any SSL errors while still having SSL-encrypted communication. However, using the --insecure parameter is not recommended. Although the data transfer is encrypted, it is not entirely secure. For enhanced security of your PMM installation, you need valid SSL certificates. For information on validating SSL certificates, refer to: SSL certificates . curl -H \"Authorization: Bearer <api_key>\" https://127.0.0.1/v1/version Use an API key in basic auth \u00b6 You can pass the API key into a REST API call as a query parameter in the following format. Replace API_KEY with your API key. Example curl -X GET https://api_key:API_KEY@localhost/v1/version","title":"API"},{"location":"details/api.html#api","text":"PMM Server lets you visually interact with API resources representing all objects within PMM. You can browse the API using the Swagger UI, accessible at the /swagger/ endpoint URL: Clicking an object lets you examine objects and execute requests on them: The objects visible are nodes, services, and agents: A Node represents a bare metal server, a virtual machine, a Docker container, or a more specific type such as an Amazon RDS Node. A node runs zero or more Services and Agents, and has zero or more Agents providing insights for it. A Service represents something useful running on the Node: Amazon Aurora MySQL, MySQL, MongoDB, etc. It runs on zero (Amazon Aurora Serverless), single (MySQL), or several (Percona XtraDB Cluster) Nodes. It also has zero or more Agents providing insights for it. An Agent represents something that runs on the Node which is not useful in itself but instead provides insights (metrics, query performance data, etc) about Nodes and/or Services. An agent always runs on the single Node (except External Exporters), and provides insights for zero or more Services and Nodes. Nodes, Services, and Agents have Types which define specific their properties, and their specific logic. Nodes and Services are external by nature \u2013 we do not manage them (create, destroy), but merely maintain a list of them (add to inventory, remove from inventory) in pmm-managed . Most Agents are started and stopped by pmm-agent . One exception is the External Exporter Type which is started externally.","title":"API"},{"location":"details/api.html#api-keys-and-authentication","text":"API keys are used to control access to the PMM server components and resources. With an API key, you are authenticated to the PMM server, have access to PMM server components and resources, and perform various actions on them. You can use API keys as a replacement for basic authentication.","title":"API Keys and authentication"},{"location":"details/api.html#generate-api-keys","text":"PMM uses the Grafana API keys for authentication. Following are the steps to generate the API keys: Login to PMM. From the side menu, click Configuration \u2192 API keys . On the Configuration page, click Add API Key . Add API key dialog box opens. Enter the following to generate an API key: key name (you can give any desired name) Select the Role from the dropdown Enter a value in the Time to live text box (hover on the tooltip for more information). Click Add. API Key Created window displays your newly created key. Make sure to copy your key and keep it secure.","title":"Generate API keys"},{"location":"details/api.html#authenticate","text":"You can authenticate your request using the HTTPS header. Important Use the -k or --insecure parameter to force cURL to ignore invalid and self-signed SSL certificate errors. The option will skip the SSL verification process, and you can bypass any SSL errors while still having SSL-encrypted communication. However, using the --insecure parameter is not recommended. Although the data transfer is encrypted, it is not entirely secure. For enhanced security of your PMM installation, you need valid SSL certificates. For information on validating SSL certificates, refer to: SSL certificates . curl -H \"Authorization: Bearer <api_key>\" https://127.0.0.1/v1/version","title":"Authenticate"},{"location":"details/api.html#use-an-api-key-in-basic-auth","text":"You can pass the API key into a REST API call as a query parameter in the following format. Replace API_KEY with your API key. Example curl -X GET https://api_key:API_KEY@localhost/v1/version","title":"Use an API key in basic auth"},{"location":"details/architecture.html","text":"Architecture \u00b6 PMM is a client/server application built by Percona comprising its own and third-party components and tools. PMM Server \u00b6 PMM Server is the heart of PMM. It receives data from clients, collects it, and stores it. Metrics are drawn as tables, charts and graphs within dashboards , each a part of the web-based user interface . PMM Client \u00b6 PMM Client is a collection of agents and exporters that run on the host being monitored. PMM Client runs on every database host or node you want to monitor. The client collects server metrics, general system metrics, query analytics and sends it to the server. Except when monitoring AWS RDS instances, a PMM Client must be running on the host to be monitored. Percona Platform \u00b6 Percona Platform provides value-added services for PMM. PMM context \u00b6 The PMM Client package provides: Exporters for each database and service type. When an exporter runs, it connects to the database or service instance, runs the metrics collection routines, and sends the results to PMM Server. pmm-agent : Run as a daemon process, it starts and stops exporters when instructed. vmagent : A VictoriaMetrics daemon process that sends metrics data ( pushes ) to PMM Server. The PMM Server package provides: pmm-managed Query Analytics Grafana VictoriaMetrics PMM Server \u00b6 PMM Server includes the following tools: Query Analytics (QAN) enables you to analyze database query performance over periods of time. In addition to the client-side QAN agent, it includes the following: QAN API is the back-end for storing and accessing query data collected by the QAN agent running on a PMM Client. QAN App is a web application for visualizing collected Query Analytics data which is part of the PMM Server\u2019s UI. Metrics Monitor provides a historical view of metrics that are critical to a MySQL or MongoDB server instance. It includes the following: VictoriaMetrics , a scalable time-series database. (Replaced Prometheus in PMM 2.12.0 .) ClickHouse is a third-party column-oriented database that facilitates the Query Analytics functionality. Grafana is a third-party dashboard and graph builder for visualizing data aggregated (by VictoriaMetrics or Prometheus) in an intuitive web interface. Percona Dashboards is a set of dashboards for Grafana developed by Percona. PMM Client \u00b6 The PMM Client package consists of the following: pmm-admin is a command-line tool for managing PMM Client, for example, adding and removing database instances that you want to monitor. ( Read more ). pmm-agent is a client-side component a minimal command-line interface, which is a central entry point in charge for bringing the client functionality: it carries on client\u2019s authentication, gets the client configuration stored on the PMM Server, manages exporters and other agents. node_exporter is an exporter that collects general system metrics. mysqld_exporter is an exporter that collects MySQL server metrics. mongodb_exporter is an exporter that collects MongoDB server metrics. postgres_exporter is an exporter that collects PostgreSQL performance metrics. proxysql_exporter is an exporter that collects ProxySQL performance metrics. rds_exporter is an exporter that collects Amazon RDS performance metrics. azure_database_exporter is an exporter that collects Azure database performance metrics. To make data transfer from PMM Client to PMM Server secure, all exporters are able to use SSL/TLS encrypted connections, and their communication with the PMM server is protected by the HTTP basic authentication.","title":"Architecture"},{"location":"details/architecture.html#architecture","text":"PMM is a client/server application built by Percona comprising its own and third-party components and tools.","title":"Architecture"},{"location":"details/architecture.html#pmm-server","text":"PMM Server is the heart of PMM. It receives data from clients, collects it, and stores it. Metrics are drawn as tables, charts and graphs within dashboards , each a part of the web-based user interface .","title":"PMM Server"},{"location":"details/architecture.html#pmm-client","text":"PMM Client is a collection of agents and exporters that run on the host being monitored. PMM Client runs on every database host or node you want to monitor. The client collects server metrics, general system metrics, query analytics and sends it to the server. Except when monitoring AWS RDS instances, a PMM Client must be running on the host to be monitored.","title":"PMM Client"},{"location":"details/architecture.html#percona-platform","text":"Percona Platform provides value-added services for PMM.","title":"Percona Platform"},{"location":"details/architecture.html#pmm-context","text":"The PMM Client package provides: Exporters for each database and service type. When an exporter runs, it connects to the database or service instance, runs the metrics collection routines, and sends the results to PMM Server. pmm-agent : Run as a daemon process, it starts and stops exporters when instructed. vmagent : A VictoriaMetrics daemon process that sends metrics data ( pushes ) to PMM Server. The PMM Server package provides: pmm-managed Query Analytics Grafana VictoriaMetrics","title":"PMM context"},{"location":"details/architecture.html#pmm-server_1","text":"PMM Server includes the following tools: Query Analytics (QAN) enables you to analyze database query performance over periods of time. In addition to the client-side QAN agent, it includes the following: QAN API is the back-end for storing and accessing query data collected by the QAN agent running on a PMM Client. QAN App is a web application for visualizing collected Query Analytics data which is part of the PMM Server\u2019s UI. Metrics Monitor provides a historical view of metrics that are critical to a MySQL or MongoDB server instance. It includes the following: VictoriaMetrics , a scalable time-series database. (Replaced Prometheus in PMM 2.12.0 .) ClickHouse is a third-party column-oriented database that facilitates the Query Analytics functionality. Grafana is a third-party dashboard and graph builder for visualizing data aggregated (by VictoriaMetrics or Prometheus) in an intuitive web interface. Percona Dashboards is a set of dashboards for Grafana developed by Percona.","title":"PMM Server"},{"location":"details/architecture.html#pmm-client_1","text":"The PMM Client package consists of the following: pmm-admin is a command-line tool for managing PMM Client, for example, adding and removing database instances that you want to monitor. ( Read more ). pmm-agent is a client-side component a minimal command-line interface, which is a central entry point in charge for bringing the client functionality: it carries on client\u2019s authentication, gets the client configuration stored on the PMM Server, manages exporters and other agents. node_exporter is an exporter that collects general system metrics. mysqld_exporter is an exporter that collects MySQL server metrics. mongodb_exporter is an exporter that collects MongoDB server metrics. postgres_exporter is an exporter that collects PostgreSQL performance metrics. proxysql_exporter is an exporter that collects ProxySQL performance metrics. rds_exporter is an exporter that collects Amazon RDS performance metrics. azure_database_exporter is an exporter that collects Azure database performance metrics. To make data transfer from PMM Client to PMM Server secure, all exporters are able to use SSL/TLS encrypted connections, and their communication with the PMM server is protected by the HTTP basic authentication.","title":"PMM Client"},{"location":"details/clickhouse%202.html","text":"ClickHouse \u00b6 You can use an external ClickHouse database instance outside the PMM Server container running on other hosts. Environment variables \u00b6 PMM predefines certain flags that allow you to use ClickHouse parameters as environment variables: Warning The PERCONA_TEST_* environment variables are experimental and subject to change. It is recommended that you use these variables for testing purposes only and not on production. To use ClickHouse as an external database instance, use the following environment variables: PERCONA_TEST_PMM_CLICKHOUSE_ADDR -> hostname:port Name of the host and port of the external ClickHouse database instance. Optional environment variables PERCONA_TEST_PMM_CLICKHOUSE_DATABASE -> database name Database name of the external ClickHouse database instance. \u200b\u200bPERCONA_TEST_PMM_CLICKHOUSE_POOL_SIZE -> pool size The maximum number of threads in the current connection thread pool. This value cannot be bigger than max_thread_pool_size. PERCONA_TEST_PMM_CLICKHOUSE_BLOCK_SIZE -> max block size The number of rows to load from tables in one block for this connection. Example To use ClickHouse as an external database instance, start the PMM docker with the specified variables for external ClickHouse: \u200b\u200b -e PERCONA_TEST_PMM_CLICKHOUSE_ADDR = $ADDRESS : $PORT -e PERCONA_TEST_PMM_CLICKHOUSE_DATABASE = $DB -e PERCONA_TEST_PMM_CLICKHOUSE_POOL_SIZE = 1 -e PERCONA_TEST_PMM_CLICKHOUSE_BLOCK_SIZE = 65000 Troubleshooting \u00b6 To troubleshoot issues, see the ClickHouse troubleshooting documentation .","title":"ClickHouse"},{"location":"details/clickhouse%202.html#clickhouse","text":"You can use an external ClickHouse database instance outside the PMM Server container running on other hosts.","title":"ClickHouse"},{"location":"details/clickhouse%202.html#environment-variables","text":"PMM predefines certain flags that allow you to use ClickHouse parameters as environment variables: Warning The PERCONA_TEST_* environment variables are experimental and subject to change. It is recommended that you use these variables for testing purposes only and not on production. To use ClickHouse as an external database instance, use the following environment variables: PERCONA_TEST_PMM_CLICKHOUSE_ADDR -> hostname:port Name of the host and port of the external ClickHouse database instance. Optional environment variables PERCONA_TEST_PMM_CLICKHOUSE_DATABASE -> database name Database name of the external ClickHouse database instance. \u200b\u200bPERCONA_TEST_PMM_CLICKHOUSE_POOL_SIZE -> pool size The maximum number of threads in the current connection thread pool. This value cannot be bigger than max_thread_pool_size. PERCONA_TEST_PMM_CLICKHOUSE_BLOCK_SIZE -> max block size The number of rows to load from tables in one block for this connection. Example To use ClickHouse as an external database instance, start the PMM docker with the specified variables for external ClickHouse: \u200b\u200b -e PERCONA_TEST_PMM_CLICKHOUSE_ADDR = $ADDRESS : $PORT -e PERCONA_TEST_PMM_CLICKHOUSE_DATABASE = $DB -e PERCONA_TEST_PMM_CLICKHOUSE_POOL_SIZE = 1 -e PERCONA_TEST_PMM_CLICKHOUSE_BLOCK_SIZE = 65000","title":"Environment variables"},{"location":"details/clickhouse%202.html#troubleshooting","text":"To troubleshoot issues, see the ClickHouse troubleshooting documentation .","title":"Troubleshooting"},{"location":"details/clickhouse.html","text":"ClickHouse \u00b6 You can use an external ClickHouse database instance outside the PMM Server container running on other hosts. Environment variables \u00b6 PMM predefines certain flags that allow you to use ClickHouse parameters as environment variables: Warning The PERCONA_TEST_* environment variables are experimental and subject to change. It is recommended that you use these variables for testing purposes only and not on production. To use ClickHouse as an external database instance, use the following environment variables: PERCONA_TEST_PMM_CLICKHOUSE_ADDR -> hostname:port Name of the host and port of the external ClickHouse database instance. Optional environment variables PERCONA_TEST_PMM_CLICKHOUSE_DATABASE -> database name Database name of the external ClickHouse database instance. \u200b\u200bPERCONA_TEST_PMM_CLICKHOUSE_POOL_SIZE -> pool size The maximum number of threads in the current connection thread pool. This value cannot be bigger than max_thread_pool_size. PERCONA_TEST_PMM_CLICKHOUSE_BLOCK_SIZE -> max block size The number of rows to load from tables in one block for this connection. Example To use ClickHouse as an external database instance, start the PMM docker with the specified variables for external ClickHouse: \u200b\u200b -e PERCONA_TEST_PMM_CLICKHOUSE_ADDR = $ADDRESS : $PORT -e PERCONA_TEST_PMM_CLICKHOUSE_DATABASE = $DB -e PERCONA_TEST_PMM_CLICKHOUSE_POOL_SIZE = 1 -e PERCONA_TEST_PMM_CLICKHOUSE_BLOCK_SIZE = 65000 Troubleshooting \u00b6 To troubleshoot issues, see the ClickHouse troubleshooting documentation .","title":"ClickHouse"},{"location":"details/clickhouse.html#clickhouse","text":"You can use an external ClickHouse database instance outside the PMM Server container running on other hosts.","title":"ClickHouse"},{"location":"details/clickhouse.html#environment-variables","text":"PMM predefines certain flags that allow you to use ClickHouse parameters as environment variables: Warning The PERCONA_TEST_* environment variables are experimental and subject to change. It is recommended that you use these variables for testing purposes only and not on production. To use ClickHouse as an external database instance, use the following environment variables: PERCONA_TEST_PMM_CLICKHOUSE_ADDR -> hostname:port Name of the host and port of the external ClickHouse database instance. Optional environment variables PERCONA_TEST_PMM_CLICKHOUSE_DATABASE -> database name Database name of the external ClickHouse database instance. \u200b\u200bPERCONA_TEST_PMM_CLICKHOUSE_POOL_SIZE -> pool size The maximum number of threads in the current connection thread pool. This value cannot be bigger than max_thread_pool_size. PERCONA_TEST_PMM_CLICKHOUSE_BLOCK_SIZE -> max block size The number of rows to load from tables in one block for this connection. Example To use ClickHouse as an external database instance, start the PMM docker with the specified variables for external ClickHouse: \u200b\u200b -e PERCONA_TEST_PMM_CLICKHOUSE_ADDR = $ADDRESS : $PORT -e PERCONA_TEST_PMM_CLICKHOUSE_DATABASE = $DB -e PERCONA_TEST_PMM_CLICKHOUSE_POOL_SIZE = 1 -e PERCONA_TEST_PMM_CLICKHOUSE_BLOCK_SIZE = 65000","title":"Environment variables"},{"location":"details/clickhouse.html#troubleshooting","text":"To troubleshoot issues, see the ClickHouse troubleshooting documentation .","title":"Troubleshooting"},{"location":"details/glossary.html","text":"Glossary \u00b6 Annotation \u00b6 A way of showing a mark on dashboards signifying an important point in time. Dimension \u00b6 In the Query Analytics dashboard, to help focus on the possible source of performance issues, you can group queries by dimension , one of: Query, Service Name, Database, Schema, User Name, Client Host EBS \u00b6 Amazon\u2019s Elastic Block Store. Fingerprint \u00b6 A normalized statement digest \u2014a query string with values removed that acts as a template or typical example for a query. IAM \u00b6 Identity and Access Management (for Amazon AWS). MM \u00b6 Metrics Monitor. NUMA \u00b6 Non-Uniform Memory Access. PEM \u00b6 Privacy Enhanced Mail. QPS \u00b6 Queries Per Second. A measure of the rate of queries being monitored. Query Analytics \u00b6 Component of PMM Server that enables you to analyze MySQL query performance over periods of time. Advisors \u00b6 Automated checks that you can run against connected databases to identify any potential security threats, configuration problems, performance concerns, policy non-compliance issues etc. Technical Preview \u00b6 Releases intended for public preview and feedback but with no support or service level agreement (SLA). Should not be used on production or business-critical systems. May contain breaking changes to UI, API, CLI. ( Read more .) VG \u00b6 Volume Group.","title":"Glossary"},{"location":"details/glossary.html#glossary","text":"","title":"Glossary"},{"location":"details/glossary.html#annotation","text":"A way of showing a mark on dashboards signifying an important point in time.","title":"Annotation"},{"location":"details/glossary.html#dimension","text":"In the Query Analytics dashboard, to help focus on the possible source of performance issues, you can group queries by dimension , one of: Query, Service Name, Database, Schema, User Name, Client Host","title":"Dimension"},{"location":"details/glossary.html#ebs","text":"Amazon\u2019s Elastic Block Store.","title":"EBS"},{"location":"details/glossary.html#fingerprint","text":"A normalized statement digest \u2014a query string with values removed that acts as a template or typical example for a query.","title":"Fingerprint"},{"location":"details/glossary.html#iam","text":"Identity and Access Management (for Amazon AWS).","title":"IAM"},{"location":"details/glossary.html#mm","text":"Metrics Monitor.","title":"MM"},{"location":"details/glossary.html#numa","text":"Non-Uniform Memory Access.","title":"NUMA"},{"location":"details/glossary.html#pem","text":"Privacy Enhanced Mail.","title":"PEM"},{"location":"details/glossary.html#qps","text":"Queries Per Second. A measure of the rate of queries being monitored.","title":"QPS"},{"location":"details/glossary.html#query-analytics","text":"Component of PMM Server that enables you to analyze MySQL query performance over periods of time.","title":"Query Analytics"},{"location":"details/glossary.html#advisors","text":"Automated checks that you can run against connected databases to identify any potential security threats, configuration problems, performance concerns, policy non-compliance issues etc.","title":"Advisors"},{"location":"details/glossary.html#technical-preview","text":"Releases intended for public preview and feedback but with no support or service level agreement (SLA). Should not be used on production or business-critical systems. May contain breaking changes to UI, API, CLI. ( Read more .)","title":"Technical Preview"},{"location":"details/glossary.html#vg","text":"Volume Group.","title":"VG"},{"location":"details/interface.html","text":"UI components \u00b6 Key Main menu (also Grafana menu , side menu ) Navigation bar View controls View selectors (dynamic contents) Shortcut menu (dynamic contents) Main menu \u00b6 The main menu is part of the Grafana framework and is visible on every page. Item (Top) Name Description Home Link to home dashboard. Search Search dashboards by name. Starred Mark your favorite dashboards. Dashboards Create dashboards or folders , manage dashboards, import dashboards, create playlists, manage snapshots. Operating System (OS) Operating System dashboard MySQL MySQL dashboard MongoDB MongoDB dashboard PostgreSQL PostgreSQL dashboard ProxySQL ProxySQL dashboard HAproxy HAproxy dashboard Query Analytics (QAN) Query Analytics Explore Run queries with PromQL . Percona Alerting Alerting , Create new alerts and manage your alert rules and alert templates. Configuration Entitlements This tab is displayed after connecting PMM to Percona Portal, and shows all your Percona Platform account information. Support Tickets Shows the list of tickets opened across your organization. This tab is only available after you connect PMM to Percona Platform. Environment Overview This tab is displayed after connecting PMM to Percona Portal. Shows the name and email of the Customer Success Manager assigned to your organization, who can help with any PMM queries. This tab tab will soon be populated with more useful information about your PMM environment. Server Admin Backup Management Backup management and storage location configuration . PMM Advisor Checks Run health assessment checks against your connected databases and check any failed checks. DBaaS Tip The DBaaS icon appears only if a server feature flag has been set. The Backup Management icon appears when Backup Management is activated in Configuration \u2192 Settings \u2192 Advanced Settings . Icon (Bottom) Description (Profile icon) User menu Help Navigation bar \u00b6 Item (left) Description (Display only.) (Name) / (Optional) Folder name. (Name) Dashboard name. Mark as favorite. Share dashboard. View controls \u00b6 Item (right) Description Dashboard settings. Cycle view mode. (time range) Time range selector. Time range zoom out. Refresh dashboard. (Time interval) Refresh period. View selectors \u00b6 This menu bar is context sensitive; it changes according to the page you are on. (With wide menus on small screens, items may wrap to the next row.) Item Description Interval Data interval. Region Filter by region. Environment Filter by environment. Cluster Filter by cluster. Replication Set Filter by replication set. Node Name Filter by node name. Service Name Filter by service name. PMM Annotations View annotations . Services menu \u00b6 The Services menu choice determines the Service Type menu. Menu Item Service type menu Description Services MongoDB Instances Overview MongoDB MongoDB dashboards. MySQL Instances Overview MySQL MySQL dashboards. Nodes Overview OS OS dashboards. PostgreSQL Instances Overview PostgreSQL PostgreSQL dashboards. PMM menu \u00b6 This item lists shortcuts to utility pages. Menu Item PMM PMM Add Instance PMM Database Checks PMM Inventory PMM Settings","title":"UI components"},{"location":"details/interface.html#ui-components","text":"Key Main menu (also Grafana menu , side menu ) Navigation bar View controls View selectors (dynamic contents) Shortcut menu (dynamic contents)","title":"UI components"},{"location":"details/interface.html#main-menu","text":"The main menu is part of the Grafana framework and is visible on every page. Item (Top) Name Description Home Link to home dashboard. Search Search dashboards by name. Starred Mark your favorite dashboards. Dashboards Create dashboards or folders , manage dashboards, import dashboards, create playlists, manage snapshots. Operating System (OS) Operating System dashboard MySQL MySQL dashboard MongoDB MongoDB dashboard PostgreSQL PostgreSQL dashboard ProxySQL ProxySQL dashboard HAproxy HAproxy dashboard Query Analytics (QAN) Query Analytics Explore Run queries with PromQL . Percona Alerting Alerting , Create new alerts and manage your alert rules and alert templates. Configuration Entitlements This tab is displayed after connecting PMM to Percona Portal, and shows all your Percona Platform account information. Support Tickets Shows the list of tickets opened across your organization. This tab is only available after you connect PMM to Percona Platform. Environment Overview This tab is displayed after connecting PMM to Percona Portal. Shows the name and email of the Customer Success Manager assigned to your organization, who can help with any PMM queries. This tab tab will soon be populated with more useful information about your PMM environment. Server Admin Backup Management Backup management and storage location configuration . PMM Advisor Checks Run health assessment checks against your connected databases and check any failed checks. DBaaS Tip The DBaaS icon appears only if a server feature flag has been set. The Backup Management icon appears when Backup Management is activated in Configuration \u2192 Settings \u2192 Advanced Settings . Icon (Bottom) Description (Profile icon) User menu Help","title":"Main menu"},{"location":"details/interface.html#navigation-bar","text":"Item (left) Description (Display only.) (Name) / (Optional) Folder name. (Name) Dashboard name. Mark as favorite. Share dashboard.","title":"Navigation bar"},{"location":"details/interface.html#view-controls","text":"Item (right) Description Dashboard settings. Cycle view mode. (time range) Time range selector. Time range zoom out. Refresh dashboard. (Time interval) Refresh period.","title":"View controls"},{"location":"details/interface.html#view-selectors","text":"This menu bar is context sensitive; it changes according to the page you are on. (With wide menus on small screens, items may wrap to the next row.) Item Description Interval Data interval. Region Filter by region. Environment Filter by environment. Cluster Filter by cluster. Replication Set Filter by replication set. Node Name Filter by node name. Service Name Filter by service name. PMM Annotations View annotations .","title":"View selectors"},{"location":"details/interface.html#services-menu","text":"The Services menu choice determines the Service Type menu. Menu Item Service type menu Description Services MongoDB Instances Overview MongoDB MongoDB dashboards. MySQL Instances Overview MySQL MySQL dashboards. Nodes Overview OS OS dashboards. PostgreSQL Instances Overview PostgreSQL PostgreSQL dashboards.","title":"Services menu"},{"location":"details/interface.html#pmm-menu","text":"This item lists shortcuts to utility pages. Menu Item PMM PMM Add Instance PMM Database Checks PMM Inventory PMM Settings","title":"PMM menu"},{"location":"details/pmm_components_and_versions.html","text":"PMM components and versions \u00b6 The following table lists all the PMM client/server components and their versions: Component Version Documentation Repository Grafana 9.1 Grafana Documentation Github Grafana VictoriaMetrics 1.77.1 VictoriaMetrics Documentation Github VictoriaMetrics Nginx 1.20.1 Nginx Documentation Github Nginx Percona Distribution for PostgreSQL 14.5 Percona Distribution for PostgreSQL 14 Documentation Clickhouse 21.3.20.1 ClickHouse Documentation Documentation Github ClickHouse PerconaToolkit 3.4.0 Percona Toolkit Documentation Github Percona Toolkit Alertmanager 0.22.0 Alertmanager Documentation Github Alertmanager MongoDB exporter 0.34.0 Github percona/mongodb_exporter RDS exporter 0.7.2 Github percona/rds_exporter MySQL exporter v0.14.0 and Percona changes MySQL Server Exporter Documentation Github Prometheus MySQL Exporter Node exporter v1.3.1 and Percona changes Node Exporter Documentation Github node exporter Azure exporter 2.30.0 Github azure_metrics_exporter","title":"PMM components and versions"},{"location":"details/pmm_components_and_versions.html#pmm-components-and-versions","text":"The following table lists all the PMM client/server components and their versions: Component Version Documentation Repository Grafana 9.1 Grafana Documentation Github Grafana VictoriaMetrics 1.77.1 VictoriaMetrics Documentation Github VictoriaMetrics Nginx 1.20.1 Nginx Documentation Github Nginx Percona Distribution for PostgreSQL 14.5 Percona Distribution for PostgreSQL 14 Documentation Clickhouse 21.3.20.1 ClickHouse Documentation Documentation Github ClickHouse PerconaToolkit 3.4.0 Percona Toolkit Documentation Github Percona Toolkit Alertmanager 0.22.0 Alertmanager Documentation Github Alertmanager MongoDB exporter 0.34.0 Github percona/mongodb_exporter RDS exporter 0.7.2 Github percona/rds_exporter MySQL exporter v0.14.0 and Percona changes MySQL Server Exporter Documentation Github Prometheus MySQL Exporter Node exporter v1.3.1 and Percona changes Node Exporter Documentation Github node exporter Azure exporter 2.30.0 Github azure_metrics_exporter","title":"PMM components and versions"},{"location":"details/victoria-metrics.html","text":"VictoriaMetrics \u00b6 VictoriaMetrics is a third-party monitoring solution and time-series database that replaced Prometheus in PMM 2.12.0 . Push/Pull modes \u00b6 VictoriaMetrics metrics data can be both \u2018pushed\u2019 to the server and \u2018pulled\u2019 by the server. When setting up services, you can decide which mode to use. The \u2018push\u2019 mode is now default for newly-added services. (In PMM 2.12.0 the default mode was \u2018pull\u2019.) The mode (push/pull) is controlled by the --metrics-mode flag for the pmm-admin config and pmm-admin add commands. If you need to change the metrics mode for an existing Service, you must remove it and re-add it with the same name and the required flags. (You cannot update a service.) Remapped targets for direct Prometheus paths \u00b6 Direct Prometheus paths return structured information directly from Prometheus, bypassing the PMM application. They are accessed by requesting a URL of the form <PMM SERVER URL>/prometheus/<PATH> . As a result of the move to VictoriaMetrics some direct Prometheus paths are no longer available. Prometheus path VictoriaMetrics equivalent /prometheus/alerts No change. /prometheus/config No equivalent, but there is some information at /prometheus/targets . /prometheus/flags The flag metrics at /prometheus/metrics . /prometheus/graph /graph/explore (Grafana) or graph/d/prometheus-advanced/advanced-data-exploration (PMM dashboard). /prometheus/rules No change. /prometheus/service-discovery No equivalent. /prometheus/status Some information at /prometheus/metrics . High cardinality metrics information at /prometheus/api/v1/status/tsdb . /prometheus/targets /victoriametrics/targets Environment variables \u00b6 PMM predefines certain flags that allow users to set all other VictoriaMetrics parameters as environment variables: The environment variable must be prepended with VM_ . Example To set downsampling, use the downsampling.period parameter as follows: -e VM_downsampling_period=20d:10m,120d:2h This instructs VictoriaMetrics to deduplicate samples older than 20 days with 10 mins intervals and samples older than 120 days with two-hour intervals. Troubleshooting \u00b6 To troubleshoot issues, see the VictoriaMetrics troubleshooting documentation . You can also contact the VictoriaMetrics team via: Google Groups Slack Reddit Telegram","title":"VictoriaMetrics"},{"location":"details/victoria-metrics.html#victoriametrics","text":"VictoriaMetrics is a third-party monitoring solution and time-series database that replaced Prometheus in PMM 2.12.0 .","title":"VictoriaMetrics"},{"location":"details/victoria-metrics.html#pushpull-modes","text":"VictoriaMetrics metrics data can be both \u2018pushed\u2019 to the server and \u2018pulled\u2019 by the server. When setting up services, you can decide which mode to use. The \u2018push\u2019 mode is now default for newly-added services. (In PMM 2.12.0 the default mode was \u2018pull\u2019.) The mode (push/pull) is controlled by the --metrics-mode flag for the pmm-admin config and pmm-admin add commands. If you need to change the metrics mode for an existing Service, you must remove it and re-add it with the same name and the required flags. (You cannot update a service.)","title":"Push/Pull modes"},{"location":"details/victoria-metrics.html#remapped-targets-for-direct-prometheus-paths","text":"Direct Prometheus paths return structured information directly from Prometheus, bypassing the PMM application. They are accessed by requesting a URL of the form <PMM SERVER URL>/prometheus/<PATH> . As a result of the move to VictoriaMetrics some direct Prometheus paths are no longer available. Prometheus path VictoriaMetrics equivalent /prometheus/alerts No change. /prometheus/config No equivalent, but there is some information at /prometheus/targets . /prometheus/flags The flag metrics at /prometheus/metrics . /prometheus/graph /graph/explore (Grafana) or graph/d/prometheus-advanced/advanced-data-exploration (PMM dashboard). /prometheus/rules No change. /prometheus/service-discovery No equivalent. /prometheus/status Some information at /prometheus/metrics . High cardinality metrics information at /prometheus/api/v1/status/tsdb . /prometheus/targets /victoriametrics/targets","title":"Remapped targets for direct Prometheus paths"},{"location":"details/victoria-metrics.html#environment-variables","text":"PMM predefines certain flags that allow users to set all other VictoriaMetrics parameters as environment variables: The environment variable must be prepended with VM_ . Example To set downsampling, use the downsampling.period parameter as follows: -e VM_downsampling_period=20d:10m,120d:2h This instructs VictoriaMetrics to deduplicate samples older than 20 days with 10 mins intervals and samples older than 120 days with two-hour intervals.","title":"Environment variables"},{"location":"details/victoria-metrics.html#troubleshooting","text":"To troubleshoot issues, see the VictoriaMetrics troubleshooting documentation . You can also contact the VictoriaMetrics team via: Google Groups Slack Reddit Telegram","title":"Troubleshooting"},{"location":"details/commands/index.html","text":"Commands \u00b6 pmm-admin \u2013 Command line tool for configuring and administering PMM pmm-agent \u2013 Daemon process, communicating between PMM Client and PMM Server","title":"Commands"},{"location":"details/commands/index.html#commands","text":"pmm-admin \u2013 Command line tool for configuring and administering PMM pmm-agent \u2013 Daemon process, communicating between PMM Client and PMM Server","title":"Commands"},{"location":"details/commands/pmm-admin.html","text":"pmm-admin - PMM Administration Tool \u00b6 NAME \u00b6 pmm-admin - Administer PMM SYNOPSIS \u00b6 pmm-admin [FLAGS] pmm-admin config [FLAGS] --server-url=server-url pmm-admin add DATABASE [FLAGS] [NAME] [ADDRESS] DATABASE:= [ MongoDB | MySQL | PostgreSQL | ProxySQL ] pmm-admin add --pmm-agent-listen-port=LISTEN_PORT DATABASE [FLAGS] [NAME] [ADDRESS] pmm-admin add external [FLAGS] [NAME] [ADDRESS] (CAUTION: Technical preview feature) pmm-admin add haproxy [FLAGS] [NAME] pmm-admin add external [FLAGS] [NAME] [ADDRESS] pmm-admin add external-serverless [FLAGS] [NAME] [ADDRESS] pmm-admin remove [FLAGS] service-type [service-name] pmm-admin register [FLAGS] [node-address] [node-type] [node-name] pmm-admin list [FLAGS] [node-address] pmm-admin status [FLAGS] [node-address] pmm-admin summary [FLAGS] [node-address] pmm-admin annotate [--node|--service] [--tags <tags>] [node-name|service-name] pmm-admin help [COMMAND] DESCRIPTION \u00b6 pmm-admin is a command-line tool for administering PMM using a set of COMMAND keywords and associated FLAGS. PMM communicates with the PMM Server via a PMM agent process. COMMON FLAGS \u00b6 -h , --help Show help and exit. --help-long Show extended help and exit. --help-man Generate man page. (Use pmm-admin --help-man | man -l - to view.) --debug Enable debug logging. --trace Enable trace logging (implies debug). --log-level (This parameter is available starting with PMM 2.29.0.) Set the level for the logs as per your requirement such as INFO, WARNING, ERROR, and FATAL. --json Enable JSON output. --version Show the application version and exit. --server-url=server-url PMM Server URL in https://username:password@pmm-server-host/ format. --server-insecure-tls Skip PMM Server TLS certificate validation. --group=<group-name> Group name for external services. Default: external COMMANDS \u00b6 GENERAL COMMANDS \u00b6 pmm-admin help [COMMAND] Show help for COMMAND . INFORMATION COMMANDS \u00b6 pmm-admin list --server-url=server-url [FLAGS] Show Services and Agents running on this Node, and the agent mode (push/pull). pmm-admin status --server-url=server-url [FLAGS] Show the following information about a local pmm-agent, and its connected server and clients: Agent: Agent ID, Node ID. PMM Server: URL and version. PMM Client: connection status, time drift, latency, vmagent status, pmm-admin version. Agents: Agent ID path and client name. FLAGS: --wait=<period><unit> Time to wait for a successful response from pmm-agent. period is an integer. unit is one of ms for milliseconds, s for seconds, m for minutes, h for hours. pmm-admin summary --server-url=server-url [FLAGS] Creates an archive file in the current directory with default file name summary_<hostname>_<year>_<month>_<date>_<hour>_<minute>_<second>.zip . The contents are two directories, client and server containing diagnostic text files. FLAGS: --filename=\"filename\" The Summary Archive filename. --skip-server Skip fetching logs.zip from PMM Server. --pprof (This parameter is available starting with PMM 2.29.0) Include performance profiling data in the summary. CONFIGURATION COMMANDS \u00b6 pmm-admin config \u00b6 pmm-admin config [FLAGS] [node-address] [node-type] [node-name] Configure a local pmm-agent . FLAGS: --node-id=node-id Node ID (default is auto-detected). --node-model=node-model Node model. --region=region Node region. --az=availability-zone Node availability zone. --metrics-mode=mode Metrics flow mode for agents node-exporter. Allowed values: - auto : chosen by server (default). - push : agent will push metrics. - pull : server scrapes metrics from agent. --paths-base=dir Base path where all binaries, tools and collectors of PMM client are located --agent-password=password (This parameter i available starting with PMM 2.29.0.) Custom agent password. pmm-admin register \u00b6 pmm-admin register [FLAGS] [node-address] [node-type] [node-name] Register the current Node with the PMM Server. --server-url=server-url PMM Server URL in https://username:password@pmm-server-host/ format. --machine-id=\"/machine_id/9812826a1c45454a98ba45c56cc4f5b0\" Node machine-id (default is auto-detected). --distro=\"linux\" Node OS distribution (default is auto-detected). --container-id=container-id Container ID. --container-name=container-name Container name. --node-model=node-model Node model. --region=region Node region. --az=availability-zone Node availability zone. --custom-labels=labels Custom user-assigned labels. --agent-password=password (This parameter is available starting with PMM 2.29.0.) Custom agent password. pmm-admin add --pmm-agent-listen-port=LISTEN_PORT \u00b6 pmm-admin add --pmm-agent-listen-port=LISTEN_PORT DATABASE [FLAGS] [NAME] [ADDRESS] Configure the PMM agent with a listen port. --pmm-agent-listen-port=LISTEN_PORT The PMM agent listen port. DATABASE:= [ MongoDB | MySQL | PostgreSQL | ProxySQL ] pmm-admin remove \u00b6 pmm-admin remove [FLAGS] service-type [service-name] Remove Service from monitoring. --service-id=service-id Service ID. --force Remove service with that name or ID and all dependent services and agents. When you remove a service, collected data remains on PMM Server for the specified retention period . pmm-admin annotate \u00b6 pmm-admin annotate [--node|--service] <annotation> [--tags <tags>] [--node-name=<node>] [--service-name=<service>] Annotate an event. ( Read more ) <annotation> The annotation string. If it contains spaces, it should be quoted. --node Annotate the current node or that specified by --node-name . --service Annotate all services running on the current node, or that specified by --service-name . --tags A quoted string that defines one or more comma-separated tags for the annotation. Example: \"tag 1,tag 2\" . --node-name The node name being annotated. --service-name The service name being annotated. Combining flags Flags may be combined as shown in the following examples. --node Current node. --node-name Node with name. --node --node-name=NODE_NAME Node with name. --node --service-name Current node and service with name. --node --node-name --service-name Node with name and service with name. --node --service Current node and all services of current node. -node --node-name --service --service-name Service with name and node with name. --service All services of the current node. --service-name Service with name. --service --service-name Service with name. --service --node-name All services of current node and node with name. --service-name --node-name Service with name and node with name. --service --service-name -node-name Service with name and node with name. Tip If node or service name is specified, they are used instead of other parameters. DATABASE COMMANDS \u00b6 MongoDB \u00b6 pmm-admin add mongodb [FLAGS] [node-name] [node-address] Add MongoDB to monitoring. FLAGS: --node-id=node-id Node ID (default is auto-detected). --pmm-agent-id=pmm-agent-id The pmm-agent identifier which runs this instance (default is auto-detected). --username=username MongoDB username. --password=password MongoDB password. --agent-password=password Override the default password for accessing the /metrics endpoint. (Username is pmm and default password is the agent ID.) Avoid using special characters like \u2018', \u2018;\u2019 and \u2018$\u2019 in the custom password. --query-source=profiler Source of queries, one of: profiler , none (default: profiler ). --environment=environment Environment name. --cluster=cluster Cluster name. --replication-set=replication-set Replication set name. --custom-labels=custom-labels Custom user-assigned labels. --skip-connection-check Skip connection check. --tls Use TLS to connect to the database. --tls-skip-verify Skip TLS certificates validation. --tls-certificate-key-file=PATHTOCERT Path to TLS certificate file. --tls-certificate-key-file-password=IFPASSWORDTOCERTISSET Password for TLS certificate file. --tls-ca-file=PATHTOCACERT Path to certificate authority file. --metrics-mode=mode Metrics flow mode for agents node-exporter. Allowed values: - auto : chosen by server (default). - push : agent will push metrics. - pull : server scrapes metrics from agent. Advanced Options \u00b6 PMM starts the MongoDB exporter by default only with diagnosticdata and replicasetstatus collectors enabled. FLAGS: --enable-all-collectors Enable all collectors. --disable-collectors Comma-separated list of collector names to exclude from exporter. --max-collections-limit=-1 Disable collstats, dbstats, topmetrics and indexstats if there are more than collections. 0: No limit. Default is -1, PMM automatically sets this value. A very high limit of max-collections-limit could impact the CPU and Memory usage. Check --stats-collections to limit the scope of collections and DB\u2019s metrics to be fetched. --stats-collections=db1,db2.col1 Collections for collstats & indexstats. Enable all collectors \u00b6 To enable all collectors, pass the parameter --enable-all-collectors in the pmm-admin add mongodb command. This will enable collstats , dbstats , indexstats , and topmetrics collectors. Disable some collectors \u00b6 To enable only some collectors, pass the parameter --enable-all-collectors along with the parameter --disable-collectors . For example, if you want all collectors except topmetrics , specify: --enable-all-collectors --disable-collectors=topmetrics Limit dbStats, collStats and indexStats \u00b6 By default, PMM decides the limit for the number of collections to monitor the collStats and indexStats collectors. You can also set an additional limit for the collStats, indexStats, dbStats, and topmetrics collectors with the \u2013max-collections-limit parameter. Set the value of the parameter --max-collections-limit to: 0: which indicates that collStats and indexStats can handle unlimited collections. n, which indicates that collStats and indexStats can handle <=n collections. If the limit is crossed - exporter stops collecting monitoring data for the collStats and indexStats collectors. -1 (default) doesn\u2019t need to be explicitly set. It indicates that PMM decides how many collections it would monitor, currently <=200 (subject to change). To further limit collections to monitor, enable collStats and indexStats for some databases or collections: Specify the databases and collections that collStats and indexStats will use to collect data using the parameter --stats-collections . This parameter receives a comma-separated list of namespaces in the form database[.collection] . Examples \u00b6 To add MongoDB with all collectors ( diagnosticdata , replicasetstatus , collstats , dbstats , indexstats , and topmetrics ) with default limit detected by PMM (currently <=200 collections, but subject to change): pmm-admin add mongodb --username=admin --password=admin_pass --enable-all-collectors mongodb_srv_1 127.0.0.1:27017 To add MongoDB with all collectors (diagnosticdata, replicasetstatus, collstats, dbstats, indexstats, and topmetrics) with max-collections-limit set to 1000: pmm-admin add mongodb --username=admin --password=admin_pass --enable-all-collectors --max-collections-limit=1000 mongodb_srv_1 127.0.0.1:27017 To enable all the collectors with an unlimited number of collections monitored: pmm-admin add mongodb --username=admin --password=admin_pass --enable-all-collectors --max-collections-limit=0 mongodb_srv_1 127.0.0.1:27017 To add MongoDB with default collectors ( diagnosticdata and replicasetstatus ): pmm-admin add mongodb --username=admin --password=admin_pass mongodb_srv_1 127.0.0.1:27017 Disable collstats collector and enable all the others without limiting max-collections-limit : pmm-admin add mongodb --username=admin --password=admin_pass --enable-all-collectors --max-collections-limit=0 --disable-collectors=collstats mongodb_srv_1 127.0.0.1:27017 If --stats-collections=db1,db2.col1 then the collectors are run as follows: Database Collector is run on db1 All the collections db2 Only for collection col1 Enable all collectors and limit monitoring for dbstats , indexstats , collstats and topmetrics for all collections in db1 and col1 collection in db2 , without limiting max-collections-limit for a number of collections in db1 : pmm-admin add mongodb --username=admin --password=admin_pass --enable-all-collectors --max-collections-limit=0 --stats-collections=db1,db2.col1 mongodb_srv_1 127.0.0.1:27017 Resolutions \u00b6 PMM collects metrics in two resolutions to decrease CPU and Memory usage: high and low resolutions. In high resolution we collect metrics from collectors which work fast: - diagnosticdata - replicasetstatus - topmetrics In low resolution we collect metrics from collectors which could take some time: - dbstats - indexstats - collstats MySQL \u00b6 pmm-admin add mysql [FLAGS] node-name node-address | [--name=service-name] --address=address[:port] | --socket Add MySQL to monitoring. FLAGS: --address MySQL address and port (default: 127.0.0.1:3306). --socket=socket Path to MySQL socket. (Find the socket path with mysql -u root -p -e \"select @@socket\" .) --node-id=node-id Node ID (default is auto-detected). --pmm-agent-id=pmm-agent-id The pmm-agent identifier which runs this instance (default is auto-detected). --username=username MySQL username. --password=password MySQL password. --agent-password=password Override the default password for accessing the /metrics endpoint. (Username is pmm and default password is the agent ID.) Avoid using special characters like \u2018', \u2018;\u2019 and \u2018$\u2019 in the custom password. --query-source=slowlog Source of SQL queries, one of: slowlog , perfschema , none (default: slowlog ). For slowlog query source you need change permissions for specific files. Root permissions are needed for this. --size-slow-logs=N Rotate slow log file at this size. If 0 , use server-defined default. Negative values disable log rotation. A unit suffix must be appended to the number and can be one of: KiB , MiB , GiB , TiB for base 2 units (1024, 1048576, etc). --disable-queryexamples Disable collection of query examples. --disable-tablestats Disable table statistics collection. Excluded collectors for low-resolution time intervals: --collect.auto_increment.columns --collect.info_schema.tables --collect.info_schema.tablestats --collect.perf_schema.indexiowaits --collect.perf_schema.tableiowaits --collect.perf_schema.file_instances Excluded collectors for medium-resolution time intervals: --collect.perf_schema.tablelocks --disable-tablestats-limit=disable-tablestats-limit Table statistics collection will be disabled if there are more than specified number of tables (default: server-defined). 0=no limit. Negative value disables collection. --environment=environment Environment name. --cluster=cluster Cluster name. --replication-set=replication-set Replication set name. --custom-labels=custom-labels Custom user-assigned labels. --skip-connection-check Skip connection check. --tls Use TLS to connect to the database. --tls-skip-verify Skip TLS certificates validation. --tls-cert-file=PATHTOCERT Path to TLS client certificate file. --tls-key=PATHTOCERTKEY Key for TLS client certificate file. --tls-ca-file=PATHTOCACERT Path to certificate authority file. --ssl-ca=PATHTOCACERT The path name of the Certificate Authority (CA) certificate file. If used must specify the same certificate used by the server. (-ssl-capath is similar but specifies the path name of a directory of CA certificate files.) --ssl-cert=PATHTOCERTKEY The path name of the client public key certificate file. --ssl-key The path name of the client private key file. --ssl-skip-verify Skip SSL certificate verification. --metrics-mode=mode Metrics flow mode for agents node-exporter. Allowed values: - auto : chosen by server (default). - push : agent will push metrics. - pull : server scrapes metrics from agent. PostgreSQL \u00b6 pmm-admin add postgresql [FLAGS] [node-name] [node-address] Add PostgreSQL to monitoring. FLAGS: --node-id=<node id> Node ID (default is auto-detected). --pmm-agent-id=<pmm agent id> The pmm-agent identifier which runs this instance (default is auto-detected). --username=<username> PostgreSQL username. --password=<password> PostgreSQL password. --database=<database> PostgreSQL database (default: postgres). --agent-password=password Override the default password for accessing the /metrics endpoint. (Username is pmm and default password is the agent ID.) Avoid using special characters like \u2018', \u2018;\u2019 and \u2018$\u2019 in the custom password. --query-source=<query source> Source of SQL queries, one of: pgstatements , pgstatmonitor , none (default: pgstatements ). --environment=<environment> Environment name. --cluster=<cluster> Cluster name. --replication-set=<replication set> Replication set name. --custom-labels=<custom labels> Custom user-assigned labels. --skip-connection-check Skip connection check. --tls Use TLS to connect to the database. --tls-skip-verify Skip TLS certificates validation. --tls-ca-file TLS CA certificate file. --tls-cert-file TLS certificate file. --tls-key-file TLS certificate key file. --metrics-mode=mode Metrics flow mode for agents node-exporter. Allowed values: - auto : chosen by server (default). - push : agent will push metrics. - pull : server scrapes metrics from agent. ProxySQL \u00b6 pmm-admin add proxysql [FLAGS] [node-name] [node-address] Add ProxySQL to monitoring. FLAGS: --node-id=node-id Node ID (default is auto-detected). --pmm-agent-id=pmm-agent-id The pmm-agent identifier which runs this instance (default is auto-detected). --username=username ProxySQL username. --password=password ProxySQL password. --agent-password=password Override the default password for accessing the /metrics endpoint. (Username is pmm and default password is the agent ID.) Avoid using special characters like \u2018', \u2018;\u2019 and \u2018$\u2019 in the custom password. --environment=environment Environment name. --cluster=cluster Cluster name. --replication-set=replication-set Replication set name. --custom-labels=custom-labels Custom user-assigned labels. --skip-connection-check Skip connection check. --tls Use TLS to connect to the database. --tls-skip-verify Skip TLS certificates validation. --metrics-mode=mode Metrics flow mode for agents node-exporter. Allowed values: - auto : chosen by server (default). - push : agent will push metrics. - pull : server scrapes metrics from agent. --disable-collectors Comma-separated list of collector names to exclude from exporter. HAProxy \u00b6 pmm-admin add haproxy [FLAGS] [NAME] Add HAProxy to monitoring. FLAGS: --server-url=SERVER-URL PMM Server URL in https://username:password@pmm-server-host/ format. --server-insecure-tls Skip PMM Server TLS certificate validation. --username=USERNAME HAProxy username. --password=PASSWORD HAProxy password. --scheme=SCHEME Scheme to generate URI to exporter metrics endpoints (http or https). --metrics-path=METRICS-PATH Path under which metrics are exposed, used to generate URI (default: /metrics). --listen-port=LISTEN-PORT Listen port of haproxy exposing the metrics for scraping metrics (Required). --service-node-id=SERVICE-NODE-ID Node ID where service runs (default is auto-detected). --environment=ENVIRONMENT Environment name like \u2018production\u2019 or \u2018qa\u2019. --cluster=CLUSTER Cluster name. --replication-set=REPLICATION-SET Replication set name. --custom-labels=CUSTOM-LABELS Custom user-assigned labels. Example: region=east,app=app1. --metrics-mode=MODE Metrics flow mode for agents node-exporter. Allowed values: - auto : chosen by server (default). - push : agent will push metrics. - pull : server scrapes metrics from agent. --skip-connection-check Skip connection check. OTHER COMMANDS \u00b6 pmm-admin add external [FLAGS] Add External source of data (like a custom exporter running on a port) to be monitored. FLAGS: --service-name=\"current-hostname\" Service name (autodetected defaults to the hostname where pmm-admin is running). --agent-node-id=AGENT-NODE-ID Node ID where agent runs (default is autodetected). --username=USERNAME External username. --password=PASSWORD External password. --scheme=http or https Scheme to generate URI to exporter metrics endpoints. --metrics-path=/metrics Path under which metrics are exposed, used to generate URI. --listen-port=LISTEN-PORT Listen port of external exporter for scraping metrics. (Required.) --service-node-id=SERVICE-NODE-ID Node ID where service runs (default is autodetected). --environment=prod Environment name like \u2018production\u2019 or \u2018qa\u2019. --cluster=east-cluster Cluster name. --replication-set=rs1 Replication set name. --custom-labels=CUSTOM-LABELS Custom user-assigned labels. Example: region=east,app=app1 . --metrics-mode=auto Metrics flow mode, can be push : agent will push metrics, pull : server scrape metrics from agent or auto : chosen by server. --group=\"external\" Group name of external service. (Default: external .) pmm-admin add external-serverless [FLAGS] Add External Service on Remote node to monitoring. Usage example: pmm-admin add external-serverless --url=http://1.2.3.4:9093/metrics . Also, individual parameters can be set instead of --url like: pmm-admin add external-serverless --scheme=http --host=1.2.3.4 --listen-port=9093 --metrics-path=/metrics --container-name=ddd --external-name=e125 . Note that some parameters are mandatory depending on the context. For example, if you specify --url , --schema and other related parameters are not mandatory. But if you specify --host you must provide all other parameters needed to build the destination URL, or you can specify --address instead of host and port as individual parameters. FLAGS: --url=URL Full URL to exporter metrics endpoints. --scheme=https Scheme to generate URL to exporter metrics endpoints. --username=USERNAME External username. --password=PASSWORD External password. --address=1.2.3.4:9000 External exporter address and port. --host=1.2.3.4 External exporters hostname or IP address. --listen-port=9999 Listen port of external exporter for scraping metrics. --metrics-path=/metrics Path under which metrics are exposed, used to generate URL. --environment=testing Environment name. --cluster=CLUSTER Cluster name. --replication-set=rs1 Replication set name. --custom-labels='app=myapp,region=s1' Custom user-assigned labels. --group=\"external\" Group name of external service. (Default: external .) --machine-id=MACHINE-ID Node machine-id. --distro=DISTRO Node OS distribution. --container-id=CONTAINER-ID Container ID. --container-name=CONTAINER-NAME Container name. --node-model=NODE-MODEL Node model. --region=REGION Node region. --az=AZ Node availability zone. EXAMPLES \u00b6 pmm-admin add mysql --query-source = slowlog --username = pmm --password = pmm sl-mysql 127 .0.0.1:3306 MySQL Service added. Service ID : /service_id/a89191d4-7d75-44a9-b37f-a528e2c4550f Service name: sl-mysql pmm-admin add mysql --username = pmm --password = pmm --service-name = ps-mysql --host = 127 .0.0.1 --port = 3306 pmm-admin status pmm-admin status --wait = 30s Agent ID: /agent_id/c2a55ac6-a12f-4172-8850-4101237a4236 Node ID : /node_id/29b2cc24-3b90-4892-8d7e-4b44258d9309 PMM Server: URL : https://x.x.x.x:443/ Version: 2.5.0 PMM Client: Connected : true Time drift: 2.152715ms Latency : 465.658\u00b5s pmm-admin version: 2.5.0 pmm-agent version: 2.5.0 Agents: /agent_id/aeb42475-486c-4f48-a906-9546fc7859e8 mysql_slowlog_agent Running Disable collectors \u00b6 pmm-admin add mysql --disable-collectors = 'heartbeat,global_status,info_schema.innodb_cmp' --username = pmm --password = pmm --service-name = db1-mysql --host = 127 .0.0.1 --port = 3306 For other collectors that you can disable with the --disable-collectors option, please visit the official repositories for each exporter: node_exporter mysqld_exporter mongodb_exporter postgres_exporter proxysql_exporter","title":"pmm-admin - PMM Administration Tool"},{"location":"details/commands/pmm-admin.html#pmm-admin-pmm-administration-tool","text":"","title":"pmm-admin - PMM Administration Tool"},{"location":"details/commands/pmm-admin.html#name","text":"pmm-admin - Administer PMM","title":"NAME"},{"location":"details/commands/pmm-admin.html#synopsis","text":"pmm-admin [FLAGS] pmm-admin config [FLAGS] --server-url=server-url pmm-admin add DATABASE [FLAGS] [NAME] [ADDRESS] DATABASE:= [ MongoDB | MySQL | PostgreSQL | ProxySQL ] pmm-admin add --pmm-agent-listen-port=LISTEN_PORT DATABASE [FLAGS] [NAME] [ADDRESS] pmm-admin add external [FLAGS] [NAME] [ADDRESS] (CAUTION: Technical preview feature) pmm-admin add haproxy [FLAGS] [NAME] pmm-admin add external [FLAGS] [NAME] [ADDRESS] pmm-admin add external-serverless [FLAGS] [NAME] [ADDRESS] pmm-admin remove [FLAGS] service-type [service-name] pmm-admin register [FLAGS] [node-address] [node-type] [node-name] pmm-admin list [FLAGS] [node-address] pmm-admin status [FLAGS] [node-address] pmm-admin summary [FLAGS] [node-address] pmm-admin annotate [--node|--service] [--tags <tags>] [node-name|service-name] pmm-admin help [COMMAND]","title":"SYNOPSIS"},{"location":"details/commands/pmm-admin.html#description","text":"pmm-admin is a command-line tool for administering PMM using a set of COMMAND keywords and associated FLAGS. PMM communicates with the PMM Server via a PMM agent process.","title":"DESCRIPTION"},{"location":"details/commands/pmm-admin.html#common-flags","text":"-h , --help Show help and exit. --help-long Show extended help and exit. --help-man Generate man page. (Use pmm-admin --help-man | man -l - to view.) --debug Enable debug logging. --trace Enable trace logging (implies debug). --log-level (This parameter is available starting with PMM 2.29.0.) Set the level for the logs as per your requirement such as INFO, WARNING, ERROR, and FATAL. --json Enable JSON output. --version Show the application version and exit. --server-url=server-url PMM Server URL in https://username:password@pmm-server-host/ format. --server-insecure-tls Skip PMM Server TLS certificate validation. --group=<group-name> Group name for external services. Default: external","title":"COMMON FLAGS"},{"location":"details/commands/pmm-admin.html#commands","text":"","title":"COMMANDS"},{"location":"details/commands/pmm-admin.html#general-commands","text":"pmm-admin help [COMMAND] Show help for COMMAND .","title":"GENERAL COMMANDS"},{"location":"details/commands/pmm-admin.html#information-commands","text":"pmm-admin list --server-url=server-url [FLAGS] Show Services and Agents running on this Node, and the agent mode (push/pull). pmm-admin status --server-url=server-url [FLAGS] Show the following information about a local pmm-agent, and its connected server and clients: Agent: Agent ID, Node ID. PMM Server: URL and version. PMM Client: connection status, time drift, latency, vmagent status, pmm-admin version. Agents: Agent ID path and client name. FLAGS: --wait=<period><unit> Time to wait for a successful response from pmm-agent. period is an integer. unit is one of ms for milliseconds, s for seconds, m for minutes, h for hours. pmm-admin summary --server-url=server-url [FLAGS] Creates an archive file in the current directory with default file name summary_<hostname>_<year>_<month>_<date>_<hour>_<minute>_<second>.zip . The contents are two directories, client and server containing diagnostic text files. FLAGS: --filename=\"filename\" The Summary Archive filename. --skip-server Skip fetching logs.zip from PMM Server. --pprof (This parameter is available starting with PMM 2.29.0) Include performance profiling data in the summary.","title":"INFORMATION COMMANDS"},{"location":"details/commands/pmm-admin.html#configuration-commands","text":"","title":"CONFIGURATION COMMANDS"},{"location":"details/commands/pmm-admin.html#pmm-admin-config","text":"pmm-admin config [FLAGS] [node-address] [node-type] [node-name] Configure a local pmm-agent . FLAGS: --node-id=node-id Node ID (default is auto-detected). --node-model=node-model Node model. --region=region Node region. --az=availability-zone Node availability zone. --metrics-mode=mode Metrics flow mode for agents node-exporter. Allowed values: - auto : chosen by server (default). - push : agent will push metrics. - pull : server scrapes metrics from agent. --paths-base=dir Base path where all binaries, tools and collectors of PMM client are located --agent-password=password (This parameter i available starting with PMM 2.29.0.) Custom agent password.","title":"pmm-admin config"},{"location":"details/commands/pmm-admin.html#pmm-admin-register","text":"pmm-admin register [FLAGS] [node-address] [node-type] [node-name] Register the current Node with the PMM Server. --server-url=server-url PMM Server URL in https://username:password@pmm-server-host/ format. --machine-id=\"/machine_id/9812826a1c45454a98ba45c56cc4f5b0\" Node machine-id (default is auto-detected). --distro=\"linux\" Node OS distribution (default is auto-detected). --container-id=container-id Container ID. --container-name=container-name Container name. --node-model=node-model Node model. --region=region Node region. --az=availability-zone Node availability zone. --custom-labels=labels Custom user-assigned labels. --agent-password=password (This parameter is available starting with PMM 2.29.0.) Custom agent password.","title":"pmm-admin register"},{"location":"details/commands/pmm-admin.html#pmm-admin-add-pmm-agent-listen-portlisten_port","text":"pmm-admin add --pmm-agent-listen-port=LISTEN_PORT DATABASE [FLAGS] [NAME] [ADDRESS] Configure the PMM agent with a listen port. --pmm-agent-listen-port=LISTEN_PORT The PMM agent listen port. DATABASE:= [ MongoDB | MySQL | PostgreSQL | ProxySQL ]","title":"pmm-admin add --pmm-agent-listen-port=LISTEN_PORT"},{"location":"details/commands/pmm-admin.html#pmm-admin-remove","text":"pmm-admin remove [FLAGS] service-type [service-name] Remove Service from monitoring. --service-id=service-id Service ID. --force Remove service with that name or ID and all dependent services and agents. When you remove a service, collected data remains on PMM Server for the specified retention period .","title":"pmm-admin remove"},{"location":"details/commands/pmm-admin.html#pmm-admin-annotate","text":"pmm-admin annotate [--node|--service] <annotation> [--tags <tags>] [--node-name=<node>] [--service-name=<service>] Annotate an event. ( Read more ) <annotation> The annotation string. If it contains spaces, it should be quoted. --node Annotate the current node or that specified by --node-name . --service Annotate all services running on the current node, or that specified by --service-name . --tags A quoted string that defines one or more comma-separated tags for the annotation. Example: \"tag 1,tag 2\" . --node-name The node name being annotated. --service-name The service name being annotated. Combining flags Flags may be combined as shown in the following examples. --node Current node. --node-name Node with name. --node --node-name=NODE_NAME Node with name. --node --service-name Current node and service with name. --node --node-name --service-name Node with name and service with name. --node --service Current node and all services of current node. -node --node-name --service --service-name Service with name and node with name. --service All services of the current node. --service-name Service with name. --service --service-name Service with name. --service --node-name All services of current node and node with name. --service-name --node-name Service with name and node with name. --service --service-name -node-name Service with name and node with name. Tip If node or service name is specified, they are used instead of other parameters.","title":"pmm-admin annotate"},{"location":"details/commands/pmm-admin.html#database-commands","text":"","title":"DATABASE COMMANDS"},{"location":"details/commands/pmm-admin.html#mongodb","text":"pmm-admin add mongodb [FLAGS] [node-name] [node-address] Add MongoDB to monitoring. FLAGS: --node-id=node-id Node ID (default is auto-detected). --pmm-agent-id=pmm-agent-id The pmm-agent identifier which runs this instance (default is auto-detected). --username=username MongoDB username. --password=password MongoDB password. --agent-password=password Override the default password for accessing the /metrics endpoint. (Username is pmm and default password is the agent ID.) Avoid using special characters like \u2018', \u2018;\u2019 and \u2018$\u2019 in the custom password. --query-source=profiler Source of queries, one of: profiler , none (default: profiler ). --environment=environment Environment name. --cluster=cluster Cluster name. --replication-set=replication-set Replication set name. --custom-labels=custom-labels Custom user-assigned labels. --skip-connection-check Skip connection check. --tls Use TLS to connect to the database. --tls-skip-verify Skip TLS certificates validation. --tls-certificate-key-file=PATHTOCERT Path to TLS certificate file. --tls-certificate-key-file-password=IFPASSWORDTOCERTISSET Password for TLS certificate file. --tls-ca-file=PATHTOCACERT Path to certificate authority file. --metrics-mode=mode Metrics flow mode for agents node-exporter. Allowed values: - auto : chosen by server (default). - push : agent will push metrics. - pull : server scrapes metrics from agent.","title":"MongoDB"},{"location":"details/commands/pmm-admin.html#advanced-options","text":"PMM starts the MongoDB exporter by default only with diagnosticdata and replicasetstatus collectors enabled. FLAGS: --enable-all-collectors Enable all collectors. --disable-collectors Comma-separated list of collector names to exclude from exporter. --max-collections-limit=-1 Disable collstats, dbstats, topmetrics and indexstats if there are more than collections. 0: No limit. Default is -1, PMM automatically sets this value. A very high limit of max-collections-limit could impact the CPU and Memory usage. Check --stats-collections to limit the scope of collections and DB\u2019s metrics to be fetched. --stats-collections=db1,db2.col1 Collections for collstats & indexstats.","title":"Advanced Options"},{"location":"details/commands/pmm-admin.html#enable-all-collectors","text":"To enable all collectors, pass the parameter --enable-all-collectors in the pmm-admin add mongodb command. This will enable collstats , dbstats , indexstats , and topmetrics collectors.","title":"Enable all collectors"},{"location":"details/commands/pmm-admin.html#disable-some-collectors","text":"To enable only some collectors, pass the parameter --enable-all-collectors along with the parameter --disable-collectors . For example, if you want all collectors except topmetrics , specify: --enable-all-collectors --disable-collectors=topmetrics","title":"Disable some collectors"},{"location":"details/commands/pmm-admin.html#limit-dbstats-collstats-and-indexstats","text":"By default, PMM decides the limit for the number of collections to monitor the collStats and indexStats collectors. You can also set an additional limit for the collStats, indexStats, dbStats, and topmetrics collectors with the \u2013max-collections-limit parameter. Set the value of the parameter --max-collections-limit to: 0: which indicates that collStats and indexStats can handle unlimited collections. n, which indicates that collStats and indexStats can handle <=n collections. If the limit is crossed - exporter stops collecting monitoring data for the collStats and indexStats collectors. -1 (default) doesn\u2019t need to be explicitly set. It indicates that PMM decides how many collections it would monitor, currently <=200 (subject to change). To further limit collections to monitor, enable collStats and indexStats for some databases or collections: Specify the databases and collections that collStats and indexStats will use to collect data using the parameter --stats-collections . This parameter receives a comma-separated list of namespaces in the form database[.collection] .","title":"Limit dbStats, collStats and indexStats"},{"location":"details/commands/pmm-admin.html#examples","text":"To add MongoDB with all collectors ( diagnosticdata , replicasetstatus , collstats , dbstats , indexstats , and topmetrics ) with default limit detected by PMM (currently <=200 collections, but subject to change): pmm-admin add mongodb --username=admin --password=admin_pass --enable-all-collectors mongodb_srv_1 127.0.0.1:27017 To add MongoDB with all collectors (diagnosticdata, replicasetstatus, collstats, dbstats, indexstats, and topmetrics) with max-collections-limit set to 1000: pmm-admin add mongodb --username=admin --password=admin_pass --enable-all-collectors --max-collections-limit=1000 mongodb_srv_1 127.0.0.1:27017 To enable all the collectors with an unlimited number of collections monitored: pmm-admin add mongodb --username=admin --password=admin_pass --enable-all-collectors --max-collections-limit=0 mongodb_srv_1 127.0.0.1:27017 To add MongoDB with default collectors ( diagnosticdata and replicasetstatus ): pmm-admin add mongodb --username=admin --password=admin_pass mongodb_srv_1 127.0.0.1:27017 Disable collstats collector and enable all the others without limiting max-collections-limit : pmm-admin add mongodb --username=admin --password=admin_pass --enable-all-collectors --max-collections-limit=0 --disable-collectors=collstats mongodb_srv_1 127.0.0.1:27017 If --stats-collections=db1,db2.col1 then the collectors are run as follows: Database Collector is run on db1 All the collections db2 Only for collection col1 Enable all collectors and limit monitoring for dbstats , indexstats , collstats and topmetrics for all collections in db1 and col1 collection in db2 , without limiting max-collections-limit for a number of collections in db1 : pmm-admin add mongodb --username=admin --password=admin_pass --enable-all-collectors --max-collections-limit=0 --stats-collections=db1,db2.col1 mongodb_srv_1 127.0.0.1:27017","title":"Examples"},{"location":"details/commands/pmm-admin.html#resolutions","text":"PMM collects metrics in two resolutions to decrease CPU and Memory usage: high and low resolutions. In high resolution we collect metrics from collectors which work fast: - diagnosticdata - replicasetstatus - topmetrics In low resolution we collect metrics from collectors which could take some time: - dbstats - indexstats - collstats","title":"Resolutions"},{"location":"details/commands/pmm-admin.html#mysql","text":"pmm-admin add mysql [FLAGS] node-name node-address | [--name=service-name] --address=address[:port] | --socket Add MySQL to monitoring. FLAGS: --address MySQL address and port (default: 127.0.0.1:3306). --socket=socket Path to MySQL socket. (Find the socket path with mysql -u root -p -e \"select @@socket\" .) --node-id=node-id Node ID (default is auto-detected). --pmm-agent-id=pmm-agent-id The pmm-agent identifier which runs this instance (default is auto-detected). --username=username MySQL username. --password=password MySQL password. --agent-password=password Override the default password for accessing the /metrics endpoint. (Username is pmm and default password is the agent ID.) Avoid using special characters like \u2018', \u2018;\u2019 and \u2018$\u2019 in the custom password. --query-source=slowlog Source of SQL queries, one of: slowlog , perfschema , none (default: slowlog ). For slowlog query source you need change permissions for specific files. Root permissions are needed for this. --size-slow-logs=N Rotate slow log file at this size. If 0 , use server-defined default. Negative values disable log rotation. A unit suffix must be appended to the number and can be one of: KiB , MiB , GiB , TiB for base 2 units (1024, 1048576, etc). --disable-queryexamples Disable collection of query examples. --disable-tablestats Disable table statistics collection. Excluded collectors for low-resolution time intervals: --collect.auto_increment.columns --collect.info_schema.tables --collect.info_schema.tablestats --collect.perf_schema.indexiowaits --collect.perf_schema.tableiowaits --collect.perf_schema.file_instances Excluded collectors for medium-resolution time intervals: --collect.perf_schema.tablelocks --disable-tablestats-limit=disable-tablestats-limit Table statistics collection will be disabled if there are more than specified number of tables (default: server-defined). 0=no limit. Negative value disables collection. --environment=environment Environment name. --cluster=cluster Cluster name. --replication-set=replication-set Replication set name. --custom-labels=custom-labels Custom user-assigned labels. --skip-connection-check Skip connection check. --tls Use TLS to connect to the database. --tls-skip-verify Skip TLS certificates validation. --tls-cert-file=PATHTOCERT Path to TLS client certificate file. --tls-key=PATHTOCERTKEY Key for TLS client certificate file. --tls-ca-file=PATHTOCACERT Path to certificate authority file. --ssl-ca=PATHTOCACERT The path name of the Certificate Authority (CA) certificate file. If used must specify the same certificate used by the server. (-ssl-capath is similar but specifies the path name of a directory of CA certificate files.) --ssl-cert=PATHTOCERTKEY The path name of the client public key certificate file. --ssl-key The path name of the client private key file. --ssl-skip-verify Skip SSL certificate verification. --metrics-mode=mode Metrics flow mode for agents node-exporter. Allowed values: - auto : chosen by server (default). - push : agent will push metrics. - pull : server scrapes metrics from agent.","title":"MySQL"},{"location":"details/commands/pmm-admin.html#postgresql","text":"pmm-admin add postgresql [FLAGS] [node-name] [node-address] Add PostgreSQL to monitoring. FLAGS: --node-id=<node id> Node ID (default is auto-detected). --pmm-agent-id=<pmm agent id> The pmm-agent identifier which runs this instance (default is auto-detected). --username=<username> PostgreSQL username. --password=<password> PostgreSQL password. --database=<database> PostgreSQL database (default: postgres). --agent-password=password Override the default password for accessing the /metrics endpoint. (Username is pmm and default password is the agent ID.) Avoid using special characters like \u2018', \u2018;\u2019 and \u2018$\u2019 in the custom password. --query-source=<query source> Source of SQL queries, one of: pgstatements , pgstatmonitor , none (default: pgstatements ). --environment=<environment> Environment name. --cluster=<cluster> Cluster name. --replication-set=<replication set> Replication set name. --custom-labels=<custom labels> Custom user-assigned labels. --skip-connection-check Skip connection check. --tls Use TLS to connect to the database. --tls-skip-verify Skip TLS certificates validation. --tls-ca-file TLS CA certificate file. --tls-cert-file TLS certificate file. --tls-key-file TLS certificate key file. --metrics-mode=mode Metrics flow mode for agents node-exporter. Allowed values: - auto : chosen by server (default). - push : agent will push metrics. - pull : server scrapes metrics from agent.","title":"PostgreSQL"},{"location":"details/commands/pmm-admin.html#proxysql","text":"pmm-admin add proxysql [FLAGS] [node-name] [node-address] Add ProxySQL to monitoring. FLAGS: --node-id=node-id Node ID (default is auto-detected). --pmm-agent-id=pmm-agent-id The pmm-agent identifier which runs this instance (default is auto-detected). --username=username ProxySQL username. --password=password ProxySQL password. --agent-password=password Override the default password for accessing the /metrics endpoint. (Username is pmm and default password is the agent ID.) Avoid using special characters like \u2018', \u2018;\u2019 and \u2018$\u2019 in the custom password. --environment=environment Environment name. --cluster=cluster Cluster name. --replication-set=replication-set Replication set name. --custom-labels=custom-labels Custom user-assigned labels. --skip-connection-check Skip connection check. --tls Use TLS to connect to the database. --tls-skip-verify Skip TLS certificates validation. --metrics-mode=mode Metrics flow mode for agents node-exporter. Allowed values: - auto : chosen by server (default). - push : agent will push metrics. - pull : server scrapes metrics from agent. --disable-collectors Comma-separated list of collector names to exclude from exporter.","title":"ProxySQL"},{"location":"details/commands/pmm-admin.html#haproxy","text":"pmm-admin add haproxy [FLAGS] [NAME] Add HAProxy to monitoring. FLAGS: --server-url=SERVER-URL PMM Server URL in https://username:password@pmm-server-host/ format. --server-insecure-tls Skip PMM Server TLS certificate validation. --username=USERNAME HAProxy username. --password=PASSWORD HAProxy password. --scheme=SCHEME Scheme to generate URI to exporter metrics endpoints (http or https). --metrics-path=METRICS-PATH Path under which metrics are exposed, used to generate URI (default: /metrics). --listen-port=LISTEN-PORT Listen port of haproxy exposing the metrics for scraping metrics (Required). --service-node-id=SERVICE-NODE-ID Node ID where service runs (default is auto-detected). --environment=ENVIRONMENT Environment name like \u2018production\u2019 or \u2018qa\u2019. --cluster=CLUSTER Cluster name. --replication-set=REPLICATION-SET Replication set name. --custom-labels=CUSTOM-LABELS Custom user-assigned labels. Example: region=east,app=app1. --metrics-mode=MODE Metrics flow mode for agents node-exporter. Allowed values: - auto : chosen by server (default). - push : agent will push metrics. - pull : server scrapes metrics from agent. --skip-connection-check Skip connection check.","title":"HAProxy"},{"location":"details/commands/pmm-admin.html#other-commands","text":"pmm-admin add external [FLAGS] Add External source of data (like a custom exporter running on a port) to be monitored. FLAGS: --service-name=\"current-hostname\" Service name (autodetected defaults to the hostname where pmm-admin is running). --agent-node-id=AGENT-NODE-ID Node ID where agent runs (default is autodetected). --username=USERNAME External username. --password=PASSWORD External password. --scheme=http or https Scheme to generate URI to exporter metrics endpoints. --metrics-path=/metrics Path under which metrics are exposed, used to generate URI. --listen-port=LISTEN-PORT Listen port of external exporter for scraping metrics. (Required.) --service-node-id=SERVICE-NODE-ID Node ID where service runs (default is autodetected). --environment=prod Environment name like \u2018production\u2019 or \u2018qa\u2019. --cluster=east-cluster Cluster name. --replication-set=rs1 Replication set name. --custom-labels=CUSTOM-LABELS Custom user-assigned labels. Example: region=east,app=app1 . --metrics-mode=auto Metrics flow mode, can be push : agent will push metrics, pull : server scrape metrics from agent or auto : chosen by server. --group=\"external\" Group name of external service. (Default: external .) pmm-admin add external-serverless [FLAGS] Add External Service on Remote node to monitoring. Usage example: pmm-admin add external-serverless --url=http://1.2.3.4:9093/metrics . Also, individual parameters can be set instead of --url like: pmm-admin add external-serverless --scheme=http --host=1.2.3.4 --listen-port=9093 --metrics-path=/metrics --container-name=ddd --external-name=e125 . Note that some parameters are mandatory depending on the context. For example, if you specify --url , --schema and other related parameters are not mandatory. But if you specify --host you must provide all other parameters needed to build the destination URL, or you can specify --address instead of host and port as individual parameters. FLAGS: --url=URL Full URL to exporter metrics endpoints. --scheme=https Scheme to generate URL to exporter metrics endpoints. --username=USERNAME External username. --password=PASSWORD External password. --address=1.2.3.4:9000 External exporter address and port. --host=1.2.3.4 External exporters hostname or IP address. --listen-port=9999 Listen port of external exporter for scraping metrics. --metrics-path=/metrics Path under which metrics are exposed, used to generate URL. --environment=testing Environment name. --cluster=CLUSTER Cluster name. --replication-set=rs1 Replication set name. --custom-labels='app=myapp,region=s1' Custom user-assigned labels. --group=\"external\" Group name of external service. (Default: external .) --machine-id=MACHINE-ID Node machine-id. --distro=DISTRO Node OS distribution. --container-id=CONTAINER-ID Container ID. --container-name=CONTAINER-NAME Container name. --node-model=NODE-MODEL Node model. --region=REGION Node region. --az=AZ Node availability zone.","title":"OTHER COMMANDS"},{"location":"details/commands/pmm-admin.html#examples_1","text":"pmm-admin add mysql --query-source = slowlog --username = pmm --password = pmm sl-mysql 127 .0.0.1:3306 MySQL Service added. Service ID : /service_id/a89191d4-7d75-44a9-b37f-a528e2c4550f Service name: sl-mysql pmm-admin add mysql --username = pmm --password = pmm --service-name = ps-mysql --host = 127 .0.0.1 --port = 3306 pmm-admin status pmm-admin status --wait = 30s Agent ID: /agent_id/c2a55ac6-a12f-4172-8850-4101237a4236 Node ID : /node_id/29b2cc24-3b90-4892-8d7e-4b44258d9309 PMM Server: URL : https://x.x.x.x:443/ Version: 2.5.0 PMM Client: Connected : true Time drift: 2.152715ms Latency : 465.658\u00b5s pmm-admin version: 2.5.0 pmm-agent version: 2.5.0 Agents: /agent_id/aeb42475-486c-4f48-a906-9546fc7859e8 mysql_slowlog_agent Running","title":"EXAMPLES"},{"location":"details/commands/pmm-admin.html#disable-collectors","text":"pmm-admin add mysql --disable-collectors = 'heartbeat,global_status,info_schema.innodb_cmp' --username = pmm --password = pmm --service-name = db1-mysql --host = 127 .0.0.1 --port = 3306 For other collectors that you can disable with the --disable-collectors option, please visit the official repositories for each exporter: node_exporter mysqld_exporter mongodb_exporter postgres_exporter proxysql_exporter","title":"Disable collectors"},{"location":"details/commands/pmm-agent.html","text":"pmm-agent - PMM Client agent \u00b6 NAME \u00b6 pmm-agent - The PMM Client daemon program. SYNOPSIS \u00b6 pmm-agent [command] [options] DESCRIPTION \u00b6 pmm-agent , part of the PMM Client package, runs as a daemon process on all monitored hosts. COMMANDS \u00b6 pmm-agent run Run pmm-agent (default). pmm-agent setup [node-address] [node-type] [node-name] Configure local pmm-agent (requires root permissions) pmm-agent help [command] Show help (for command) and exit. OPTIONS AND ENVIRONMENT \u00b6 Most options can be set via environment variables (shown in parentheses). Option Environment variable Description --server-password=SERVER-PASSWORD PMM_AGENT_SERVER_PASSWORD Password to connect to PMM Server. --server-username=SERVER-USERNAME PMM_AGENT_SERVER_USERNAME Username to connect to PMM Server. --server-address=host:port PMM_AGENT_SERVER_ADDRESS PMM Server address and port number. --server-insecure-tls PMM_AGENT_SERVER_INSECURE_TLS Skip PMM Server TLS certificate validation. --az=AZ PMM_AGENT_SETUP_AZ Node availability zone. --config-file=path_to/pmm-agent.yaml PMM_AGENT_CONFIG_FILE Configuration file path and name. --container-id=CONTAINER-ID PMM_AGENT_SETUP_CONTAINER_ID Container ID. --container-name=CONTAINER-NAME PMM_AGENT_SETUP_CONTAINER_NAME Container name. --debug PMM_AGENT_DEBUG Enable debug output. --distro=distro PMM_AGENT_SETUP_DISTRO Node OS distribution (default is auto-detected). --force PMM_AGENT_SETUP_FORCE Remove Node with that name and all dependent Services and Agents (if existing). --id=/agent_id/... PMM_AGENT_ID ID of this pmm-agent. --listen-address=LISTEN-ADDRESS PMM_AGENT_LISTEN_ADDRESS Agent local API address. --listen-port=LISTEN-PORT PMM_AGENT_LISTEN_PORT Agent local API port. --machine-id=machine-id PMM_AGENT_SETUP_MACHINE_ID Node machine ID (default is auto-detected). --metrics-mode=auto PMM_AGENT_SETUP_METRICS_MODE Metrics flow mode for agents node-exporter. Can be push (agent will push metrics), pull (server scrapes metrics from agent) or auto (chosen by server). --node-model=NODE-MODEL PMM_AGENT_SETUP_NODE_MODEL Node model. --paths-base=PATH PMM_AGENT_PATHS_BASE Base path for PMM client, where all binaries, tools and collectors are located. If not set, default is /usr/local/percona/pmm2 . --paths-exporters_base=PATH PMM_AGENT_PATHS_EXPORTERS_BASE Base path for exporters to use. If not set, or set to a relative path, uses value of --paths-base prepended to it. --paths-mongodb_exporter=PATH PMM_AGENT_PATHS_MONGODB_EXPORTER Path to mongodb_exporter . --paths-mysqld_exporter=PATH PMM_AGENT_PATHS_MYSQLD_EXPORTER Path to mysqld_exporter . --paths-node_exporter=PATH PMM_AGENT_PATHS_NODE_EXPORTER Path to node_exporter . --paths-postgres_exporter=PATH PMM_AGENT_PATHS_POSTGRES_EXPORTER Path to postgres_exporter . --paths-proxysql_exporter=PATH PMM_AGENT_PATHS_PROXYSQL_EXPORTER Path to proxysql_exporter . --paths-pt-summary=PATH PMM_AGENT_PATHS_PT_SUMMARY Path to pt-summary . --paths-pt-mysql-summary=PATH PMM_AGENT_PATHS_PT_MYSQL_SUMMARY Path to pt-mysql-summary . --paths-pt-pg-summary=PATH PMM_AGENT_PATHS_PT_PG_SUMMARY Path to pt-pg-summary . --paths-tempdir=PATH PMM_AGENT_PATHS_TEMPDIR Temporary directory for exporters. --ports-max=PORTS-MAX PMM_AGENT_PORTS_MAX Highest allowed port number for listening sockets. --ports-min=PORTS-MIN PMM_AGENT_PORTS_MIN Lowest allowed port number for listening sockets. --region=REGION PMM_AGENT_SETUP_REGION Node region. --skip-registration PMM_AGENT_SETUP_SKIP_REGISTRATION Skip registration on PMM Server. --trace PMM_AGENT_TRACE Enable trace output (implies --debug ). -h , --help Show help (synonym for pmm-agent help ). --version Show application version, PMM version, time-stamp, git commit hash and branch. USAGE AND EXAMPLES OF paths-base FLAG \u00b6 Since 2.23.0 this flag could be used for easier setup of pmm agent. With this flag the root permissions for PMM client aren\u2019t needed anymore and it will be fully working. Examples: Case 1: There are no root permissions for /usr/local/percona/pmm2 folder or there is a need to change default folder for PMM files. Command: pmm-agent setup --paths-base=/home/user/custom/pmm2 --config-file=pmm-agent-dev.yaml --server-insecure-tls --server-address=127.0.0.1:443 --server-username=admin --server-password=admin Config output: # Updated by `pmm-agent setup`. --- id: /agent_id/be568008-b1b4-4bd9-98c7-392d1f4b724e listen-address: 127.0.0.1 listen-port: 7777 server: address: 127.0.0.1:443 username: admin password: admin insecure-tls: true paths: paths_base: /home/user/custom/pmm2 exporters_base: /home/user/custom/pmm2/exporters node_exporter: /home/user/custom/pmm2/exporters/node_exporter mysqld_exporter: /home/user/custom/pmm2/exporters/mysqld_exporter mongodb_exporter: /home/user/custom/pmm2/exporters/mongodb_exporter postgres_exporter: /home/user/custom/pmm2/exporters/postgres_exporter proxysql_exporter: /home/user/custom/pmm2/exporters/proxysql_exporter rds_exporter: /home/user/custom/pmm2/exporters/rds_exporter azure_exporter: /home/user/custom/pmm2/exporters/azure_exporter vmagent: /home/user/custom/pmm2/exporters/vmagent tempdir: /tmp pt_summary: /home/user/custom/pmm2/tools/pt-summary pt_pg_summary: /home/user/custom/pmm2/tools/pt-pg-summary pt_mysql_summary: /home/user/custom/pmm2/tools/pt-mysql-summary pt_mongodb_summary: /home/user/custom/pmm2/tools/pt-mongodb-summary ports: min: 42000 max: 51999 debug: false trace: false As could be seen above, base for all exporters and tools was changed only by setting --paths-base . With this tag the folder for PMM that doesn\u2019t require root access could be specified. Case 2: The older --paths-exporters_base flag could be passed along with the --paths-base Command: pmm-agent setup --paths-base=/home/user/custom/pmm2 --paths-exporters_base=/home/user/exporters --config-file=pmm-agent-dev.yaml --server-insecure-tls --server-address=127.0.0.1:443 --server-username=admin --server-password=admin Config output: # Updated by `pmm-agent setup`. --- id: /agent_id/afce1917-8836-4857-b3e5-ad372c2ddbe5 listen-address: 127.0.0.1 listen-port: 7777 server: address: 127.0.0.1:443 username: admin password: admin insecure-tls: true paths: paths_base: /home/user/custom/pmm2 exporters_base: /home/user/exporters node_exporter: /home/user/exporters/node_exporter mysqld_exporter: /home/user/exporters/mysqld_exporter mongodb_exporter: /home/user/exporters/mongodb_exporter postgres_exporter: /home/user/exporters/postgres_exporter proxysql_exporter: /home/user/exporters/proxysql_exporter rds_exporter: /home/user/exporters/rds_exporter azure_exporter: /home/user/exporters/azure_exporter vmagent: /home/user/exporters/vmagent tempdir: /tmp pt_summary: /home/user/custom/pmm2/tools/pt-summary pt_pg_summary: /home/user/custom/pmm2/tools/pt-pg-summary pt_mysql_summary: /home/user/custom/pmm2/tools/pt-mysql-summary pt_mongodb_summary: /home/user/custom/pmm2/tools/pt-mongodb-summary ports: min: 42000 max: 51999 debug: false trace: false As could be seen above the behavior for the --paths-base was the same, but paths for all exporters were overwritten by the --paths-exporter_base flag. Summary: Flag --paths-base will set path for all exporters and tools, but each one could be overridden by specific flag (like --paths-mongodb_exporter , --paths-pt-mysql-summary and etc). LOGGING \u00b6 By default, pmm-agent sends messages to stderr and to the system log ( syslogd or journald on Linux). To get a separate log file, edit the pmm-agent start-up script. systemd -based systems Script file: /usr/lib/systemd/system/pmm-agent.service Parameter: StandardError Default value: file:/var/log/pmm-agent.log Example: StandardError = file:/var/log/pmm-agent.log initd -based systems Script file: /etc/init.d/pmm-agent Parameter: pmm_log Default value: /var/log/pmm-agent.log Example: pmm_log = \"/var/log/pmm-agent.log\" If you change the default log file name, reflect the change in the log rotation rules file /etc/logrotate.d/pmm-agent-logrotate .","title":"pmm-agent - PMM Client agent"},{"location":"details/commands/pmm-agent.html#pmm-agent-pmm-client-agent","text":"","title":"pmm-agent - PMM Client agent"},{"location":"details/commands/pmm-agent.html#name","text":"pmm-agent - The PMM Client daemon program.","title":"NAME"},{"location":"details/commands/pmm-agent.html#synopsis","text":"pmm-agent [command] [options]","title":"SYNOPSIS"},{"location":"details/commands/pmm-agent.html#description","text":"pmm-agent , part of the PMM Client package, runs as a daemon process on all monitored hosts.","title":"DESCRIPTION"},{"location":"details/commands/pmm-agent.html#commands","text":"pmm-agent run Run pmm-agent (default). pmm-agent setup [node-address] [node-type] [node-name] Configure local pmm-agent (requires root permissions) pmm-agent help [command] Show help (for command) and exit.","title":"COMMANDS"},{"location":"details/commands/pmm-agent.html#options-and-environment","text":"Most options can be set via environment variables (shown in parentheses). Option Environment variable Description --server-password=SERVER-PASSWORD PMM_AGENT_SERVER_PASSWORD Password to connect to PMM Server. --server-username=SERVER-USERNAME PMM_AGENT_SERVER_USERNAME Username to connect to PMM Server. --server-address=host:port PMM_AGENT_SERVER_ADDRESS PMM Server address and port number. --server-insecure-tls PMM_AGENT_SERVER_INSECURE_TLS Skip PMM Server TLS certificate validation. --az=AZ PMM_AGENT_SETUP_AZ Node availability zone. --config-file=path_to/pmm-agent.yaml PMM_AGENT_CONFIG_FILE Configuration file path and name. --container-id=CONTAINER-ID PMM_AGENT_SETUP_CONTAINER_ID Container ID. --container-name=CONTAINER-NAME PMM_AGENT_SETUP_CONTAINER_NAME Container name. --debug PMM_AGENT_DEBUG Enable debug output. --distro=distro PMM_AGENT_SETUP_DISTRO Node OS distribution (default is auto-detected). --force PMM_AGENT_SETUP_FORCE Remove Node with that name and all dependent Services and Agents (if existing). --id=/agent_id/... PMM_AGENT_ID ID of this pmm-agent. --listen-address=LISTEN-ADDRESS PMM_AGENT_LISTEN_ADDRESS Agent local API address. --listen-port=LISTEN-PORT PMM_AGENT_LISTEN_PORT Agent local API port. --machine-id=machine-id PMM_AGENT_SETUP_MACHINE_ID Node machine ID (default is auto-detected). --metrics-mode=auto PMM_AGENT_SETUP_METRICS_MODE Metrics flow mode for agents node-exporter. Can be push (agent will push metrics), pull (server scrapes metrics from agent) or auto (chosen by server). --node-model=NODE-MODEL PMM_AGENT_SETUP_NODE_MODEL Node model. --paths-base=PATH PMM_AGENT_PATHS_BASE Base path for PMM client, where all binaries, tools and collectors are located. If not set, default is /usr/local/percona/pmm2 . --paths-exporters_base=PATH PMM_AGENT_PATHS_EXPORTERS_BASE Base path for exporters to use. If not set, or set to a relative path, uses value of --paths-base prepended to it. --paths-mongodb_exporter=PATH PMM_AGENT_PATHS_MONGODB_EXPORTER Path to mongodb_exporter . --paths-mysqld_exporter=PATH PMM_AGENT_PATHS_MYSQLD_EXPORTER Path to mysqld_exporter . --paths-node_exporter=PATH PMM_AGENT_PATHS_NODE_EXPORTER Path to node_exporter . --paths-postgres_exporter=PATH PMM_AGENT_PATHS_POSTGRES_EXPORTER Path to postgres_exporter . --paths-proxysql_exporter=PATH PMM_AGENT_PATHS_PROXYSQL_EXPORTER Path to proxysql_exporter . --paths-pt-summary=PATH PMM_AGENT_PATHS_PT_SUMMARY Path to pt-summary . --paths-pt-mysql-summary=PATH PMM_AGENT_PATHS_PT_MYSQL_SUMMARY Path to pt-mysql-summary . --paths-pt-pg-summary=PATH PMM_AGENT_PATHS_PT_PG_SUMMARY Path to pt-pg-summary . --paths-tempdir=PATH PMM_AGENT_PATHS_TEMPDIR Temporary directory for exporters. --ports-max=PORTS-MAX PMM_AGENT_PORTS_MAX Highest allowed port number for listening sockets. --ports-min=PORTS-MIN PMM_AGENT_PORTS_MIN Lowest allowed port number for listening sockets. --region=REGION PMM_AGENT_SETUP_REGION Node region. --skip-registration PMM_AGENT_SETUP_SKIP_REGISTRATION Skip registration on PMM Server. --trace PMM_AGENT_TRACE Enable trace output (implies --debug ). -h , --help Show help (synonym for pmm-agent help ). --version Show application version, PMM version, time-stamp, git commit hash and branch.","title":"OPTIONS AND ENVIRONMENT"},{"location":"details/commands/pmm-agent.html#usage-and-examples-of-paths-base-flag","text":"Since 2.23.0 this flag could be used for easier setup of pmm agent. With this flag the root permissions for PMM client aren\u2019t needed anymore and it will be fully working. Examples: Case 1: There are no root permissions for /usr/local/percona/pmm2 folder or there is a need to change default folder for PMM files. Command: pmm-agent setup --paths-base=/home/user/custom/pmm2 --config-file=pmm-agent-dev.yaml --server-insecure-tls --server-address=127.0.0.1:443 --server-username=admin --server-password=admin Config output: # Updated by `pmm-agent setup`. --- id: /agent_id/be568008-b1b4-4bd9-98c7-392d1f4b724e listen-address: 127.0.0.1 listen-port: 7777 server: address: 127.0.0.1:443 username: admin password: admin insecure-tls: true paths: paths_base: /home/user/custom/pmm2 exporters_base: /home/user/custom/pmm2/exporters node_exporter: /home/user/custom/pmm2/exporters/node_exporter mysqld_exporter: /home/user/custom/pmm2/exporters/mysqld_exporter mongodb_exporter: /home/user/custom/pmm2/exporters/mongodb_exporter postgres_exporter: /home/user/custom/pmm2/exporters/postgres_exporter proxysql_exporter: /home/user/custom/pmm2/exporters/proxysql_exporter rds_exporter: /home/user/custom/pmm2/exporters/rds_exporter azure_exporter: /home/user/custom/pmm2/exporters/azure_exporter vmagent: /home/user/custom/pmm2/exporters/vmagent tempdir: /tmp pt_summary: /home/user/custom/pmm2/tools/pt-summary pt_pg_summary: /home/user/custom/pmm2/tools/pt-pg-summary pt_mysql_summary: /home/user/custom/pmm2/tools/pt-mysql-summary pt_mongodb_summary: /home/user/custom/pmm2/tools/pt-mongodb-summary ports: min: 42000 max: 51999 debug: false trace: false As could be seen above, base for all exporters and tools was changed only by setting --paths-base . With this tag the folder for PMM that doesn\u2019t require root access could be specified. Case 2: The older --paths-exporters_base flag could be passed along with the --paths-base Command: pmm-agent setup --paths-base=/home/user/custom/pmm2 --paths-exporters_base=/home/user/exporters --config-file=pmm-agent-dev.yaml --server-insecure-tls --server-address=127.0.0.1:443 --server-username=admin --server-password=admin Config output: # Updated by `pmm-agent setup`. --- id: /agent_id/afce1917-8836-4857-b3e5-ad372c2ddbe5 listen-address: 127.0.0.1 listen-port: 7777 server: address: 127.0.0.1:443 username: admin password: admin insecure-tls: true paths: paths_base: /home/user/custom/pmm2 exporters_base: /home/user/exporters node_exporter: /home/user/exporters/node_exporter mysqld_exporter: /home/user/exporters/mysqld_exporter mongodb_exporter: /home/user/exporters/mongodb_exporter postgres_exporter: /home/user/exporters/postgres_exporter proxysql_exporter: /home/user/exporters/proxysql_exporter rds_exporter: /home/user/exporters/rds_exporter azure_exporter: /home/user/exporters/azure_exporter vmagent: /home/user/exporters/vmagent tempdir: /tmp pt_summary: /home/user/custom/pmm2/tools/pt-summary pt_pg_summary: /home/user/custom/pmm2/tools/pt-pg-summary pt_mysql_summary: /home/user/custom/pmm2/tools/pt-mysql-summary pt_mongodb_summary: /home/user/custom/pmm2/tools/pt-mongodb-summary ports: min: 42000 max: 51999 debug: false trace: false As could be seen above the behavior for the --paths-base was the same, but paths for all exporters were overwritten by the --paths-exporter_base flag. Summary: Flag --paths-base will set path for all exporters and tools, but each one could be overridden by specific flag (like --paths-mongodb_exporter , --paths-pt-mysql-summary and etc).","title":"USAGE AND EXAMPLES OF paths-base FLAG"},{"location":"details/commands/pmm-agent.html#logging","text":"By default, pmm-agent sends messages to stderr and to the system log ( syslogd or journald on Linux). To get a separate log file, edit the pmm-agent start-up script. systemd -based systems Script file: /usr/lib/systemd/system/pmm-agent.service Parameter: StandardError Default value: file:/var/log/pmm-agent.log Example: StandardError = file:/var/log/pmm-agent.log initd -based systems Script file: /etc/init.d/pmm-agent Parameter: pmm_log Default value: /var/log/pmm-agent.log Example: pmm_log = \"/var/log/pmm-agent.log\" If you change the default log file name, reflect the change in the log rotation rules file /etc/logrotate.d/pmm-agent-logrotate .","title":"LOGGING"},{"location":"details/dashboards/index.html","text":"Dashboards \u00b6 Category Dashboard Elements Insight Advanced Data Exploration 7 Insight Home Dashboard 26 Insight Experimental Home Dashboard 26 Insight Prometheus Exporter Status 57 Insight Prometheus Exporters Overview 27 Insight VictoriaMetrics 52 Insight VictoriaMetrics Agents Overview 58 PMM PMM Inventory 3 PMM Environment Overview 0 PMM Environment Summary 0 DBaas DB Cluster Summary 0 OS CPU Utilization Details 21 OS Disk Details 34 OS Network Details 70 OS Memory Details 116 OS Node Temperature Details 6 OS Nodes Compare 74 OS Nodes Overview 115 OS Node Summary 67 OS NUMA Details 72 OS Processes Details 35 Prometheus Prometheus Exporter Status 57 Prometheus Prometheus Exporters Overview 27 MySQL MySQL Amazon Aurora Details 20 MySQL MySQL Command/Handler Counters Compare 11 MySQL MySQL InnoDB Compression Details 41 MySQL MySQL InnoDB Details 339 MySQL MySQL MyISAM/Aria Details 55 MySQL MySQL MyRocks Details 101 MySQL MySQL Instance Summary 90 MySQL MySQL Instances Compare 70 MySQL MySQL Instances Overview 96 MySQL MySQL Wait Event Analyses Details 42 MySQL MySQL Performance Schema Details 48 MySQL MySQL Query Response Time Details 49 MySQL MySQL Replication Summary 50 MySQL MySQL Group Replication Summary 18 MySQL MySQL Table Details 45 MySQL MySQL User Details 62 MongoDB Experimental MongoDB Collection Overview 100 MongoDB Experimental MongoDB Collection Details 100 MongoDB Experimental MongoDB Oplog Details 100 MongoDB MongoDB Cluster Summary 55 MongoDB MongoDB Instance Summary 42 MongoDB MongoDB Instances Compare 19 MongoDB MongoDB ReplSet Summary 130 MongoDB MongoDB InMemory Details 46 MongoDB MongoDB MMAPv1 Details 52 MongoDB MongoDB WiredTiger Details 54 PostgreSQL PostgreSQL Instances Overview 114 PostgreSQL Experimental PostgreSQL Vacuum Monitoring 114 PostgreSQL PostgreSQL Instance Summary 67 PostgreSQL PostgreSQL Instances Compare 89 ProxySQL ProxySQL Instance Summary 55 High-availability PXC/Galera Node Summary 32 High-availability PXC/Galera Cluster Summary 19 High-availability Experimental PXC/Galera Cluster Summary 7 High-availability PXC/Galera Nodes Compare 55 High-availability HAProxy Instance Summary 113","title":"Dashboards"},{"location":"details/dashboards/index.html#dashboards","text":"Category Dashboard Elements Insight Advanced Data Exploration 7 Insight Home Dashboard 26 Insight Experimental Home Dashboard 26 Insight Prometheus Exporter Status 57 Insight Prometheus Exporters Overview 27 Insight VictoriaMetrics 52 Insight VictoriaMetrics Agents Overview 58 PMM PMM Inventory 3 PMM Environment Overview 0 PMM Environment Summary 0 DBaas DB Cluster Summary 0 OS CPU Utilization Details 21 OS Disk Details 34 OS Network Details 70 OS Memory Details 116 OS Node Temperature Details 6 OS Nodes Compare 74 OS Nodes Overview 115 OS Node Summary 67 OS NUMA Details 72 OS Processes Details 35 Prometheus Prometheus Exporter Status 57 Prometheus Prometheus Exporters Overview 27 MySQL MySQL Amazon Aurora Details 20 MySQL MySQL Command/Handler Counters Compare 11 MySQL MySQL InnoDB Compression Details 41 MySQL MySQL InnoDB Details 339 MySQL MySQL MyISAM/Aria Details 55 MySQL MySQL MyRocks Details 101 MySQL MySQL Instance Summary 90 MySQL MySQL Instances Compare 70 MySQL MySQL Instances Overview 96 MySQL MySQL Wait Event Analyses Details 42 MySQL MySQL Performance Schema Details 48 MySQL MySQL Query Response Time Details 49 MySQL MySQL Replication Summary 50 MySQL MySQL Group Replication Summary 18 MySQL MySQL Table Details 45 MySQL MySQL User Details 62 MongoDB Experimental MongoDB Collection Overview 100 MongoDB Experimental MongoDB Collection Details 100 MongoDB Experimental MongoDB Oplog Details 100 MongoDB MongoDB Cluster Summary 55 MongoDB MongoDB Instance Summary 42 MongoDB MongoDB Instances Compare 19 MongoDB MongoDB ReplSet Summary 130 MongoDB MongoDB InMemory Details 46 MongoDB MongoDB MMAPv1 Details 52 MongoDB MongoDB WiredTiger Details 54 PostgreSQL PostgreSQL Instances Overview 114 PostgreSQL Experimental PostgreSQL Vacuum Monitoring 114 PostgreSQL PostgreSQL Instance Summary 67 PostgreSQL PostgreSQL Instances Compare 89 ProxySQL ProxySQL Instance Summary 55 High-availability PXC/Galera Node Summary 32 High-availability PXC/Galera Cluster Summary 19 High-availability Experimental PXC/Galera Cluster Summary 7 High-availability PXC/Galera Nodes Compare 55 High-availability HAProxy Instance Summary 113","title":"Dashboards"},{"location":"details/dashboards/dashboard-advanced-data-exploration.html","text":"Advanced Data Exploration \u00b6 The Advanced Data Exploration dashboard provides detailed information about the progress of a single Prometheus metric across one or more hosts. View actual metric values (Gauge) \u00b6 A gauge is a metric that represents a single numerical value that can arbitrarily go up and down. Gauges are typically used for measured values like temperatures or current memory usage, but also \u201ccounts\u201d that can go up and down, like the number of running goroutines. View Metric Rate of Change (Counter) \u00b6 A counter is a cumulative metric that represents a single numerical value that only ever goes up. A counter is typically used to count requests served, tasks completed, errors occurred, etc. Counters should not be used to expose current counts of items whose number can also go down, e.g. the number of currently running goroutines. Use gauges for this use case. Metric Rates \u00b6 Shows the number of samples Per second stored for a given interval in the time series. This dashboard supports metrics related to NUMA. The names of all these metrics start with node_memory_numa .","title":"Advanced Data Exploration"},{"location":"details/dashboards/dashboard-advanced-data-exploration.html#advanced-data-exploration","text":"The Advanced Data Exploration dashboard provides detailed information about the progress of a single Prometheus metric across one or more hosts.","title":"Advanced Data Exploration"},{"location":"details/dashboards/dashboard-advanced-data-exploration.html#view-actual-metric-values-gauge","text":"A gauge is a metric that represents a single numerical value that can arbitrarily go up and down. Gauges are typically used for measured values like temperatures or current memory usage, but also \u201ccounts\u201d that can go up and down, like the number of running goroutines.","title":"View actual metric values (Gauge)"},{"location":"details/dashboards/dashboard-advanced-data-exploration.html#view-metric-rate-of-change-counter","text":"A counter is a cumulative metric that represents a single numerical value that only ever goes up. A counter is typically used to count requests served, tasks completed, errors occurred, etc. Counters should not be used to expose current counts of items whose number can also go down, e.g. the number of currently running goroutines. Use gauges for this use case.","title":"View Metric Rate of Change (Counter)"},{"location":"details/dashboards/dashboard-advanced-data-exploration.html#metric-rates","text":"Shows the number of samples Per second stored for a given interval in the time series. This dashboard supports metrics related to NUMA. The names of all these metrics start with node_memory_numa .","title":"Metric Rates"},{"location":"details/dashboards/dashboard-cluster-summary.html","text":"DB Cluster Summmary \u00b6 Disclaimer This is an Experimental Dashboard that is not part of the official Percona Monitoring and Management (PMM) deployment and might be updated. We ship this Dashboard to obtain feedback from our users. This Dashboard is a part of DBaaS solution inside PMM. This Dashboard is designed to show the resource consumption inside Kubernetes (K8s) Cluster.","title":"DBaas"},{"location":"details/dashboards/dashboard-cluster-summary.html#db-cluster-summmary","text":"Disclaimer This is an Experimental Dashboard that is not part of the official Percona Monitoring and Management (PMM) deployment and might be updated. We ship this Dashboard to obtain feedback from our users. This Dashboard is a part of DBaaS solution inside PMM. This Dashboard is designed to show the resource consumption inside Kubernetes (K8s) Cluster.","title":"DB Cluster Summmary"},{"location":"details/dashboards/dashboard-cpu-utilization-details.html","text":"CPU Utilization Details \u00b6 Overall CPU Utilization \u00b6 The Overall CPU Utilization metric shows how much of the overall CPU time is used by the server. It has these components: Max Core Utilization No description System This component the proportion of time the CPUs spent inside the Linux kernel for operations like context switching, memory allocation and queue handling. User This component is the time spent in the user space. Normally, most of the MySQL CPU time is in user space. A high value of user time indicates a CPU bound workload. Softirq This component is the portion of time the CPU spent servicing software interrupts generated by the device drivers. A high value of softirq may indicates a poorly configured device. The network devices are generally the main source of high softirq values. Steal When multiple virtual machines share the same physical host, some virtual machines may be allowed to use more of their share of CPU and that CPU time is accounted as Steal by the virtual machine from which the time is taken. Iowait This component is the time the CPU spent waiting for disk IO requests to complete. A high value of iowait indicates a disk bound load. Nice No description In addition, sampling of the Max utilization of a single core is shown. This metric presents global values: while there may be a lot of unused CPU, a single core may be saturated. Look at the Max Core Utilization to see if any core is reaching close to 100%. Current CPU Threads Utilization \u00b6 This shows the total utilization of each CPU core along with the average utilization of all CPU cores. Watch for any core close to 100% utilization and investigate the root cause. CPU Threads Frequency \u00b6 No description Current CPU Cores Temperature \u00b6 No description Overall CPU Threads Utilization Details \u00b6 No description","title":"OS Dashboards"},{"location":"details/dashboards/dashboard-cpu-utilization-details.html#cpu-utilization-details","text":"","title":"CPU Utilization Details"},{"location":"details/dashboards/dashboard-cpu-utilization-details.html#overall-cpu-utilization","text":"The Overall CPU Utilization metric shows how much of the overall CPU time is used by the server. It has these components: Max Core Utilization No description System This component the proportion of time the CPUs spent inside the Linux kernel for operations like context switching, memory allocation and queue handling. User This component is the time spent in the user space. Normally, most of the MySQL CPU time is in user space. A high value of user time indicates a CPU bound workload. Softirq This component is the portion of time the CPU spent servicing software interrupts generated by the device drivers. A high value of softirq may indicates a poorly configured device. The network devices are generally the main source of high softirq values. Steal When multiple virtual machines share the same physical host, some virtual machines may be allowed to use more of their share of CPU and that CPU time is accounted as Steal by the virtual machine from which the time is taken. Iowait This component is the time the CPU spent waiting for disk IO requests to complete. A high value of iowait indicates a disk bound load. Nice No description In addition, sampling of the Max utilization of a single core is shown. This metric presents global values: while there may be a lot of unused CPU, a single core may be saturated. Look at the Max Core Utilization to see if any core is reaching close to 100%.","title":"Overall CPU Utilization"},{"location":"details/dashboards/dashboard-cpu-utilization-details.html#current-cpu-threads-utilization","text":"This shows the total utilization of each CPU core along with the average utilization of all CPU cores. Watch for any core close to 100% utilization and investigate the root cause.","title":"Current CPU Threads Utilization"},{"location":"details/dashboards/dashboard-cpu-utilization-details.html#cpu-threads-frequency","text":"No description","title":"CPU Threads Frequency"},{"location":"details/dashboards/dashboard-cpu-utilization-details.html#current-cpu-cores-temperature","text":"No description","title":"Current CPU Cores Temperature"},{"location":"details/dashboards/dashboard-cpu-utilization-details.html#overall-cpu-threads-utilization-details","text":"No description","title":"Overall CPU Threads Utilization Details"},{"location":"details/dashboards/dashboard-disk-details.html","text":"Disk Details \u00b6 mount point Usage \u00b6 Shows the percentage of disk space utilization for every mount point defined on the system. Having some of the mount points close to 100% space utilization is not good because of the risk of a \u201cdisk full\u201d error that can block one of the services or even cause a crash of the entire system. In cases where the mount point is close to 100% consider removing unused files or expanding the space allocated to the mount point. mount point \u00b6 Shows information about the disk space usage of the specified mount point. Used is the amount of space used. Free is the amount of space not in use. Used+Free is the total disk space allocated to the mount point. Having Free close to 0 B is not good because of the risk of a \u201cdisk full\u201d error that can block one of the services or even cause a crash of the entire system. In cases where Free is close to 0 B consider removing unused files or expanding the space allocated to the mount point. Disk Latency \u00b6 Shows average latency for Reads and Writes IO Devices. Higher than typical latency for highly loaded storage indicates saturation (overload) and is frequent cause of performance problems. Higher than normal latency also can indicate internal storage problems. Disk Operations \u00b6 Shows amount of physical IOs (reads and writes) different devices are serving. Spikes in number of IOs served often corresponds to performance problems due to IO subsystem overload. Disk Bandwidth \u00b6 Shows volume of reads and writes the storage is handling. This can be better measure of IO capacity usage for network attached and SSD storage as it is often bandwidth limited. Amount of data being written to the disk can be used to estimate Flash storage life time. Disk Load \u00b6 Shows how much disk was loaded for reads or writes as average number of outstanding requests at different period of time. High disk load is a good measure of actual storage utilization. Different storage types handle load differently - some will show latency increases on low loads others can handle higher load with no problems. Disk IO Utilization \u00b6 Shows disk Utilization as percent of the time when there was at least one IO request in flight. It is designed to match utilization available in iostat tool. It is not very good measure of true IO Capacity Utilization. Consider looking at IO latency and Disk Load Graphs instead. Avg Disks Operations Merge Ratio \u00b6 Shows how effectively Operating System is able to merge logical IO requests into physical requests. This is a good measure of the IO locality which can be used for workload characterization. Disk IO Size \u00b6 Shows average size of a single disk operation.","title":"Disk Details"},{"location":"details/dashboards/dashboard-disk-details.html#disk-details","text":"","title":"Disk Details"},{"location":"details/dashboards/dashboard-disk-details.html#mount-point-usage","text":"Shows the percentage of disk space utilization for every mount point defined on the system. Having some of the mount points close to 100% space utilization is not good because of the risk of a \u201cdisk full\u201d error that can block one of the services or even cause a crash of the entire system. In cases where the mount point is close to 100% consider removing unused files or expanding the space allocated to the mount point.","title":"mount point Usage"},{"location":"details/dashboards/dashboard-disk-details.html#mount-point","text":"Shows information about the disk space usage of the specified mount point. Used is the amount of space used. Free is the amount of space not in use. Used+Free is the total disk space allocated to the mount point. Having Free close to 0 B is not good because of the risk of a \u201cdisk full\u201d error that can block one of the services or even cause a crash of the entire system. In cases where Free is close to 0 B consider removing unused files or expanding the space allocated to the mount point.","title":"mount point"},{"location":"details/dashboards/dashboard-disk-details.html#disk-latency","text":"Shows average latency for Reads and Writes IO Devices. Higher than typical latency for highly loaded storage indicates saturation (overload) and is frequent cause of performance problems. Higher than normal latency also can indicate internal storage problems.","title":"Disk Latency"},{"location":"details/dashboards/dashboard-disk-details.html#disk-operations","text":"Shows amount of physical IOs (reads and writes) different devices are serving. Spikes in number of IOs served often corresponds to performance problems due to IO subsystem overload.","title":"Disk Operations"},{"location":"details/dashboards/dashboard-disk-details.html#disk-bandwidth","text":"Shows volume of reads and writes the storage is handling. This can be better measure of IO capacity usage for network attached and SSD storage as it is often bandwidth limited. Amount of data being written to the disk can be used to estimate Flash storage life time.","title":"Disk Bandwidth"},{"location":"details/dashboards/dashboard-disk-details.html#disk-load","text":"Shows how much disk was loaded for reads or writes as average number of outstanding requests at different period of time. High disk load is a good measure of actual storage utilization. Different storage types handle load differently - some will show latency increases on low loads others can handle higher load with no problems.","title":"Disk Load"},{"location":"details/dashboards/dashboard-disk-details.html#disk-io-utilization","text":"Shows disk Utilization as percent of the time when there was at least one IO request in flight. It is designed to match utilization available in iostat tool. It is not very good measure of true IO Capacity Utilization. Consider looking at IO latency and Disk Load Graphs instead.","title":"Disk IO Utilization"},{"location":"details/dashboards/dashboard-disk-details.html#avg-disks-operations-merge-ratio","text":"Shows how effectively Operating System is able to merge logical IO requests into physical requests. This is a good measure of the IO locality which can be used for workload characterization.","title":"Avg Disks Operations Merge Ratio"},{"location":"details/dashboards/dashboard-disk-details.html#disk-io-size","text":"Shows average size of a single disk operation.","title":"Disk IO Size"},{"location":"details/dashboards/dashboard-env-overview.html","text":"Environment Overview \u00b6 Disclaimer This is an Experimental Dashboard that is not part of the official Percona Monitoring and Management (PMM) deployment and might be updated. We ship this Dashboard to obtain feedback from our users. The Dashboard provides the user with a high-level view of all the environments in PMM.","title":"Environment Overview"},{"location":"details/dashboards/dashboard-env-overview.html#environment-overview","text":"Disclaimer This is an Experimental Dashboard that is not part of the official Percona Monitoring and Management (PMM) deployment and might be updated. We ship this Dashboard to obtain feedback from our users. The Dashboard provides the user with a high-level view of all the environments in PMM.","title":"Environment Overview"},{"location":"details/dashboards/dashboard-environent-summary.html","text":"Environment Summary \u00b6 Disclaimer This is an Experimental Dashboard that is not part of the official Percona Monitoring and Management (PMM) deployment and might be updated. We ship this Dashboard to obtain feedback from our users. The Environment Summary Dashboard provides an at-a-glance view specific to the selected environment in PMM, including an overview of the services and nodes running in that environment.","title":"Environment Summary"},{"location":"details/dashboards/dashboard-environent-summary.html#environment-summary","text":"Disclaimer This is an Experimental Dashboard that is not part of the official Percona Monitoring and Management (PMM) deployment and might be updated. We ship this Dashboard to obtain feedback from our users. The Environment Summary Dashboard provides an at-a-glance view specific to the selected environment in PMM, including an overview of the services and nodes running in that environment.","title":"Environment Summary"},{"location":"details/dashboards/dashboard-haproxy-instance-summary.html","text":"HAProxy Instance Summary \u00b6 No description.","title":"HAProxy Instance Summary"},{"location":"details/dashboards/dashboard-haproxy-instance-summary.html#haproxy-instance-summary","text":"No description.","title":"HAProxy Instance Summary"},{"location":"details/dashboards/dashboard-home-experimental.html","text":"Experimental Home Dashboard \u00b6 The experimental Home Dashboard provides a high level view of your environment. This new Home Dashboard displays data that is organized in panels as given below. Overview \u00b6 This panel lists all added hosts along with essential information about their performance. For each host, you can find the current values of the following metrics: CPU Busy Memory Available Disk Reads Disk Writes Network IO DB Connections DB QPS Virtual CPUs RAM Host Uptime DB Uptime Anomaly Detection \u00b6 This panel lists all the anomalies such as: CPU anomalies High CPU servers Disk Queue anomalies High disk queue Command Center \u00b6 This panel provides critical information such as CPU utlization, memory utilization, anomalies, read and write latency, etc., for your environment. Service Summary \u00b6 This panel provides the following information for the services being monitored: DB connections DB QPS (Query per sec) DB uptime","title":"Experimental Home Dashboard"},{"location":"details/dashboards/dashboard-home-experimental.html#experimental-home-dashboard","text":"The experimental Home Dashboard provides a high level view of your environment. This new Home Dashboard displays data that is organized in panels as given below.","title":"Experimental Home Dashboard"},{"location":"details/dashboards/dashboard-home-experimental.html#overview","text":"This panel lists all added hosts along with essential information about their performance. For each host, you can find the current values of the following metrics: CPU Busy Memory Available Disk Reads Disk Writes Network IO DB Connections DB QPS Virtual CPUs RAM Host Uptime DB Uptime","title":"Overview"},{"location":"details/dashboards/dashboard-home-experimental.html#anomaly-detection","text":"This panel lists all the anomalies such as: CPU anomalies High CPU servers Disk Queue anomalies High disk queue","title":"Anomaly Detection"},{"location":"details/dashboards/dashboard-home-experimental.html#command-center","text":"This panel provides critical information such as CPU utlization, memory utilization, anomalies, read and write latency, etc., for your environment.","title":"Command Center"},{"location":"details/dashboards/dashboard-home-experimental.html#service-summary","text":"This panel provides the following information for the services being monitored: DB connections DB QPS (Query per sec) DB uptime","title":"Service Summary"},{"location":"details/dashboards/dashboard-home.html","text":"Home Dashboard \u00b6 The Home Dashboard is a high-level overview of your environment, the starting page of the PMM portal from which you can open the tools of PMM, and browse to online resources. On the PMM home page, you can also find the version number and a button to update your PMM Server. General Information \u00b6 This section contains links to online resources, such as PMM documentation, releases notes, and blogs. Shared and Recently Used Dashboards \u00b6 This section is automatically updated to show the most recent dashboards that you worked with. It also contains the dashboards that you have bookmarked. Statistics \u00b6 This section shows the total number of hosts added to PMM and the total number of database instances being monitored. This section also displays the current version number. Use the Upgrade to X.X.X version to upgrade to the most recent version of PMM. Environment Overview \u00b6 This section lists all added hosts along with essential information about their performance. For each host, you can find the current values of the following metrics: CPU Busy Memory Available Disk Reads Disk Writes Network IO DB Connections DB QPS Virtual CPUs RAM Host Uptime DB Uptime","title":"Insight"},{"location":"details/dashboards/dashboard-home.html#home-dashboard","text":"The Home Dashboard is a high-level overview of your environment, the starting page of the PMM portal from which you can open the tools of PMM, and browse to online resources. On the PMM home page, you can also find the version number and a button to update your PMM Server.","title":"Home Dashboard"},{"location":"details/dashboards/dashboard-home.html#general-information","text":"This section contains links to online resources, such as PMM documentation, releases notes, and blogs.","title":"General Information"},{"location":"details/dashboards/dashboard-home.html#shared-and-recently-used-dashboards","text":"This section is automatically updated to show the most recent dashboards that you worked with. It also contains the dashboards that you have bookmarked.","title":"Shared and Recently Used Dashboards"},{"location":"details/dashboards/dashboard-home.html#statistics","text":"This section shows the total number of hosts added to PMM and the total number of database instances being monitored. This section also displays the current version number. Use the Upgrade to X.X.X version to upgrade to the most recent version of PMM.","title":"Statistics"},{"location":"details/dashboards/dashboard-home.html#environment-overview","text":"This section lists all added hosts along with essential information about their performance. For each host, you can find the current values of the following metrics: CPU Busy Memory Available Disk Reads Disk Writes Network IO DB Connections DB QPS Virtual CPUs RAM Host Uptime DB Uptime","title":"Environment Overview"},{"location":"details/dashboards/dashboard-inventory.html","text":"PMM Inventory \u00b6 The Inventory dashboard is a high level overview of all objects registered by PMM. To see it select Configuration \u2192 PMM Inventory \u2192 Inventory list . Inventory objects form a hierarchy with Node at the top, then Service and Agents assigned to a Node. There are three tabs where items for each type are listed with their details: Services Individual service names and where they run, against which agents will be assigned. Each instance of a service gets a service_id value that is related to a node_id . Examples are MySQL, Amazon Aurora MySQL. This feature also allows to support multiple mysqld instances on a single node, with different service names, e.g. mysql1-3306 , and mysql1-3307 . Agents Each binary (exporter, agent) running on a client will get an agent_id value. Examples: pmm-agent one is the top of the tree, assigned to a node_id node_exporter is assigned to pmm-agent agent_id mysqld_exporter and QAN MySQL Perfschema are assigned to a service_id . Nodes Where the service and agents will run. Assigned a node_id , associated with a machine_id (from /etc/machine-id ). Some examples are bare metal, virtualized, container. Removing items from the inventory \u00b6 You can remove items from the inventory. Select Configuration \u2192 PMM Inventory \u2192 Inventory list . In the first column, select the items to be removed. Click Delete . The interface will ask you to confirm the operation:","title":"PMM Inventory"},{"location":"details/dashboards/dashboard-inventory.html#pmm-inventory","text":"The Inventory dashboard is a high level overview of all objects registered by PMM. To see it select Configuration \u2192 PMM Inventory \u2192 Inventory list . Inventory objects form a hierarchy with Node at the top, then Service and Agents assigned to a Node. There are three tabs where items for each type are listed with their details: Services Individual service names and where they run, against which agents will be assigned. Each instance of a service gets a service_id value that is related to a node_id . Examples are MySQL, Amazon Aurora MySQL. This feature also allows to support multiple mysqld instances on a single node, with different service names, e.g. mysql1-3306 , and mysql1-3307 . Agents Each binary (exporter, agent) running on a client will get an agent_id value. Examples: pmm-agent one is the top of the tree, assigned to a node_id node_exporter is assigned to pmm-agent agent_id mysqld_exporter and QAN MySQL Perfschema are assigned to a service_id . Nodes Where the service and agents will run. Assigned a node_id , associated with a machine_id (from /etc/machine-id ). Some examples are bare metal, virtualized, container.","title":"PMM Inventory"},{"location":"details/dashboards/dashboard-inventory.html#removing-items-from-the-inventory","text":"You can remove items from the inventory. Select Configuration \u2192 PMM Inventory \u2192 Inventory list . In the first column, select the items to be removed. Click Delete . The interface will ask you to confirm the operation:","title":"Removing items from the inventory"},{"location":"details/dashboards/dashboard-manage-dashboards.html","text":"Manage dashboards \u00b6 This section describes how to manage your PMM dashboards and the widgets on those dashboards, including: Creating dashboard folders Managing dashboard folders Setting custom Home dashboard Create dashboard folders \u00b6 Folders help you organize and group PMM dashboards, which is crucial when you have multiple dashboards or teams using the same PMM instance. Note To create a dashboard folder, you must have PMM\u2019s Admin privileges. To create a dashboard folder: On the PMM dashboards page, from the side menu, go to Dashboards > New folder . Enter a unique name for your folder and click Create . Managing dashboard folders \u00b6 This section describes how to delete multiple dashboards, move dashboards from one folder to another and navigate to a folder page where you can assign folder and dashboard permissions. Delete multiple dashboards \u00b6 To delete multiple dashboards at once: From the side menu, go to Dashboards > Browse and check the dashboards that you want to delete, and click Delete . Move dashboards from one folder to another \u00b6 You can move dashboards from one folder to another in the following two ways: From the side menu, go to Dashboards > Browse and check the dashboards that you want to move. Click Move . On the Choose Dashboard Folder dialog box select the dashboards that you want to move from the drop-down. Click Move . The other way of moving dashboards from one folder to another is: Open the dashboard that you want to move to another folder. Click on icon to open Dashboard Settings . On the General page, under Folder select the folder name that you want to move from the dropdown. Click Save Dashboard on the the left to save the change. Note You should have atleast an Editor role to move a dashboard. Navigate to a dashboard folder page to assign permissions \u00b6 From the side menu, go to Dashboards > Browse and hover over the dashboard folder whose permissions you want to set. Click Go to Folder . Go to the Permissions tab and select the requisite permission from the drop-down for the various roles. Setting custom Home Dashboard \u00b6 The home dashboard you set is the dashboard all the users will see after logging in to PMM UI. You can set the home dashboard for a server, an organization, a team, or your user account. Set home dashboard for your organization \u00b6 Organization Admins can set the home dashboard for their organization. For information on managing users in an organization, see Manage Users Navigate to the dashboard that you want to set as the home dashboard. Click the star next to the dashboard title to mark the dashboard as a favorite. Hover your cursor over Configuration Click Preferences . In the Home Dashboard field, select the dashboard that you want to set as your home dashboard. Click Save . Set home dashboard for your team \u00b6 Organization and team Admins can set the home dashboard for their team as follows: Navigate to the dashboard that you want to set as your home dashboard. Click star next to the dashboard to mark the dashboard as a favorite. On the main menu, hover your cursor over Configuration . Click Teams . Grafana displays the team list. Click on the team for whom you want to set the home dashboard and then navigate to the Settings tab. In the Home Dashboard field, select the dashboard that you want to use for your home dashboard. Click Save . Set your Personal Home Dashboard \u00b6 From the main menu, go to Dashboards > Browse and select the dashboard you want to set as your home dashboard. Click the star next to the dashboard title to mark it as a favorite. From the side menu go to Configuration > Preferences . In the Home Dashboard field, select the dashboard that you want to set as your home dashboard. Click Save .","title":"Manage dashboards"},{"location":"details/dashboards/dashboard-manage-dashboards.html#manage-dashboards","text":"This section describes how to manage your PMM dashboards and the widgets on those dashboards, including: Creating dashboard folders Managing dashboard folders Setting custom Home dashboard","title":"Manage dashboards"},{"location":"details/dashboards/dashboard-manage-dashboards.html#create-dashboard-folders","text":"Folders help you organize and group PMM dashboards, which is crucial when you have multiple dashboards or teams using the same PMM instance. Note To create a dashboard folder, you must have PMM\u2019s Admin privileges. To create a dashboard folder: On the PMM dashboards page, from the side menu, go to Dashboards > New folder . Enter a unique name for your folder and click Create .","title":"Create dashboard folders"},{"location":"details/dashboards/dashboard-manage-dashboards.html#managing-dashboard-folders","text":"This section describes how to delete multiple dashboards, move dashboards from one folder to another and navigate to a folder page where you can assign folder and dashboard permissions.","title":"Managing dashboard folders"},{"location":"details/dashboards/dashboard-manage-dashboards.html#delete-multiple-dashboards","text":"To delete multiple dashboards at once: From the side menu, go to Dashboards > Browse and check the dashboards that you want to delete, and click Delete .","title":"Delete multiple dashboards"},{"location":"details/dashboards/dashboard-manage-dashboards.html#move-dashboards-from-one-folder-to-another","text":"You can move dashboards from one folder to another in the following two ways: From the side menu, go to Dashboards > Browse and check the dashboards that you want to move. Click Move . On the Choose Dashboard Folder dialog box select the dashboards that you want to move from the drop-down. Click Move . The other way of moving dashboards from one folder to another is: Open the dashboard that you want to move to another folder. Click on icon to open Dashboard Settings . On the General page, under Folder select the folder name that you want to move from the dropdown. Click Save Dashboard on the the left to save the change. Note You should have atleast an Editor role to move a dashboard.","title":"Move dashboards from one folder to another"},{"location":"details/dashboards/dashboard-manage-dashboards.html#navigate-to-a-dashboard-folder-page-to-assign-permissions","text":"From the side menu, go to Dashboards > Browse and hover over the dashboard folder whose permissions you want to set. Click Go to Folder . Go to the Permissions tab and select the requisite permission from the drop-down for the various roles.","title":"Navigate to a dashboard folder page to assign permissions"},{"location":"details/dashboards/dashboard-manage-dashboards.html#setting-custom-home-dashboard","text":"The home dashboard you set is the dashboard all the users will see after logging in to PMM UI. You can set the home dashboard for a server, an organization, a team, or your user account.","title":"Setting custom Home Dashboard"},{"location":"details/dashboards/dashboard-manage-dashboards.html#set-home-dashboard-for-your-organization","text":"Organization Admins can set the home dashboard for their organization. For information on managing users in an organization, see Manage Users Navigate to the dashboard that you want to set as the home dashboard. Click the star next to the dashboard title to mark the dashboard as a favorite. Hover your cursor over Configuration Click Preferences . In the Home Dashboard field, select the dashboard that you want to set as your home dashboard. Click Save .","title":"Set home dashboard for your organization"},{"location":"details/dashboards/dashboard-manage-dashboards.html#set-home-dashboard-for-your-team","text":"Organization and team Admins can set the home dashboard for their team as follows: Navigate to the dashboard that you want to set as your home dashboard. Click star next to the dashboard to mark the dashboard as a favorite. On the main menu, hover your cursor over Configuration . Click Teams . Grafana displays the team list. Click on the team for whom you want to set the home dashboard and then navigate to the Settings tab. In the Home Dashboard field, select the dashboard that you want to use for your home dashboard. Click Save .","title":"Set home dashboard for your team"},{"location":"details/dashboards/dashboard-manage-dashboards.html#set-your-personal-home-dashboard","text":"From the main menu, go to Dashboards > Browse and select the dashboard you want to set as your home dashboard. Click the star next to the dashboard title to mark it as a favorite. From the side menu go to Configuration > Preferences . In the Home Dashboard field, select the dashboard that you want to set as your home dashboard. Click Save .","title":"Set your Personal Home Dashboard"},{"location":"details/dashboards/dashboard-memory-details.html","text":"Memory Details \u00b6 Memory Usage \u00b6 No description","title":"Memory Details"},{"location":"details/dashboards/dashboard-memory-details.html#memory-details","text":"","title":"Memory Details"},{"location":"details/dashboards/dashboard-memory-details.html#memory-usage","text":"No description","title":"Memory Usage"},{"location":"details/dashboards/dashboard-mongodb-cluster-summary.html","text":"MongoDB Cluster Summary \u00b6 Current Connections Per Shard \u00b6 TCP connections (Incoming) in mongod processes. Total Connections \u00b6 Incoming connections to mongos nodes. Cursors Per Shard \u00b6 The Cursor is a MongoDB Collection of the document which is returned upon the find method execution. Mongos Cursors \u00b6 The Cursor is a MongoDB Collection of the document which is returned upon the find method execution. Operations Per Shard \u00b6 Ops/sec, classified by legacy wire protocol type ( query , insert , update , delete , getmore ). Total Mongos Operations \u00b6 Ops/sec, classified by legacy wire protocol type ( query , insert , update , delete , getmore ). Change Log Events \u00b6 Count, over last 10 minutes, of all types of configuration db changelog events. Oplog Range by Set \u00b6 Timespan \u2018window\u2019 between oldest and newest ops in the Oplog collection.","title":"MongoDB Cluster Summary"},{"location":"details/dashboards/dashboard-mongodb-cluster-summary.html#mongodb-cluster-summary","text":"","title":"MongoDB Cluster Summary"},{"location":"details/dashboards/dashboard-mongodb-cluster-summary.html#current-connections-per-shard","text":"TCP connections (Incoming) in mongod processes.","title":"Current Connections Per Shard"},{"location":"details/dashboards/dashboard-mongodb-cluster-summary.html#total-connections","text":"Incoming connections to mongos nodes.","title":"Total Connections"},{"location":"details/dashboards/dashboard-mongodb-cluster-summary.html#cursors-per-shard","text":"The Cursor is a MongoDB Collection of the document which is returned upon the find method execution.","title":"Cursors Per Shard"},{"location":"details/dashboards/dashboard-mongodb-cluster-summary.html#mongos-cursors","text":"The Cursor is a MongoDB Collection of the document which is returned upon the find method execution.","title":"Mongos Cursors"},{"location":"details/dashboards/dashboard-mongodb-cluster-summary.html#operations-per-shard","text":"Ops/sec, classified by legacy wire protocol type ( query , insert , update , delete , getmore ).","title":"Operations Per Shard"},{"location":"details/dashboards/dashboard-mongodb-cluster-summary.html#total-mongos-operations","text":"Ops/sec, classified by legacy wire protocol type ( query , insert , update , delete , getmore ).","title":"Total Mongos Operations"},{"location":"details/dashboards/dashboard-mongodb-cluster-summary.html#change-log-events","text":"Count, over last 10 minutes, of all types of configuration db changelog events.","title":"Change Log Events"},{"location":"details/dashboards/dashboard-mongodb-cluster-summary.html#oplog-range-by-set","text":"Timespan \u2018window\u2019 between oldest and newest ops in the Oplog collection.","title":"Oplog Range by Set"},{"location":"details/dashboards/dashboard-mongodb-experimental_collection_details.html","text":"Experimental MongoDB Collection Details \u00b6 Disclaimer This is an Experimental Dashboard that is not part of the official Percona Monitoring and Management (PMM) deployment and might be updated. We ship this Dashboard to obtain feedback from our users. Availability This experimental dashboard is available starting with PMM 2.30.0. This realtime experimental dashboard provides detailed information about the top collections by document count, size, and document read for MongoDB databases.","title":"Experimental MongoDB Collection Details"},{"location":"details/dashboards/dashboard-mongodb-experimental_collection_details.html#experimental-mongodb-collection-details","text":"Disclaimer This is an Experimental Dashboard that is not part of the official Percona Monitoring and Management (PMM) deployment and might be updated. We ship this Dashboard to obtain feedback from our users. Availability This experimental dashboard is available starting with PMM 2.30.0. This realtime experimental dashboard provides detailed information about the top collections by document count, size, and document read for MongoDB databases.","title":"Experimental MongoDB Collection Details"},{"location":"details/dashboards/dashboard-mongodb-experimental_collection_overview.html","text":"Experimental MongoDB Collection Overview \u00b6 Disclaimer This is an Experimental Dashboard that is not part of the official Percona Monitoring and Management (PMM) deployment and might be updated. We ship this Dashboard to obtain feedback from our users. Availability This experimental dashboard is available starting with PMM 2.30.0. This realtime dashboard contains panels of data about the Hottest Collections in the MongoDB database. The Instance level includes two panels, one for the Hottest Collections by Read (Total) and the Hottest Collections by Write (total) . The next panel displays data at the Database Level , where you can view MongoDB metrics such as Commands , Inserts , Updates , Removes , and Getmore . The last panel shows the number of operations in the chosen database.","title":"MongoDB Dashboards"},{"location":"details/dashboards/dashboard-mongodb-experimental_collection_overview.html#experimental-mongodb-collection-overview","text":"Disclaimer This is an Experimental Dashboard that is not part of the official Percona Monitoring and Management (PMM) deployment and might be updated. We ship this Dashboard to obtain feedback from our users. Availability This experimental dashboard is available starting with PMM 2.30.0. This realtime dashboard contains panels of data about the Hottest Collections in the MongoDB database. The Instance level includes two panels, one for the Hottest Collections by Read (Total) and the Hottest Collections by Write (total) . The next panel displays data at the Database Level , where you can view MongoDB metrics such as Commands , Inserts , Updates , Removes , and Getmore . The last panel shows the number of operations in the chosen database.","title":"Experimental MongoDB Collection Overview"},{"location":"details/dashboards/dashboard-mongodb-experimental_oplog.html","text":"Experimental MongoDB Oplog Details \u00b6 Disclaimer This is an Experimental Dashboard that is not part of the official Percona Monitoring and Management (PMM) deployment and might be updated. We ship this Dashboard to obtain feedback from our users. Availability This experimental dashboard is available starting with PMM 2.30.0. This realtime dashboard contains Oplog details such as Recovery Window, Processing Time, Buffer Capacity, and Oplog Operations.","title":"Experimental MongoDB Oplog Details"},{"location":"details/dashboards/dashboard-mongodb-experimental_oplog.html#experimental-mongodb-oplog-details","text":"Disclaimer This is an Experimental Dashboard that is not part of the official Percona Monitoring and Management (PMM) deployment and might be updated. We ship this Dashboard to obtain feedback from our users. Availability This experimental dashboard is available starting with PMM 2.30.0. This realtime dashboard contains Oplog details such as Recovery Window, Processing Time, Buffer Capacity, and Oplog Operations.","title":"Experimental MongoDB Oplog Details"},{"location":"details/dashboards/dashboard-mongodb-inmemory-details.html","text":"MongoDB InMemory Details \u00b6 InMemory Transactions \u00b6 WiredTiger internal transactions InMemory Capacity \u00b6 Configured max and current size of the WiredTiger cache. InMemory Sessions \u00b6 Internal WiredTiger storage engine cursors and sessions currently open. InMemory Pages \u00b6 Pages in the WiredTiger cache InMemory Concurrency Tickets \u00b6 A WT \u2018ticket\u2019 is assigned out for every operation running simultaneously in the WT storage engine. \u201cTickets available\u201d = hard coded high value - \u201cTickets Out\u201d. Queued Operations \u00b6 Operations queued due to a lock Document Changes \u00b6 Mixed metrics: Docs per second inserted, updated, deleted or returned on any type of node (primary or secondary); + replicated write Ops/sec; + TTL deletes per second. InMemory Cache Eviction \u00b6 This panel shows the number of pages that have been evicted from the WiredTiger cache for the given time period. The InMemory storage engine only evicts modified pages which signals a compaction of the data and removal of the dirty pages. Scanned and Moved Objects \u00b6 This panel shows the number of objects (both data ( scanned_objects ) and index ( scanned )) as well as the number of documents that were moved to a new location due to the size of the document growing. Moved documents only apply to the MMAPv1 storage engine. Page Faults \u00b6 Unix or Window memory page faults. Not necessarily from MongoDB.","title":"MongoDB InMemory Details"},{"location":"details/dashboards/dashboard-mongodb-inmemory-details.html#mongodb-inmemory-details","text":"","title":"MongoDB InMemory Details"},{"location":"details/dashboards/dashboard-mongodb-inmemory-details.html#inmemory-transactions","text":"WiredTiger internal transactions","title":"InMemory Transactions"},{"location":"details/dashboards/dashboard-mongodb-inmemory-details.html#inmemory-capacity","text":"Configured max and current size of the WiredTiger cache.","title":"InMemory Capacity"},{"location":"details/dashboards/dashboard-mongodb-inmemory-details.html#inmemory-sessions","text":"Internal WiredTiger storage engine cursors and sessions currently open.","title":"InMemory Sessions"},{"location":"details/dashboards/dashboard-mongodb-inmemory-details.html#inmemory-pages","text":"Pages in the WiredTiger cache","title":"InMemory Pages"},{"location":"details/dashboards/dashboard-mongodb-inmemory-details.html#inmemory-concurrency-tickets","text":"A WT \u2018ticket\u2019 is assigned out for every operation running simultaneously in the WT storage engine. \u201cTickets available\u201d = hard coded high value - \u201cTickets Out\u201d.","title":"InMemory Concurrency Tickets"},{"location":"details/dashboards/dashboard-mongodb-inmemory-details.html#queued-operations","text":"Operations queued due to a lock","title":"Queued Operations"},{"location":"details/dashboards/dashboard-mongodb-inmemory-details.html#document-changes","text":"Mixed metrics: Docs per second inserted, updated, deleted or returned on any type of node (primary or secondary); + replicated write Ops/sec; + TTL deletes per second.","title":"Document Changes"},{"location":"details/dashboards/dashboard-mongodb-inmemory-details.html#inmemory-cache-eviction","text":"This panel shows the number of pages that have been evicted from the WiredTiger cache for the given time period. The InMemory storage engine only evicts modified pages which signals a compaction of the data and removal of the dirty pages.","title":"InMemory Cache Eviction"},{"location":"details/dashboards/dashboard-mongodb-inmemory-details.html#scanned-and-moved-objects","text":"This panel shows the number of objects (both data ( scanned_objects ) and index ( scanned )) as well as the number of documents that were moved to a new location due to the size of the document growing. Moved documents only apply to the MMAPv1 storage engine.","title":"Scanned and Moved Objects"},{"location":"details/dashboards/dashboard-mongodb-inmemory-details.html#page-faults","text":"Unix or Window memory page faults. Not necessarily from MongoDB.","title":"Page Faults"},{"location":"details/dashboards/dashboard-mongodb-instance-summary.html","text":"MongoDB Instance Summary \u00b6 Command Operations \u00b6 Ops or Replicated Ops/sec classified by legacy wire protocol type ( query , insert , update , delete , getmore ). And (from the internal TTL threads) the docs deletes/sec by TTL indexes. Latency Detail \u00b6 Average latency of operations (classified by read, write, or (other) command) Connections \u00b6 TCP connections (Incoming) Cursors \u00b6 Open cursors. Includes idle cursors. Document Operations \u00b6 Docs per second inserted, updated, deleted or returned. (not 1-to-1 with operation counts.) Queued Operations \u00b6 Operations queued due to a lock. Query Efficiency \u00b6 Ratio of Documents returned or Index entries scanned / full documents scanned Scanned and Moved Objects \u00b6 This panel shows the number of objects (both data ( scanned_objects ) and index ( scanned )) as well as the number of documents that were moved to a new location due to the size of the document growing. Moved documents only apply to the MMAPv1 storage engine. getLastError Write Time \u00b6 Legacy driver operation: Number of, and Sum of time spent, per second executing getLastError commands to confirm write concern. getLastError Write Operations \u00b6 Legacy driver operation: Number of getLastError commands that timed out trying to confirm write concern. Assert Events \u00b6 This panel shows the number of assert events per second on average over the given time period. In most cases assertions are trivial, but you would want to check your log files if this counter spikes or is consistently high. Page Faults \u00b6 Unix or Window memory page faults. Not necessarily from MongoDB.","title":"MongoDB Instance Summary"},{"location":"details/dashboards/dashboard-mongodb-instance-summary.html#mongodb-instance-summary","text":"","title":"MongoDB Instance Summary"},{"location":"details/dashboards/dashboard-mongodb-instance-summary.html#command-operations","text":"Ops or Replicated Ops/sec classified by legacy wire protocol type ( query , insert , update , delete , getmore ). And (from the internal TTL threads) the docs deletes/sec by TTL indexes.","title":"Command Operations"},{"location":"details/dashboards/dashboard-mongodb-instance-summary.html#latency-detail","text":"Average latency of operations (classified by read, write, or (other) command)","title":"Latency Detail"},{"location":"details/dashboards/dashboard-mongodb-instance-summary.html#connections","text":"TCP connections (Incoming)","title":"Connections"},{"location":"details/dashboards/dashboard-mongodb-instance-summary.html#cursors","text":"Open cursors. Includes idle cursors.","title":"Cursors"},{"location":"details/dashboards/dashboard-mongodb-instance-summary.html#document-operations","text":"Docs per second inserted, updated, deleted or returned. (not 1-to-1 with operation counts.)","title":"Document Operations"},{"location":"details/dashboards/dashboard-mongodb-instance-summary.html#queued-operations","text":"Operations queued due to a lock.","title":"Queued Operations"},{"location":"details/dashboards/dashboard-mongodb-instance-summary.html#query-efficiency","text":"Ratio of Documents returned or Index entries scanned / full documents scanned","title":"Query Efficiency"},{"location":"details/dashboards/dashboard-mongodb-instance-summary.html#scanned-and-moved-objects","text":"This panel shows the number of objects (both data ( scanned_objects ) and index ( scanned )) as well as the number of documents that were moved to a new location due to the size of the document growing. Moved documents only apply to the MMAPv1 storage engine.","title":"Scanned and Moved Objects"},{"location":"details/dashboards/dashboard-mongodb-instance-summary.html#getlasterror-write-time","text":"Legacy driver operation: Number of, and Sum of time spent, per second executing getLastError commands to confirm write concern.","title":"getLastError Write Time"},{"location":"details/dashboards/dashboard-mongodb-instance-summary.html#getlasterror-write-operations","text":"Legacy driver operation: Number of getLastError commands that timed out trying to confirm write concern.","title":"getLastError Write Operations"},{"location":"details/dashboards/dashboard-mongodb-instance-summary.html#assert-events","text":"This panel shows the number of assert events per second on average over the given time period. In most cases assertions are trivial, but you would want to check your log files if this counter spikes or is consistently high.","title":"Assert Events"},{"location":"details/dashboards/dashboard-mongodb-instance-summary.html#page-faults","text":"Unix or Window memory page faults. Not necessarily from MongoDB.","title":"Page Faults"},{"location":"details/dashboards/dashboard-mongodb-instances-compare.html","text":"MongoDB Instances Compare \u00b6 Connections \u00b6 No description Cursors \u00b6 No description Latency \u00b6 Average latency of operations (classified by read, write, or (other) command) Scan Ratios \u00b6 Ratio of index entries scanned or whole docs scanned / number of documents returned Index Filtering Effectiveness \u00b6 No description Requests \u00b6 Ops/sec (classified by (legacy) wire protocol request type) Document Operations \u00b6 Documents inserted/updated/deleted or returned per sec Queued Operations \u00b6 The number of operations that are currently queued and waiting for a lock Used Memory \u00b6 No description","title":"MongoDB Instances Compare"},{"location":"details/dashboards/dashboard-mongodb-instances-compare.html#mongodb-instances-compare","text":"","title":"MongoDB Instances Compare"},{"location":"details/dashboards/dashboard-mongodb-instances-compare.html#connections","text":"No description","title":"Connections"},{"location":"details/dashboards/dashboard-mongodb-instances-compare.html#cursors","text":"No description","title":"Cursors"},{"location":"details/dashboards/dashboard-mongodb-instances-compare.html#latency","text":"Average latency of operations (classified by read, write, or (other) command)","title":"Latency"},{"location":"details/dashboards/dashboard-mongodb-instances-compare.html#scan-ratios","text":"Ratio of index entries scanned or whole docs scanned / number of documents returned","title":"Scan Ratios"},{"location":"details/dashboards/dashboard-mongodb-instances-compare.html#index-filtering-effectiveness","text":"No description","title":"Index Filtering Effectiveness"},{"location":"details/dashboards/dashboard-mongodb-instances-compare.html#requests","text":"Ops/sec (classified by (legacy) wire protocol request type)","title":"Requests"},{"location":"details/dashboards/dashboard-mongodb-instances-compare.html#document-operations","text":"Documents inserted/updated/deleted or returned per sec","title":"Document Operations"},{"location":"details/dashboards/dashboard-mongodb-instances-compare.html#queued-operations","text":"The number of operations that are currently queued and waiting for a lock","title":"Queued Operations"},{"location":"details/dashboards/dashboard-mongodb-instances-compare.html#used-memory","text":"No description","title":"Used Memory"},{"location":"details/dashboards/dashboard-mongodb-instances-overview.html","text":"MongoDB Instances Overview \u00b6 This dashboard provides basic information about MongoDB instances. Command Operations \u00b6 Shows how many times a command is executed per second on average during the selected interval. Look for peaks and drops and correlate them with other graphs. Connections \u00b6 Keep in mind the hard limit on the maximum number of connections set by your distribution. Anything over 5,000 should be a concern, because the application may not close connections correctly. Cursors \u00b6 Helps identify why connections are increasing. Shows active cursors compared to cursors being automatically killed after 10 minutes due to an application not closing the connection. Document Operations \u00b6 When used in combination with Command Operations , this graph can help identify write amplification . For example, when one insert or update command actually inserts or updates hundreds, thousands, or even millions of documents. Queued Operations \u00b6 Any number of queued operations for long periods of time is an indication of possible issues. Find the cause and fix it before requests get stuck in the queue. getLastError Write Time, getLastError Write Operations \u00b6 This is useful for write-heavy workloads to understand how long it takes to verify writes and how many concurrent writes are occurring. Asserts \u00b6 Asserts are not important by themselves, but you can correlate spikes with other graphs. Memory Faults \u00b6 Memory faults indicate that requests are processed from disk either because an index is missing or there is not enough memory for the data set. Consider increasing memory or sharding out.","title":"MongoDB Instances Overview"},{"location":"details/dashboards/dashboard-mongodb-instances-overview.html#mongodb-instances-overview","text":"This dashboard provides basic information about MongoDB instances.","title":"MongoDB Instances Overview"},{"location":"details/dashboards/dashboard-mongodb-instances-overview.html#command-operations","text":"Shows how many times a command is executed per second on average during the selected interval. Look for peaks and drops and correlate them with other graphs.","title":"Command Operations"},{"location":"details/dashboards/dashboard-mongodb-instances-overview.html#connections","text":"Keep in mind the hard limit on the maximum number of connections set by your distribution. Anything over 5,000 should be a concern, because the application may not close connections correctly.","title":"Connections"},{"location":"details/dashboards/dashboard-mongodb-instances-overview.html#cursors","text":"Helps identify why connections are increasing. Shows active cursors compared to cursors being automatically killed after 10 minutes due to an application not closing the connection.","title":"Cursors"},{"location":"details/dashboards/dashboard-mongodb-instances-overview.html#document-operations","text":"When used in combination with Command Operations , this graph can help identify write amplification . For example, when one insert or update command actually inserts or updates hundreds, thousands, or even millions of documents.","title":"Document Operations"},{"location":"details/dashboards/dashboard-mongodb-instances-overview.html#queued-operations","text":"Any number of queued operations for long periods of time is an indication of possible issues. Find the cause and fix it before requests get stuck in the queue.","title":"Queued Operations"},{"location":"details/dashboards/dashboard-mongodb-instances-overview.html#getlasterror-write-time-getlasterror-write-operations","text":"This is useful for write-heavy workloads to understand how long it takes to verify writes and how many concurrent writes are occurring.","title":"getLastError Write Time, getLastError Write Operations"},{"location":"details/dashboards/dashboard-mongodb-instances-overview.html#asserts","text":"Asserts are not important by themselves, but you can correlate spikes with other graphs.","title":"Asserts"},{"location":"details/dashboards/dashboard-mongodb-instances-overview.html#memory-faults","text":"Memory faults indicate that requests are processed from disk either because an index is missing or there is not enough memory for the data set. Consider increasing memory or sharding out.","title":"Memory Faults"},{"location":"details/dashboards/dashboard-mongodb-mmapv1-details.html","text":"MongoDB MMAPv1 Details \u00b6 Document Activity \u00b6 Docs per second inserted, updated, deleted or returned. Also showing replicated write ops and internal TTL index deletes. MMAPv1 Lock Wait Time \u00b6 Time spent per second waiting to acquire locks. MMAPv1 Page Faults \u00b6 Unix or Window memory page faults. Not necessarily from MongoDB. MMAPv1 Journal Write Activity \u00b6 MB processed through the journal in memory. MMAPv1 Journal Commit Activity \u00b6 MB committed to disk for the journal. MMAPv1 Background Flushing Time \u00b6 Average time in ms, over full uptime of mongod process, the MMAP background flushes have taken. Queued Operations \u00b6 Queue size of ops waiting to be submitted to storage engine layer. (see WiredTiger concurrency tickets for number of ops being processed simultaneously in storage engine layer.) Client Operations \u00b6 Ops and Replicated Ops/sec, classified by legacy wire protocol type ( query , insert , update , delete , getmore ). Scanned and Moved Objects \u00b6 This panel shows the number of objects (both data ( scanned_objects ) and index ( scanned )) as well as the number of documents that were moved to a new location due to the size of the document growing. Moved documents only apply to the MMAPv1 storage engine.","title":"MongoDB MMAPv1 Details"},{"location":"details/dashboards/dashboard-mongodb-mmapv1-details.html#mongodb-mmapv1-details","text":"","title":"MongoDB MMAPv1 Details"},{"location":"details/dashboards/dashboard-mongodb-mmapv1-details.html#document-activity","text":"Docs per second inserted, updated, deleted or returned. Also showing replicated write ops and internal TTL index deletes.","title":"Document Activity"},{"location":"details/dashboards/dashboard-mongodb-mmapv1-details.html#mmapv1-lock-wait-time","text":"Time spent per second waiting to acquire locks.","title":"MMAPv1 Lock Wait Time"},{"location":"details/dashboards/dashboard-mongodb-mmapv1-details.html#mmapv1-page-faults","text":"Unix or Window memory page faults. Not necessarily from MongoDB.","title":"MMAPv1 Page Faults"},{"location":"details/dashboards/dashboard-mongodb-mmapv1-details.html#mmapv1-journal-write-activity","text":"MB processed through the journal in memory.","title":"MMAPv1 Journal Write Activity"},{"location":"details/dashboards/dashboard-mongodb-mmapv1-details.html#mmapv1-journal-commit-activity","text":"MB committed to disk for the journal.","title":"MMAPv1 Journal Commit Activity"},{"location":"details/dashboards/dashboard-mongodb-mmapv1-details.html#mmapv1-background-flushing-time","text":"Average time in ms, over full uptime of mongod process, the MMAP background flushes have taken.","title":"MMAPv1 Background Flushing Time"},{"location":"details/dashboards/dashboard-mongodb-mmapv1-details.html#queued-operations","text":"Queue size of ops waiting to be submitted to storage engine layer. (see WiredTiger concurrency tickets for number of ops being processed simultaneously in storage engine layer.)","title":"Queued Operations"},{"location":"details/dashboards/dashboard-mongodb-mmapv1-details.html#client-operations","text":"Ops and Replicated Ops/sec, classified by legacy wire protocol type ( query , insert , update , delete , getmore ).","title":"Client Operations"},{"location":"details/dashboards/dashboard-mongodb-mmapv1-details.html#scanned-and-moved-objects","text":"This panel shows the number of objects (both data ( scanned_objects ) and index ( scanned )) as well as the number of documents that were moved to a new location due to the size of the document growing. Moved documents only apply to the MMAPv1 storage engine.","title":"Scanned and Moved Objects"},{"location":"details/dashboards/dashboard-mongodb-replset-summary.html","text":"MongoDB ReplSet Summary \u00b6 Replication Lag \u00b6 MongoDB replication lag occurs when the secondary node cannot replicate data fast enough to keep up with the rate that data is being written to the primary node. It could be caused by something as simple as network latency, packet loss within your network, or a routing issue. Operations - by service name \u00b6 Operations are classified by legacy wire protocol type (insert, update, and delete only). Max Member Ping Time - by service name \u00b6 This metric can show a correlation with the replication lag value. Max Heartbeat Time \u00b6 Time span between now and last heartbeat from replicaset members. Elections \u00b6 Count of elections. Usually zero; 1 count by each healthy node will appear in each election. Happens when the primary role changes due to either normal maintenance or trouble events. Oplog Recovery Window - by service name \u00b6 Timespan \u2018window\u2019 between newest and the oldest op in the Oplog collection.","title":"MongoDB ReplSet Summary"},{"location":"details/dashboards/dashboard-mongodb-replset-summary.html#mongodb-replset-summary","text":"","title":"MongoDB ReplSet Summary"},{"location":"details/dashboards/dashboard-mongodb-replset-summary.html#replication-lag","text":"MongoDB replication lag occurs when the secondary node cannot replicate data fast enough to keep up with the rate that data is being written to the primary node. It could be caused by something as simple as network latency, packet loss within your network, or a routing issue.","title":"Replication Lag"},{"location":"details/dashboards/dashboard-mongodb-replset-summary.html#operations-by-service-name","text":"Operations are classified by legacy wire protocol type (insert, update, and delete only).","title":"Operations - by service name"},{"location":"details/dashboards/dashboard-mongodb-replset-summary.html#max-member-ping-time-by-service-name","text":"This metric can show a correlation with the replication lag value.","title":"Max Member Ping Time - by service name"},{"location":"details/dashboards/dashboard-mongodb-replset-summary.html#max-heartbeat-time","text":"Time span between now and last heartbeat from replicaset members.","title":"Max Heartbeat Time"},{"location":"details/dashboards/dashboard-mongodb-replset-summary.html#elections","text":"Count of elections. Usually zero; 1 count by each healthy node will appear in each election. Happens when the primary role changes due to either normal maintenance or trouble events.","title":"Elections"},{"location":"details/dashboards/dashboard-mongodb-replset-summary.html#oplog-recovery-window-by-service-name","text":"Timespan \u2018window\u2019 between newest and the oldest op in the Oplog collection.","title":"Oplog Recovery Window - by service name"},{"location":"details/dashboards/dashboard-mongodb-wiredtiger-details.html","text":"MongoDB WiredTiger Details \u00b6 WiredTiger Transactions \u00b6 WiredTiger internal transactions WiredTiger Cache Activity \u00b6 Data volume transferred per second between the WT cache and data files. Writes out always imply disk; Reads are often from OS file buffer cache already in RAM, but disk if not. WiredTiger Block Activity \u00b6 Data volume handled by the WT block manager per second WiredTiger Sessions \u00b6 Internal WT storage engine cursors and sessions currently open WiredTiger Concurrency Tickets Available \u00b6 A WT \u2018ticket\u2019 is assigned out for every operation running simultaneously in the WT storage engine. \u201cAvailable\u201d = hard-coded high value - \u201cOut\u201d. Queued Operations \u00b6 Operations queued due to a lock. WiredTiger Checkpoint Time \u00b6 The time spent in WT checkpoint phase. Warning: This calculation averages the cyclical event (default: 1 min) execution to a per-second value. WiredTiger Cache Eviction \u00b6 Least-recently used pages being evicted due to WT cache becoming full. WiredTiger Cache Capacity \u00b6 Configured max and current size of the WT cache. WiredTiger Cache Pages \u00b6 WiredTiger Log Operations \u00b6 WT internal write-ahead log operations. WiredTiger Log Activity \u00b6 Data volume moved per second in WT internal write-ahead log. WiredTiger Log Records \u00b6 Number of records appended per second in WT internal log. Document Changes \u00b6 Mixed metrics: Docs per second inserted, updated, deleted or returned on any type of node (primary or secondary); + replicated write Ops/sec; + TTL deletes per second. Scanned and Moved Objects \u00b6 This panel shows the number of objects (both data ( scanned_objects ) and index ( scanned )) as well as the number of documents that were moved to a new location due to the size of the document growing. Moved documents only apply to the MMAPv1 storage engine. Page Faults \u00b6 Unix or Window memory page faults. Not necessarily from MongoDB.","title":"MongoDB WiredTiger Details"},{"location":"details/dashboards/dashboard-mongodb-wiredtiger-details.html#mongodb-wiredtiger-details","text":"","title":"MongoDB WiredTiger Details"},{"location":"details/dashboards/dashboard-mongodb-wiredtiger-details.html#wiredtiger-transactions","text":"WiredTiger internal transactions","title":"WiredTiger Transactions"},{"location":"details/dashboards/dashboard-mongodb-wiredtiger-details.html#wiredtiger-cache-activity","text":"Data volume transferred per second between the WT cache and data files. Writes out always imply disk; Reads are often from OS file buffer cache already in RAM, but disk if not.","title":"WiredTiger Cache Activity"},{"location":"details/dashboards/dashboard-mongodb-wiredtiger-details.html#wiredtiger-block-activity","text":"Data volume handled by the WT block manager per second","title":"WiredTiger Block Activity"},{"location":"details/dashboards/dashboard-mongodb-wiredtiger-details.html#wiredtiger-sessions","text":"Internal WT storage engine cursors and sessions currently open","title":"WiredTiger Sessions"},{"location":"details/dashboards/dashboard-mongodb-wiredtiger-details.html#wiredtiger-concurrency-tickets-available","text":"A WT \u2018ticket\u2019 is assigned out for every operation running simultaneously in the WT storage engine. \u201cAvailable\u201d = hard-coded high value - \u201cOut\u201d.","title":"WiredTiger Concurrency Tickets Available"},{"location":"details/dashboards/dashboard-mongodb-wiredtiger-details.html#queued-operations","text":"Operations queued due to a lock.","title":"Queued Operations"},{"location":"details/dashboards/dashboard-mongodb-wiredtiger-details.html#wiredtiger-checkpoint-time","text":"The time spent in WT checkpoint phase. Warning: This calculation averages the cyclical event (default: 1 min) execution to a per-second value.","title":"WiredTiger Checkpoint Time"},{"location":"details/dashboards/dashboard-mongodb-wiredtiger-details.html#wiredtiger-cache-eviction","text":"Least-recently used pages being evicted due to WT cache becoming full.","title":"WiredTiger Cache Eviction"},{"location":"details/dashboards/dashboard-mongodb-wiredtiger-details.html#wiredtiger-cache-capacity","text":"Configured max and current size of the WT cache.","title":"WiredTiger Cache Capacity"},{"location":"details/dashboards/dashboard-mongodb-wiredtiger-details.html#wiredtiger-cache-pages","text":"","title":"WiredTiger Cache Pages"},{"location":"details/dashboards/dashboard-mongodb-wiredtiger-details.html#wiredtiger-log-operations","text":"WT internal write-ahead log operations.","title":"WiredTiger Log Operations"},{"location":"details/dashboards/dashboard-mongodb-wiredtiger-details.html#wiredtiger-log-activity","text":"Data volume moved per second in WT internal write-ahead log.","title":"WiredTiger Log Activity"},{"location":"details/dashboards/dashboard-mongodb-wiredtiger-details.html#wiredtiger-log-records","text":"Number of records appended per second in WT internal log.","title":"WiredTiger Log Records"},{"location":"details/dashboards/dashboard-mongodb-wiredtiger-details.html#document-changes","text":"Mixed metrics: Docs per second inserted, updated, deleted or returned on any type of node (primary or secondary); + replicated write Ops/sec; + TTL deletes per second.","title":"Document Changes"},{"location":"details/dashboards/dashboard-mongodb-wiredtiger-details.html#scanned-and-moved-objects","text":"This panel shows the number of objects (both data ( scanned_objects ) and index ( scanned )) as well as the number of documents that were moved to a new location due to the size of the document growing. Moved documents only apply to the MMAPv1 storage engine.","title":"Scanned and Moved Objects"},{"location":"details/dashboards/dashboard-mongodb-wiredtiger-details.html#page-faults","text":"Unix or Window memory page faults. Not necessarily from MongoDB.","title":"Page Faults"},{"location":"details/dashboards/dashboard-mysql-amazon-aurora-details.html","text":"MySQL Amazon Aurora Details \u00b6 Amazon Aurora Transaction Commits \u00b6 This graph shows the number of Commits which Amazon Aurora engine performed as well as average commit latency. Graph Latency does not always correlate with the number of performed commits and can be quite high in certain situations. Number of Amazon Aurora Commits : The average number of commit operations per second. Amazon Aurora Commit avg Latency : The average amount of latency for commit operations Amazon Aurora Load \u00b6 This graph shows us what statements contribute most load on the system as well as what load corresponds to Amazon Aurora transaction commit. Write Transaction Commit Load : Load in Average Active Sessions per second for COMMIT operations UPDATE load : Load in Average Active Sessions per second for UPDATE queries SELECT load : Load in Average Active Sessions per second for SELECT queries DELETE load : Load in Average Active Sessions per second for DELETE queries INSERT load : Load in Average Active Sessions per second for INSERT queries An active session is a connection that has submitted work to the database engine and is waiting for a response from it. For example, if you submit an SQL query to the database engine, the database session is active while the database engine is processing that query. Aurora Memory Used \u00b6 This graph shows how much memory is used by Amazon Aurora lock manager as well as amount of memory used by Amazon Aurora to store Data Dictionary. Aurora Lock Manager Memory : the amount of memory used by the Lock Manager, the module responsible for handling row lock requests for concurrent transactions. Aurora Dictionary Memory : the amount of memory used by the Dictionary, the space that contains metadata used to keep track of database objects, such as tables and indexes. Amazon Aurora Statement Latency \u00b6 This graph shows average latency for the most important types of statements. Latency spikes are often indicative of the instance overload. DDL Latency: Average time to execute DDL queries DELETE Latency : Average time to execute DELETE queries UPDATE Latency : Average time to execute UPDATE queries SELECT Latency : Average time to execute SELECT queries INSERT Latency : Average time to execute INSERT queries Amazon Aurora Special Command Counters \u00b6 Amazon Aurora MySQL allows a number of commands which are not available in standard MySQL. This graph shows usage of such commands. Regular unit_test calls can be seen in default Amazon Aurora install, the rest will depend on your workload. show_volume_status : The number of executions per second of the command SHOW VOLUME STATUS. The SHOW VOLUME STATUS query returns two server status variables, Disks and Nodes. These variables represent the total number of logical blocks of data and storage nodes, respectively, for the DB cluster volume. awslambda : The number of AWS Lambda calls per second. AWS Lambda is an event-drive, server-less computing platform provided by AWS. It is a compute service that run codes in response to an event. You can run any kind of code from Aurora invoking Lambda from a stored procedure or a trigger. alter_system : The number of executions per second of the special query ALTER SYSTEM, that is a special query to simulate an instance crash, a disk failure, a disk congestion or a replica failure. It\u2019s a useful query for testing the system. Amazon Aurora Problems \u00b6 This graph shows different kinds of Internal Amazon Aurora MySQL Problems which general should be zero in normal operation. Anything non-zero is worth examining in greater depth.","title":"MySQL Dashboards"},{"location":"details/dashboards/dashboard-mysql-amazon-aurora-details.html#mysql-amazon-aurora-details","text":"","title":"MySQL Amazon Aurora Details"},{"location":"details/dashboards/dashboard-mysql-amazon-aurora-details.html#amazon-aurora-transaction-commits","text":"This graph shows the number of Commits which Amazon Aurora engine performed as well as average commit latency. Graph Latency does not always correlate with the number of performed commits and can be quite high in certain situations. Number of Amazon Aurora Commits : The average number of commit operations per second. Amazon Aurora Commit avg Latency : The average amount of latency for commit operations","title":"Amazon Aurora Transaction Commits"},{"location":"details/dashboards/dashboard-mysql-amazon-aurora-details.html#amazon-aurora-load","text":"This graph shows us what statements contribute most load on the system as well as what load corresponds to Amazon Aurora transaction commit. Write Transaction Commit Load : Load in Average Active Sessions per second for COMMIT operations UPDATE load : Load in Average Active Sessions per second for UPDATE queries SELECT load : Load in Average Active Sessions per second for SELECT queries DELETE load : Load in Average Active Sessions per second for DELETE queries INSERT load : Load in Average Active Sessions per second for INSERT queries An active session is a connection that has submitted work to the database engine and is waiting for a response from it. For example, if you submit an SQL query to the database engine, the database session is active while the database engine is processing that query.","title":"Amazon Aurora Load"},{"location":"details/dashboards/dashboard-mysql-amazon-aurora-details.html#aurora-memory-used","text":"This graph shows how much memory is used by Amazon Aurora lock manager as well as amount of memory used by Amazon Aurora to store Data Dictionary. Aurora Lock Manager Memory : the amount of memory used by the Lock Manager, the module responsible for handling row lock requests for concurrent transactions. Aurora Dictionary Memory : the amount of memory used by the Dictionary, the space that contains metadata used to keep track of database objects, such as tables and indexes.","title":"Aurora Memory Used"},{"location":"details/dashboards/dashboard-mysql-amazon-aurora-details.html#amazon-aurora-statement-latency","text":"This graph shows average latency for the most important types of statements. Latency spikes are often indicative of the instance overload. DDL Latency: Average time to execute DDL queries DELETE Latency : Average time to execute DELETE queries UPDATE Latency : Average time to execute UPDATE queries SELECT Latency : Average time to execute SELECT queries INSERT Latency : Average time to execute INSERT queries","title":"Amazon Aurora Statement Latency"},{"location":"details/dashboards/dashboard-mysql-amazon-aurora-details.html#amazon-aurora-special-command-counters","text":"Amazon Aurora MySQL allows a number of commands which are not available in standard MySQL. This graph shows usage of such commands. Regular unit_test calls can be seen in default Amazon Aurora install, the rest will depend on your workload. show_volume_status : The number of executions per second of the command SHOW VOLUME STATUS. The SHOW VOLUME STATUS query returns two server status variables, Disks and Nodes. These variables represent the total number of logical blocks of data and storage nodes, respectively, for the DB cluster volume. awslambda : The number of AWS Lambda calls per second. AWS Lambda is an event-drive, server-less computing platform provided by AWS. It is a compute service that run codes in response to an event. You can run any kind of code from Aurora invoking Lambda from a stored procedure or a trigger. alter_system : The number of executions per second of the special query ALTER SYSTEM, that is a special query to simulate an instance crash, a disk failure, a disk congestion or a replica failure. It\u2019s a useful query for testing the system.","title":"Amazon Aurora Special Command Counters"},{"location":"details/dashboards/dashboard-mysql-amazon-aurora-details.html#amazon-aurora-problems","text":"This graph shows different kinds of Internal Amazon Aurora MySQL Problems which general should be zero in normal operation. Anything non-zero is worth examining in greater depth.","title":"Amazon Aurora Problems"},{"location":"details/dashboards/dashboard-mysql-command-handler-counters-compare.html","text":"MySQL Command/Handler Counters Compare \u00b6 This dashboard shows server status variables. On this dashboard, you may select multiple servers and compare their counters simultaneously. Server status variables appear in two sections: Commands and Handlers . Choose one or more variables in the Command and Handler fields in the top menu to select the variables which will appear in the COMMANDS or HANDLERS section for each host. Your comparison may include from one up to three hosts. By default or if no item is selected in the menu, PMM displays each command or handler respectively.","title":"MySQL Command/Handler Counters Compare"},{"location":"details/dashboards/dashboard-mysql-command-handler-counters-compare.html#mysql-commandhandler-counters-compare","text":"This dashboard shows server status variables. On this dashboard, you may select multiple servers and compare their counters simultaneously. Server status variables appear in two sections: Commands and Handlers . Choose one or more variables in the Command and Handler fields in the top menu to select the variables which will appear in the COMMANDS or HANDLERS section for each host. Your comparison may include from one up to three hosts. By default or if no item is selected in the menu, PMM displays each command or handler respectively.","title":"MySQL Command/Handler Counters Compare"},{"location":"details/dashboards/dashboard-mysql-group-replication-summary.html","text":"MySQL Group Replication Summary \u00b6 Overview \u00b6 PRIMARY Service Group Replication Service States Replication Group Members Replication Lag Replication Delay Transport Time Transactions \u00b6 Transaction Details Applied Transactions Sent Transactions Checked Transactions Rolled Back Transactions Transactions Row Validating Transactions in the Queue for Checking Received Transactions Queue Conflicts \u00b6 Detected Conflicts","title":"MySQL Group Replication Summary"},{"location":"details/dashboards/dashboard-mysql-group-replication-summary.html#mysql-group-replication-summary","text":"","title":"MySQL Group Replication Summary"},{"location":"details/dashboards/dashboard-mysql-group-replication-summary.html#overview","text":"PRIMARY Service Group Replication Service States Replication Group Members Replication Lag Replication Delay Transport Time","title":"Overview"},{"location":"details/dashboards/dashboard-mysql-group-replication-summary.html#transactions","text":"Transaction Details Applied Transactions Sent Transactions Checked Transactions Rolled Back Transactions Transactions Row Validating Transactions in the Queue for Checking Received Transactions Queue","title":"Transactions"},{"location":"details/dashboards/dashboard-mysql-group-replication-summary.html#conflicts","text":"Detected Conflicts","title":"Conflicts"},{"location":"details/dashboards/dashboard-mysql-innodb-compression-details.html","text":"MySQL InnoDB Compression Details \u00b6 This dashboard helps you analyze the efficiency of InnoDB compression. Compression level and failure rate threshold \u00b6 InnoDB Compression Level The level of zlib compression to use for InnoDB compressed tables and indexes. InnoDB Compression Failure Threshold The compression failure rate threshold for a table. Compression Failure Rate Threshold The maximum percentage that can be reserved as free space within each compressed page, allowing room to reorganize the data and modification log within the page when a compressed table or index is updated and the data might be recompressed. Write Pages to the Redo Log Specifies whether images of re-compressed pages are written to the redo log. Re-compression may occur when changes are made to compressed data. Statistic of compression operations \u00b6 Compress Attempts Number of compression operations attempted. Pages are compressed whenever an empty page is created or the space for the uncompressed modification log runs out. Uncompressed Attempts Number of uncompression operations performed. Compressed InnoDB pages are uncompressed whenever compression fails, or the first time a compressed page is accessed in the buffer pool and the uncompressed page does not exist. CPU Core Usage \u00b6 CPU Core Usage for Compression Shows the time in seconds spent by InnoDB Compression operations. CPU Core Usage for Uncompression Shows the time in seconds spent by InnoDB Uncompression operations. Buffer Pool Total \u00b6 Total Used Pages Shows the total amount of used compressed pages into the InnoDB Buffer Pool split by page size. Total Free Pages Shows the total amount of free compressed pages into the InnoDB Buffer Pool split by page size.","title":"MySQL InnoDB Compression Details"},{"location":"details/dashboards/dashboard-mysql-innodb-compression-details.html#mysql-innodb-compression-details","text":"This dashboard helps you analyze the efficiency of InnoDB compression.","title":"MySQL InnoDB Compression Details"},{"location":"details/dashboards/dashboard-mysql-innodb-compression-details.html#compression-level-and-failure-rate-threshold","text":"InnoDB Compression Level The level of zlib compression to use for InnoDB compressed tables and indexes. InnoDB Compression Failure Threshold The compression failure rate threshold for a table. Compression Failure Rate Threshold The maximum percentage that can be reserved as free space within each compressed page, allowing room to reorganize the data and modification log within the page when a compressed table or index is updated and the data might be recompressed. Write Pages to the Redo Log Specifies whether images of re-compressed pages are written to the redo log. Re-compression may occur when changes are made to compressed data.","title":"Compression level and failure rate threshold"},{"location":"details/dashboards/dashboard-mysql-innodb-compression-details.html#statistic-of-compression-operations","text":"Compress Attempts Number of compression operations attempted. Pages are compressed whenever an empty page is created or the space for the uncompressed modification log runs out. Uncompressed Attempts Number of uncompression operations performed. Compressed InnoDB pages are uncompressed whenever compression fails, or the first time a compressed page is accessed in the buffer pool and the uncompressed page does not exist.","title":"Statistic of compression operations"},{"location":"details/dashboards/dashboard-mysql-innodb-compression-details.html#cpu-core-usage","text":"CPU Core Usage for Compression Shows the time in seconds spent by InnoDB Compression operations. CPU Core Usage for Uncompression Shows the time in seconds spent by InnoDB Uncompression operations.","title":"CPU Core Usage"},{"location":"details/dashboards/dashboard-mysql-innodb-compression-details.html#buffer-pool-total","text":"Total Used Pages Shows the total amount of used compressed pages into the InnoDB Buffer Pool split by page size. Total Free Pages Shows the total amount of free compressed pages into the InnoDB Buffer Pool split by page size.","title":"Buffer Pool Total"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html","text":"MySQL InnoDB Details \u00b6 Tip If metrics are missing, try running: SET GLOBAL innodb_monitor_enable=all; in the MySQL client. InnoDB Activity \u00b6 Writes (Rows) \u00b6 Writes (Transactions) \u00b6 Row Writes per Trx \u00b6 Rows Written Per Transactions which modify rows. This is better indicator of transaction write size than looking at all transactions which did not do any writes as well. Rows Read Per Trx \u00b6 Log Space per Trx \u00b6 Rollbacks \u00b6 Percent of Transaction Rollbacks (as portion of read-write transactions). BP Reqs Per Row \u00b6 Number of Buffer Pool requests per Row Access. High numbers here indicate going through long undo chains, deep trees and other inefficient data access. It can be less than zero due to several rows being read from single page. Log Fsync Per Trx \u00b6 Log Fsync Per Transaction. InnoDB Row Reads \u00b6 InnoDB Row Operations \u00b6 This graph allows you to see which operations occur and the number of rows affected per operation. A graph like Queries Per Second will give you an idea of queries, but one query could effect millions of rows. InnoDB Row Writes \u00b6 InnoDB Row Operations \u00b6 This graph allows you to see which operations occur and the number of rows affected per operation. A graph like Queries Per Second will give you an idea of queries, but one query could effect millions of rows. InnoDB Read-Only Transactions \u00b6 InnoDB Read-Write Transactions \u00b6 InnoDB Transactions Information (RW) \u00b6 The InnoDB Transactions Information graph shows details about the recent transactions. Transaction IDs Assigned represents the total number of transactions initiated by InnoDB. RW Transaction Commits are the number of transactions not read-only. Insert-Update Transactions Commits are transactions on the Undo entries. Non Locking RO Transaction Commits are transactions commit from select statement in auto-commit mode or transactions explicitly started with \u201cstart transaction read only\u201d. Misc InnoDB Transactions Information \u00b6 Additional InnoDB Transaction Information InnoDB Storage Summary \u00b6 InnoDB Tables \u00b6 Current Number of InnoDB Tables in database Data Buffer Pool Fit \u00b6 Buffer Pool Size as Portion of the Data Avg Row Size \u00b6 Amount of Data Per Row Index Size Per Row \u00b6 Index Size Per Row shows how much space we\u2019re using for indexes on per row basics InnoDB Data Summary \u00b6 Space Allocated \u00b6 Total Amount of Space Allocated. May not exactly match amount of space used on file system but provided great guidance. Space Used \u00b6 Space used in All InnoDB Tables. Reported Allocated Space Less Free Space. Data Length \u00b6 Space Used by Data (Including Primary Key). Index Length \u00b6 Space Used by Secondary Indexes. Estimated Rows \u00b6 Estimated number of Rows in InnoDB Storage Engine. It is not exact value and it can change abruptly as information is updated. Indexing Overhead \u00b6 How Much Indexes Take Compared to Data. Free Space Percent \u00b6 How Much Space is Free. Too high value wastes space on disk. Free \u00b6 Allocated Space not currently used by Data or Indexes. InnoDB File Per Table \u00b6 If Enabled, By Default every Table will have its own Tablespace represented as its own .idb file rather than all tables stored in single system tablespace. InnoDB Disk IO \u00b6 InnoDB Page Size \u00b6 Avg Data Read Rq Size \u00b6 Avg Data Write Rq Size \u00b6 Avg Log Write Rq Size \u00b6 Data Written Per Fsync \u00b6 Log Written Per Fsync \u00b6 Data Read Per Row Read \u00b6 Data Written Per Row Written \u00b6 Due to difference in timing of Row Write and Data Write the value may be misleading on short intervals. InnoDB Data I/O \u00b6 InnoDB I/O Data Writes - The total number of InnoDB data writes. Data Reads - The total number of InnoDB data reads (OS file reads). Log Writes - The number of physical writes to the InnoDB redo log file. Data Fsyncs - The number of fsync() operations. The frequency of fsync() calls is influenced by the setting of the innodb_flush_method configuration option. InnoDB Data Bandwidth \u00b6 InnoDB Log IO \u00b6 InnoDB I/O Data Writes - The total number of InnoDB data writes. Data Reads - The total number of InnoDB data reads (OS file reads). Log Writes - The number of physical writes to the InnoDB redo log file. Data Fsyncs - The number of fsync() operations. The frequency of fsync() calls is influenced by the setting of the innodb_flush_method configuration option. InnoDB FSyncs \u00b6 InnoDB Pending IO \u00b6 InnoDB Pending Fsyncs \u00b6 InnoDB Auto Extend Increment \u00b6 When Growing InnoDB System Tablespace extend it by this size at the time. InnoDB Double Write \u00b6 Whether InnoDB Double Write Buffer is enabled. Doing so doubles amount of writes InnoDB has to do to storage but is required to avoid potential data corruption during the crash on most storage subsystems. InnoDB Fast Shutdown \u00b6 Fast Shutdown means InnoDB will not perform complete Undo Space and Change Buffer cleanup on shutdown, which is faster but may interfere with certain major upgrade operations. InnoDB Open Files \u00b6 Maximum Number of Files InnoDB is Allowed to use. InnoDB File Use \u00b6 Portion of Allowed InnoDB Open Files Use. InnoDB IO Objects \u00b6 InnoDB IO Targets Write Load \u00b6 Write Load Includes both Write and fsync (referred as misc). InnoDB Buffer Pool \u00b6 Buffer Pool Size \u00b6 InnoDB Buffer Pool Size InnoDB maintains a storage area called the buffer pool for caching data and indexes in memory. Knowing how the InnoDB buffer pool works, and taking advantage of it to keep frequently accessed data in memory, is one of the most important aspects of MySQL tuning. The goal is to keep the working set in memory. In most cases, this should be between 60%-90% of available memory on a dedicated database host, but depends on many factors. Buffer Pool Size of Total RAM \u00b6 InnoDB Buffer Pool Size % of Total RAM InnoDB maintains a storage area called the buffer pool for caching data and indexes in memory. Knowing how the InnoDB buffer pool works, and taking advantage of it to keep frequently accessed data in memory, is one of the most important aspects of MySQL tuning. The goal is to keep the working set in memory. In most cases, this should be between 60%-90% of available memory on a dedicated database host, but depends on many factors. NUMA Interleave \u00b6 Interleave Buffer Pool between NUMA zones to better support NUMA systems. Buffer Pool Activity \u00b6 Combined value of Buffer Pool Read and Write requests. BP Data \u00b6 Percent of Buffer Pool Occupied by Cached Data. BP Data Dirty \u00b6 Percent of Data which is Dirty. BP Miss Ratio \u00b6 How often buffer pool read requests have to do read from the disk. Keep this percent low for good performance. BP Write Buffering \u00b6 Number of Logical Writes to Buffer Pool Per logical Write. InnoDB Buffer Pool LRU Sub-Chain Churn \u00b6 Buffer Pool Chunk Size \u00b6 Size of the \u201cChunk\u201d for buffer pool allocation. Allocation of buffer pool will be rounded by this number. It also affects the performance impact of online buffer pool resize. Buffer Pool Instances \u00b6 Number of Buffer Pool Instances. Higher values allow to reduce contention but also increase overhead. Read Ahead IO Percent \u00b6 Percent of Reads Caused by InnoDB Read Ahead. Read Ahead Wasted \u00b6 Percent of Pages Fetched by Read Ahead Evicted Without Access. Dump Buffer Pool on Shutdown \u00b6 Load Buffer Pool at Startup \u00b6 Portion of Buffer Pool To Dump/Load \u00b6 Larger Portion increases dump/load time but get more of original buffer pool content and hence may reduce warmup time. Include Buffer Pool in Core Dump \u00b6 Whenever to Include Buffer Pool in Crash Core Dumps. Doing so may dramatically increase core dump file slow down restart. Only makes a difference if core dumping on crash is enabled. InnoDB Old Blocks \u00b6 Percent of The Buffer Pool To be Reserved for \u201cOld Blocks\u201d - which has been touched repeatedly over period of time. InnoDB Old Blocks Time \u00b6 The Time which has to pass between multiple touches for the block for it to qualify as old block. InnoDB Random Read Ahead \u00b6 Is InnoDB Random ReadAhead Enabled. InnoDB Random Read Ahead \u00b6 The Threshold (in Pages) to trigger Linear Read Ahead. InnoDB Read IO Threads \u00b6 Number of Threads used to Schedule Reads. InnoDB Write IO Threads \u00b6 Number of Threads used to Schedule Writes. InnoDB Native AIO Enabled \u00b6 Whether Native Asynchronous IO is enabled. Strongly recommended for optimal performance. InnoDB Buffer Pool - Replacement Management \u00b6 LRU Scan Depth \u00b6 InnoDB LRU Scan Depth This variable defines InnoDB Free Page Target per buffer pool. When number of free pages falls below this number this number page cleaner will make required amount of pages free, flushing or evicting pages from the tail of LRU as needed. LRU Clean Page Searches \u00b6 When Page is being read (or created) the Page need to be allocated in Buffer Pool. Free List Miss Rate \u00b6 The most efficient way to get a clean page is to grab one from free list. However if no pages are available in Free List the LRU scan needs to be performed. LRU Get Free Loops \u00b6 If Free List was empty LRU Get Free Loop will be performed. It may perform LRU scan or may use some other heuristics and shortcuts to get free page. LRU Scans \u00b6 If Page could not be find any Free list and other shortcuts did not work, free page will be searched by scanning LRU chain which is not efficient. Pages Scanned in LRU Scans \u00b6 Pages Scanned Per Second while doing LRU scans. If this value is large (thousands) it means a lot of resources are wasted. Pages scanned per LRU Scan \u00b6 Number of pages scanned per LRU scan in Average. Large number of scans can consume a lot of resources and also introduce significant addition latency to queries. LRU Get Free Waits \u00b6 If InnoDB could not find a free page in LRU list and had to sleep. Should be zero. InnoDB Checkpointing and Flushing \u00b6 Pages Flushed from Flush List \u00b6 Number of Pages Flushed from \u201cFlush List\u201d This combines Pages Flushed through Adaptive Flush and Background Flush. Page Flush Batches Executed \u00b6 InnoDB Flush Cycle typically Runs on 1 second intervals. If too far off from this number it can indicate an issue. Pages Flushed Per Batch \u00b6 How many pages are flushed per Batch. Large Batches can \u201cchoke\u201d IO subsystem and starve other IO which needs to happen. Neighbor Flushing Enabled \u00b6 Neighbor Flushing is Optimized for Rotational Media and unless you\u2019re Running spinning disks you should disable it. InnoDB Checkpoint Age \u00b6 InnoDB Checkpoint Age The maximum checkpoint age is determined by the total length of all transaction log files ( innodb_log_file_size ). When the checkpoint age reaches the maximum checkpoint age, blocks are flushed synchronously. The rules of the thumb is to keep one hour of traffic in those logs and let the check-pointing perform its work as smooth as possible. If you don\u2019t do this, InnoDB will do synchronous flushing at the worst possible time, i.e., when you are busiest. Pages Flushed (Adaptive) \u00b6 Adaptive Flush Flushes pages from Flush List based on the need to advance Checkpoint (driven by Redo Generation Rate) and by maintaining number of dirty pages within set limit. Adaptive Flush Batches Executed \u00b6 Pages Per Batch (Adaptive) \u00b6 Pages Flushed Per Adaptive Batch. Neighbor Flushing \u00b6 To optimize IO for rotational Media InnoDB may flush neighbor pages. It can cause significant wasted IO for flash storage. Generally for flash you should run with innodb_flush_neighbors=0 but otherwise this shows how much IO you\u2019re wasting. Pages Flushed (LRU) \u00b6 Flushing from the tail of the LRU list is needed to keep readily-available free pages for new data to be read when data does not fit in the buffer pool. LRU Flush Batches Executed \u00b6 Pages Per Batch (LRU) \u00b6 Pages Flushed Per Neighbor. LSN Age Flush Batch Target \u00b6 Target for Pages to Flush due to LSN Age. Pages Flushed (Neighbor) \u00b6 Number of Neighbor pages flushed (If neighbor flushing is enabled) from Flush List and LRU List Combined. Neighbor Flush Batches Executed \u00b6 Pages Per Batch (Neighbor) \u00b6 Pages Flushed Per Neighbor. Sync Flush Waits \u00b6 If InnoDB could not keep up with Checkpoint Flushing and had to trigger Sync flush. This should never happen. Pages Flushed (Background) \u00b6 Pages Flushed by Background Flush which is activated when server is considered to be idle. Background Flush Batches Executed \u00b6 Pages Per Batch (Background) \u00b6 Pages Flushed Per Background Batch. Redo Generation Rate \u00b6 Rate at which LSN (Redo) is Created. It may not match how much data is written to log files due to block size rounding. InnoDB Flushing by Type \u00b6 Pages Evicted (LRU) \u00b6 This correspond to number of clean pages which were evicted (made free) from the tail of LRU buffer. Page Eviction Batches \u00b6 Pages Evicted per Batch \u00b6 Max Log Space Used \u00b6 Single Page Flushes \u00b6 Single Page flushes happen in rare case, then clean page could not be found in LRU list. It should be zero for most workloads. Single Page Flush Pages Scanned \u00b6 Pages Scanned Per Single Page Flush \u00b6 InnoDB IO Capacity \u00b6 Estimated number of IOPS storage system can provide. Is used to scale background activities. Do not set it to actual storage capacity. InnoDB IO Capacity Max \u00b6 InnoDB IO Capacity to use when falling behind and need to catch up with Flushing. InnoDB Logging \u00b6 Total Log Space \u00b6 Number of InnoDB Log Files Multiplied by Their Size. Log Buffer Size \u00b6 InnoDB Log Buffer Size The size of buffer InnoDB uses for buffering writes to log files. At Transaction Commit \u00b6 What to do with Log file At Transaction Commit. Do nothing and wait for timeout to flush the data from Log Buffer, Flush it to OS Cache but not FSYNC or Flush only. Flush Transaction Log Every \u00b6 Every Specified Number of Seconds Flush Transaction Log. InnoDB Write Ahead Block Size \u00b6 This variable can be seen as minimum IO alignment InnoDB will use for Redo log file. High Values cause waste, low values can make IO less efficient. Log Write Amplification \u00b6 How much Writes to Log Are Amplified compared to how much Redo is Generated. Log Fsync Rate \u00b6 Redo Generated per Trx \u00b6 Amount of Redo Generated Per Write Transaction. This is a good indicator of transaction size. InnoDB Log File Usage Hourly \u00b6 InnoDB Log File Usage Hourly Along with the buffer pool size, innodb_log_file_size is the most important setting when we are working with InnoDB. This graph shows how much data was written to InnoDB\u2019s redo logs over each hour. When the InnoDB log files are full, InnoDB needs to flush the modified pages from memory to disk. The rules of the thumb is to keep one hour of traffic in those logs and let the checkpointing perform its work as smooth as possible. If you don\u2019t do this, InnoDB will do synchronous flushing at the worst possible time, i.e., when you are busiest. This graph can help guide you in setting the correct innodb_log_file_size . Log Padding Written \u00b6 Amount of Log Padding Written. InnoDB Log File Size \u00b6 InnoDB Log Files \u00b6 Number of InnoDB Redo Log Files. Log Bandwidth \u00b6 Redo Generation Rate \u00b6 Rate at which LSN (Redo) is Created. It may not match how much data is written to log files due to block size rounding. InnoDB Group Commit Batch Size \u00b6 The InnoDB Group Commit Batch Size graph shows how many bytes were written to the InnoDB log files per attempt to write. If many threads are committing at the same time, one of them will write the log entries of all the waiting threads and flush the file. Such process reduces the number of disk operations needed and enlarge the batch size. InnoDB Locking \u00b6 Lock Wait Timeout \u00b6 InnoDB Lock Wait Timeout How long to wait for row lock before timing out. InnoDB Deadlock Detection \u00b6 If Disabled InnoDB Will not detect deadlocks but rely on timeouts. InnoDB Auto Increment Lock Mode \u00b6 Will Define How much locking will come from working with Auto Increment Columns. Rollback on Timeout \u00b6 Whenever to rollback all transaction on timeout or just last statement. Row Lock Blocking \u00b6 Percent of Active Sections which are blocked due to waiting on InnoDB Row Locks. Row Writes per Trx \u00b6 Rows Written Per Transactions which modify rows. This is better indicator of transaction write size than looking at all transactions which did not do any writes as well. Rollbacks \u00b6 Percent of Transaction Rollbacks (as portion of read-write transactions). InnoDB Row Lock Wait Activity \u00b6 InnoDB Row Lock Wait Time \u00b6 InnoDB Row Lock Wait Load \u00b6 Average Number of Sessions blocked from proceeding due to waiting on row level lock. InnoDB Row Locks Activity \u00b6 InnoDB Table Lock Activity \u00b6 Current Locks \u00b6 InnoDB Undo Space and Purging \u00b6 Undo Tablespaces \u00b6 Max Undo Log Size \u00b6 InnoDB Undo Log Truncate \u00b6 Purge Threads \u00b6 Max Purge Lag \u00b6 Maximum number of Unpurged Transactions, if this number exceeded delay will be introduced to incoming DDL statements. Max Purge Lag Delay \u00b6 Current Purge Delay \u00b6 The Delay Injected due to Purge Thread(s) unable to keep up with purge progress. Rollback Segments \u00b6 InnoDB Purge Activity \u00b6 The InnoDB Purge Performance graph shows metrics about the page purging process. The purge process removed the undo entries from the history list and cleanup the pages of the old versions of modified rows and effectively remove deleted rows. Transactions and Undo Records \u00b6 InnoDB Undo Space Usage \u00b6 The InnoDB Undo Space Usage graph shows the amount of space used by the Undo segment. If the amount of space grows too much, look for long running transactions holding read views opened in the InnoDB status. Transaction History \u00b6 InnoDB Purge Throttling \u00b6 Records Per Undo Log Page \u00b6 How Many Undo Operations Are Handled Per Each Undo Log Page. Purge Invoked \u00b6 How Frequently Purge Operation is Invoked. Ops Per Purge \u00b6 Home Many Purge Actions are done Per invocation. Undo Slots Used \u00b6 Number of Undo Slots Used. Max Transaction History Length \u00b6 Purge Batch Size \u00b6 Rseg Truncate Frequency \u00b6 InnoDB Page Operations \u00b6 InnoDB Page Splits and Merges \u00b6 The InnoDB Page Splits graph shows the InnoDB page maintenance activity related to splitting and merging pages. When an InnoDB page, other than the top most leaf page, has too much data to accept a row update or a row insert, it has to be split in two. Similarly, if an InnoDB page, after a row update or delete operation, ends up being less than half full, an attempt is made to merge the page with a neighbor page. If the resulting page size is larger than the InnoDB page size, the operation fails. If your workload causes a large number of page splits, try lowering the innodb_fill_factor variable (5.7+). Page Merge Success Ratio \u00b6 InnoDB Page Reorg Attempts \u00b6 The InnoDB Page Reorgs graph shows information about the page reorganization operations. When a page receives an update or an insert that affect the offset of other rows in the page, a reorganization is needed. If the reorganization process finds out there is not enough room in the page, the page will be split. Page reorganization can only fail for compressed pages. InnoDB Page Reorgs Failures \u00b6 The InnoDB Page Reorgs graph shows information about the page reorganization operations. When a page receives an update or an insert that affect the offset of other rows in the page, a reorganization is needed. If the reorganization process finds out there is not enough room in the page, the page will be split. Page reorganization can only fail for compressed pages. InnoDB Fill Factor \u00b6 The portion of the page to fill then doing sorted Index Build. Lowering this value will worsen space utilization but will reduce need to split pages when new data is inserted in the index. InnoDB Adaptive Hash Index \u00b6 Adaptive Hash Index Enabled \u00b6 Adaptive Hash Index helps to optimize index Look-ups but can be severe hotspot for some workloads. Adaptive Hash Index Partitions \u00b6 How many Partitions Used for Adaptive Hash Index (to reduce contention). Percent of Pages Hashed \u00b6 Number of Pages Added to AHI vs Number of Pages Added to Buffer Pool. AHI Miss Ratio \u00b6 Percent of Searches which could not be resolved through AHI. Rows Added Per Page \u00b6 Number of Rows \u201cHashed\u201d Per Each Page which needs to be added to AHI. AHI ROI \u00b6 How Many Successful Searches using AHI are performed per each row maintenance operation. InnoDB AHI Usage \u00b6 The InnoDB AHI Usage graph shows the search operations on the InnoDB adaptive hash index and its efficiency. The adaptive hash index is a search hash designed to speed access to InnoDB pages in memory. If the Hit Ratio is small, the working data set is larger than the buffer pool, the AHI should likely be disabled. InnoDB AHI Miss Ratio \u00b6 InnoDB AHI Churn - Rows \u00b6 InnoDB AHI Churn - Pages \u00b6 InnoDB Change Buffer \u00b6 Change Buffer Max Size \u00b6 The Maximum Size of Change Buffer (as Percent of Buffer Pool Size). Change Buffer Max Size \u00b6 The Maximum Size of Change Buffer (Bytes). InnoDB Change Buffer Merge Load \u00b6 Number of Average of Active Merge Buffer Operations in Process. InnoDB Contention \u00b6 InnoDB Thread Concurrency \u00b6 If Enabled limits number of Threads allowed inside InnoDB Kernel at the same time. InnoDB Commit Concurrency \u00b6 If Enabled limits number of Threads allowed inside InnoDB Kernel at the same time during Commit Stage. InnoDB Thread Sleep Delay \u00b6 The Time the thread will Sleep before Re-Entering InnoDB Kernel if high contention. InnoDB Adaptive Max Sleep Delay \u00b6 If Set to Non-Zero Value InnoDB Thread Sleep Delay will be adjusted automatically depending on the load up to the value specified by this variable. InnoDB Concurrency Tickets \u00b6 Number of low level operations InnoDB can do after it entered InnoDB kernel before it is forced to exit and yield to another thread waiting. InnoDB Spin Wait Delay \u00b6 InnoDB Spin Wait Pause Multiplier \u00b6 InnoDB Sync Spin Loops \u00b6 InnoDB Contention - OS Waits \u00b6 The InnoDB Contention - OS Waits graph shows the number of time an OS wait operation was required while waiting to get the lock. This happens once the spin rounds are exhausted. InnoDB Contention - Spin Rounds \u00b6 The InnoDB Contention - Spin Rounds graph shows the number of spin rounds executed to get a lock. A spin round is a fast retry to get the lock in a loop. InnoDB Misc \u00b6 InnoDB Main Thread Utilization \u00b6 The InnoDB Main Thread Utilization graph shows the portion of time the InnoDB main thread spent at various task. InnoDB Activity \u00b6 The InnoDB Activity graph shows a measure of the activity of the InnoDB threads. InnoDB Dedicated Server \u00b6 InnoDB automatically optimized for Dedicated Server Environment (auto scaling cache and some other variables). InnoDB Sort Buffer Size \u00b6 This Buffer is used for Building InnoDB Indexes using Sort algorithm. InnoDB Stats Auto Recalc \u00b6 Update Stats when Metadata Queried \u00b6 Refresh InnoDB Statistics when meta-data queries by SHOW TABLE STATUS or INFORMATION_SCHEMA queries. If Enabled can cause severe performance issues. Index Condition Pushdown (ICP) \u00b6 Index Condition Pushdown (ICP) is an optimization for the case where MySQL retrieves rows from a table using an index. Without ICP, the storage engine traverses the index to locate rows in the base table and returns them to the MySQL server which evaluates the WHERE condition for the rows. With ICP enabled, and if parts of the WHERE condition can be evaluated by using only columns from the index, the MySQL server pushes this part of the WHERE condition down to the storage engine. The storage engine then evaluates the pushed index condition by using the index entry and only if this is satisfied is the row read from the table. ICP can reduce the number of times the storage engine must access the base table and the number of times the MySQL server must access the storage engine. InnoDB Persistent Statistics \u00b6 InnoDB Persistent Sample Pages \u00b6 Number of Pages To Sample if Persistent Statistics are Enabled. InnoDB Transient Sample Pages \u00b6 Number of Pages To Sample if Persistent Statistics are Disabled. InnoDB Online Operations (MariaDB) \u00b6 InnoDB Defragmentation \u00b6 The InnoDB Defragmentation graph shows the status information related to the InnoDB online defragmentation feature of MariaDB for the optimize table command. To enable this feature, the variable innodb-defragment must be set to 1 in the configuration file. Currently available only on a MariaDB server. InnoDB Online DDL \u00b6 The InnoDB Online DDL graph shows the state of the online DDL (alter table) operations in InnoDB. The progress metric is estimate of the percentage of the rows processed by the online DDL. Currently available only on a MariaDB server. MySQL Summary \u00b6 MySQL Uptime \u00b6 MySQL Uptime The amount of time since the last restart of the MySQL server process. Current QPS \u00b6 Current QPS Based on the queries reported by MySQL\u2019s SHOW STATUS command, it is the number of statements executed by the server within the last second. This variable includes statements executed within stored programs, unlike the Questions variable. It does not count COM_PING or COM_STATISTICS commands. File Handlers Used \u00b6 Table Open Cache Miss Ratio \u00b6 Table Open Cache Size \u00b6 Table Definition Cache Size \u00b6 MySQL Connections \u00b6 Max Connections Max Connections is the maximum permitted number of simultaneous client connections. By default, this is 151. Increasing this value increases the number of file descriptors that mysqld requires. If the required number of descriptors are not available, the server reduces the value of Max Connections. mysqld actually permits Max Connections + 1 clients to connect. The extra connection is reserved for use by accounts that have the SUPER privilege, such as root. Max Used Connections is the maximum number of connections that have been in use simultaneously since the server started. Connections is the number of connection attempts (successful or not) to the MySQL server. MySQL Client Thread Activity \u00b6 MySQL Active Threads Threads Connected is the number of open connections, while Threads Running is the number of threads not sleeping. MySQL Handlers \u00b6 MySQL Handlers Handler statistics are internal statistics on how MySQL is selecting, updating, inserting, and modifying rows, tables, and indexes. This is in fact the layer between the Storage Engine and MySQL. read_rnd_next is incremented when the server performs a full table scan and this is a counter you don\u2019t really want to see with a high value. read_key is incremented when a read is done with an index. read_next is incremented when the storage engine is asked to \u2018read the next index entry\u2019. A high value means a lot of index scans are being done. Top Command Counters \u00b6 Top Command Counters The Com_{{ xxx }} statement counter variables indicate the number of times each xxx statement has been executed. There is one status variable for each type of statement. For example, Com_delete and Com_update count DELETE and UPDATE statements, respectively. Com_delete_multi and Com_update_multi are similar but apply to DELETE and UPDATE statements that use multiple-table syntax. MySQL Network Traffic \u00b6 MySQL Network Traffic Here we can see how much network traffic is generated by MySQL. Outbound is network traffic sent from MySQL and Inbound is network traffic MySQL has received. Node Summary \u00b6 System Uptime \u00b6 The parameter shows how long a system has been up and running without a shut down or restart. Load Average \u00b6 The system load is a measurement of the computational work the system is performing. Each running process either using or waiting for CPU resources adds 1 to the load. RAM \u00b6 RAM (Random Access Memory) is the hardware in a computing device where the operating system, application programs and data in current use are kept so they can be quickly reached by the device\u2019s processor. Memory Available \u00b6 Percent of Memory Available On Modern Linux Kernels amount of Memory Available for application is not the same as Free+Cached+Buffers. Virtual Memory \u00b6 RAM + SWAP Disk Space \u00b6 Sum of disk space on all partitions. It can be significantly over-reported in some installations. Min Space Available \u00b6 Lowest percent of the disk space available. CPU Usage \u00b6 The CPU time is measured in clock ticks or seconds. It is useful to measure CPU time as a percentage of the CPU\u2019s capacity, which is called the CPU usage. CPU Saturation and Max Core Usage \u00b6 When a system is running with maximum CPU utilization, the transmitting and receiving threads must all share the available CPU. This will cause data to be queued more frequently to cope with the lack of CPU. CPU Saturation may be measured as the length of a wait queue, or the time spent waiting on the queue. Disk I/O and Swap Activity \u00b6 Disk I/O includes read or write or input/output operations involving a physical disk. It is the speed with which the data transfer takes place between the hard disk drive and RAM. Swap Activity is memory management that involves swapping sections of memory to and from physical storage. Network Traffic \u00b6 Network traffic refers to the amount of data moving across a network at a given point in time.","title":"MySQL InnoDB Details"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#mysql-innodb-details","text":"Tip If metrics are missing, try running: SET GLOBAL innodb_monitor_enable=all; in the MySQL client.","title":"MySQL InnoDB Details"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-activity","text":"","title":"InnoDB Activity"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#writes-rows","text":"","title":"Writes (Rows)"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#writes-transactions","text":"","title":"Writes (Transactions)"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#row-writes-per-trx","text":"Rows Written Per Transactions which modify rows. This is better indicator of transaction write size than looking at all transactions which did not do any writes as well.","title":"Row Writes per Trx"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#rows-read-per-trx","text":"","title":"Rows Read Per Trx"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#log-space-per-trx","text":"","title":"Log Space per Trx"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#rollbacks","text":"Percent of Transaction Rollbacks (as portion of read-write transactions).","title":"Rollbacks"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#bp-reqs-per-row","text":"Number of Buffer Pool requests per Row Access. High numbers here indicate going through long undo chains, deep trees and other inefficient data access. It can be less than zero due to several rows being read from single page.","title":"BP Reqs Per Row"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#log-fsync-per-trx","text":"Log Fsync Per Transaction.","title":"Log Fsync Per Trx"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-row-reads","text":"","title":"InnoDB Row Reads"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-row-operations","text":"This graph allows you to see which operations occur and the number of rows affected per operation. A graph like Queries Per Second will give you an idea of queries, but one query could effect millions of rows.","title":"InnoDB Row Operations"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-row-writes","text":"","title":"InnoDB Row Writes"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-row-operations_1","text":"This graph allows you to see which operations occur and the number of rows affected per operation. A graph like Queries Per Second will give you an idea of queries, but one query could effect millions of rows.","title":"InnoDB Row Operations"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-read-only-transactions","text":"","title":"InnoDB Read-Only Transactions"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-read-write-transactions","text":"","title":"InnoDB Read-Write Transactions"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-transactions-information-rw","text":"The InnoDB Transactions Information graph shows details about the recent transactions. Transaction IDs Assigned represents the total number of transactions initiated by InnoDB. RW Transaction Commits are the number of transactions not read-only. Insert-Update Transactions Commits are transactions on the Undo entries. Non Locking RO Transaction Commits are transactions commit from select statement in auto-commit mode or transactions explicitly started with \u201cstart transaction read only\u201d.","title":"InnoDB Transactions Information (RW)"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#misc-innodb-transactions-information","text":"Additional InnoDB Transaction Information","title":"Misc InnoDB Transactions Information"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-storage-summary","text":"","title":"InnoDB Storage Summary"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-tables","text":"Current Number of InnoDB Tables in database","title":"InnoDB Tables"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#data-buffer-pool-fit","text":"Buffer Pool Size as Portion of the Data","title":"Data Buffer Pool Fit"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#avg-row-size","text":"Amount of Data Per Row","title":"Avg Row Size"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#index-size-per-row","text":"Index Size Per Row shows how much space we\u2019re using for indexes on per row basics","title":"Index Size Per Row"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-data-summary","text":"","title":"InnoDB Data Summary"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#space-allocated","text":"Total Amount of Space Allocated. May not exactly match amount of space used on file system but provided great guidance.","title":"Space Allocated"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#space-used","text":"Space used in All InnoDB Tables. Reported Allocated Space Less Free Space.","title":"Space Used"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#data-length","text":"Space Used by Data (Including Primary Key).","title":"Data Length"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#index-length","text":"Space Used by Secondary Indexes.","title":"Index Length"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#estimated-rows","text":"Estimated number of Rows in InnoDB Storage Engine. It is not exact value and it can change abruptly as information is updated.","title":"Estimated Rows"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#indexing-overhead","text":"How Much Indexes Take Compared to Data.","title":"Indexing Overhead"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#free-space-percent","text":"How Much Space is Free. Too high value wastes space on disk.","title":"Free Space Percent"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#free","text":"Allocated Space not currently used by Data or Indexes.","title":"Free"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-file-per-table","text":"If Enabled, By Default every Table will have its own Tablespace represented as its own .idb file rather than all tables stored in single system tablespace.","title":"InnoDB File Per Table"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-disk-io","text":"","title":"InnoDB Disk IO"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-page-size","text":"","title":"InnoDB Page Size"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#avg-data-read-rq-size","text":"","title":"Avg Data Read Rq Size"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#avg-data-write-rq-size","text":"","title":"Avg Data Write Rq Size"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#avg-log-write-rq-size","text":"","title":"Avg Log Write Rq Size"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#data-written-per-fsync","text":"","title":"Data Written Per Fsync"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#log-written-per-fsync","text":"","title":"Log Written Per Fsync"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#data-read-per-row-read","text":"","title":"Data Read Per Row Read"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#data-written-per-row-written","text":"Due to difference in timing of Row Write and Data Write the value may be misleading on short intervals.","title":"Data Written Per Row Written"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-data-io","text":"InnoDB I/O Data Writes - The total number of InnoDB data writes. Data Reads - The total number of InnoDB data reads (OS file reads). Log Writes - The number of physical writes to the InnoDB redo log file. Data Fsyncs - The number of fsync() operations. The frequency of fsync() calls is influenced by the setting of the innodb_flush_method configuration option.","title":"InnoDB Data I/O"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-data-bandwidth","text":"","title":"InnoDB Data Bandwidth"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-log-io","text":"InnoDB I/O Data Writes - The total number of InnoDB data writes. Data Reads - The total number of InnoDB data reads (OS file reads). Log Writes - The number of physical writes to the InnoDB redo log file. Data Fsyncs - The number of fsync() operations. The frequency of fsync() calls is influenced by the setting of the innodb_flush_method configuration option.","title":"InnoDB Log IO"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-fsyncs","text":"","title":"InnoDB FSyncs"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-pending-io","text":"","title":"InnoDB Pending IO"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-pending-fsyncs","text":"","title":"InnoDB Pending Fsyncs"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-auto-extend-increment","text":"When Growing InnoDB System Tablespace extend it by this size at the time.","title":"InnoDB Auto Extend Increment"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-double-write","text":"Whether InnoDB Double Write Buffer is enabled. Doing so doubles amount of writes InnoDB has to do to storage but is required to avoid potential data corruption during the crash on most storage subsystems.","title":"InnoDB Double Write"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-fast-shutdown","text":"Fast Shutdown means InnoDB will not perform complete Undo Space and Change Buffer cleanup on shutdown, which is faster but may interfere with certain major upgrade operations.","title":"InnoDB Fast Shutdown"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-open-files","text":"Maximum Number of Files InnoDB is Allowed to use.","title":"InnoDB Open Files"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-file-use","text":"Portion of Allowed InnoDB Open Files Use.","title":"InnoDB File Use"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-io-objects","text":"","title":"InnoDB IO Objects"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-io-targets-write-load","text":"Write Load Includes both Write and fsync (referred as misc).","title":"InnoDB IO Targets Write Load"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-buffer-pool","text":"","title":"InnoDB Buffer Pool"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#buffer-pool-size","text":"InnoDB Buffer Pool Size InnoDB maintains a storage area called the buffer pool for caching data and indexes in memory. Knowing how the InnoDB buffer pool works, and taking advantage of it to keep frequently accessed data in memory, is one of the most important aspects of MySQL tuning. The goal is to keep the working set in memory. In most cases, this should be between 60%-90% of available memory on a dedicated database host, but depends on many factors.","title":"Buffer Pool Size"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#buffer-pool-size-of-total-ram","text":"InnoDB Buffer Pool Size % of Total RAM InnoDB maintains a storage area called the buffer pool for caching data and indexes in memory. Knowing how the InnoDB buffer pool works, and taking advantage of it to keep frequently accessed data in memory, is one of the most important aspects of MySQL tuning. The goal is to keep the working set in memory. In most cases, this should be between 60%-90% of available memory on a dedicated database host, but depends on many factors.","title":"Buffer Pool Size of Total RAM"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#numa-interleave","text":"Interleave Buffer Pool between NUMA zones to better support NUMA systems.","title":"NUMA Interleave"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#buffer-pool-activity","text":"Combined value of Buffer Pool Read and Write requests.","title":"Buffer Pool Activity"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#bp-data","text":"Percent of Buffer Pool Occupied by Cached Data.","title":"BP Data"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#bp-data-dirty","text":"Percent of Data which is Dirty.","title":"BP Data Dirty"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#bp-miss-ratio","text":"How often buffer pool read requests have to do read from the disk. Keep this percent low for good performance.","title":"BP Miss Ratio"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#bp-write-buffering","text":"Number of Logical Writes to Buffer Pool Per logical Write.","title":"BP Write Buffering"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-buffer-pool-lru-sub-chain-churn","text":"","title":"InnoDB Buffer Pool LRU Sub-Chain Churn"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#buffer-pool-chunk-size","text":"Size of the \u201cChunk\u201d for buffer pool allocation. Allocation of buffer pool will be rounded by this number. It also affects the performance impact of online buffer pool resize.","title":"Buffer Pool Chunk Size"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#buffer-pool-instances","text":"Number of Buffer Pool Instances. Higher values allow to reduce contention but also increase overhead.","title":"Buffer Pool Instances"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#read-ahead-io-percent","text":"Percent of Reads Caused by InnoDB Read Ahead.","title":"Read Ahead IO Percent"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#read-ahead-wasted","text":"Percent of Pages Fetched by Read Ahead Evicted Without Access.","title":"Read Ahead Wasted"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#dump-buffer-pool-on-shutdown","text":"","title":"Dump Buffer Pool on Shutdown"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#load-buffer-pool-at-startup","text":"","title":"Load Buffer Pool at Startup"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#portion-of-buffer-pool-to-dumpload","text":"Larger Portion increases dump/load time but get more of original buffer pool content and hence may reduce warmup time.","title":"Portion of Buffer Pool To Dump/Load"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#include-buffer-pool-in-core-dump","text":"Whenever to Include Buffer Pool in Crash Core Dumps. Doing so may dramatically increase core dump file slow down restart. Only makes a difference if core dumping on crash is enabled.","title":"Include Buffer Pool in Core Dump"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-old-blocks","text":"Percent of The Buffer Pool To be Reserved for \u201cOld Blocks\u201d - which has been touched repeatedly over period of time.","title":"InnoDB Old Blocks"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-old-blocks-time","text":"The Time which has to pass between multiple touches for the block for it to qualify as old block.","title":"InnoDB Old Blocks Time"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-random-read-ahead","text":"Is InnoDB Random ReadAhead Enabled.","title":"InnoDB Random Read Ahead"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-random-read-ahead_1","text":"The Threshold (in Pages) to trigger Linear Read Ahead.","title":"InnoDB Random Read Ahead"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-read-io-threads","text":"Number of Threads used to Schedule Reads.","title":"InnoDB Read IO Threads"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-write-io-threads","text":"Number of Threads used to Schedule Writes.","title":"InnoDB Write IO Threads"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-native-aio-enabled","text":"Whether Native Asynchronous IO is enabled. Strongly recommended for optimal performance.","title":"InnoDB Native AIO Enabled"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-buffer-pool-replacement-management","text":"","title":"InnoDB Buffer Pool - Replacement Management"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#lru-scan-depth","text":"InnoDB LRU Scan Depth This variable defines InnoDB Free Page Target per buffer pool. When number of free pages falls below this number this number page cleaner will make required amount of pages free, flushing or evicting pages from the tail of LRU as needed.","title":"LRU Scan Depth"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#lru-clean-page-searches","text":"When Page is being read (or created) the Page need to be allocated in Buffer Pool.","title":"LRU Clean Page Searches"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#free-list-miss-rate","text":"The most efficient way to get a clean page is to grab one from free list. However if no pages are available in Free List the LRU scan needs to be performed.","title":"Free List Miss Rate"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#lru-get-free-loops","text":"If Free List was empty LRU Get Free Loop will be performed. It may perform LRU scan or may use some other heuristics and shortcuts to get free page.","title":"LRU Get Free Loops"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#lru-scans","text":"If Page could not be find any Free list and other shortcuts did not work, free page will be searched by scanning LRU chain which is not efficient.","title":"LRU Scans"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#pages-scanned-in-lru-scans","text":"Pages Scanned Per Second while doing LRU scans. If this value is large (thousands) it means a lot of resources are wasted.","title":"Pages Scanned in LRU Scans"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#pages-scanned-per-lru-scan","text":"Number of pages scanned per LRU scan in Average. Large number of scans can consume a lot of resources and also introduce significant addition latency to queries.","title":"Pages scanned per LRU Scan"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#lru-get-free-waits","text":"If InnoDB could not find a free page in LRU list and had to sleep. Should be zero.","title":"LRU Get Free Waits"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-checkpointing-and-flushing","text":"","title":"InnoDB Checkpointing and Flushing"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#pages-flushed-from-flush-list","text":"Number of Pages Flushed from \u201cFlush List\u201d This combines Pages Flushed through Adaptive Flush and Background Flush.","title":"Pages Flushed from Flush List"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#page-flush-batches-executed","text":"InnoDB Flush Cycle typically Runs on 1 second intervals. If too far off from this number it can indicate an issue.","title":"Page Flush Batches Executed"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#pages-flushed-per-batch","text":"How many pages are flushed per Batch. Large Batches can \u201cchoke\u201d IO subsystem and starve other IO which needs to happen.","title":"Pages Flushed Per Batch"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#neighbor-flushing-enabled","text":"Neighbor Flushing is Optimized for Rotational Media and unless you\u2019re Running spinning disks you should disable it.","title":"Neighbor Flushing Enabled"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-checkpoint-age","text":"InnoDB Checkpoint Age The maximum checkpoint age is determined by the total length of all transaction log files ( innodb_log_file_size ). When the checkpoint age reaches the maximum checkpoint age, blocks are flushed synchronously. The rules of the thumb is to keep one hour of traffic in those logs and let the check-pointing perform its work as smooth as possible. If you don\u2019t do this, InnoDB will do synchronous flushing at the worst possible time, i.e., when you are busiest.","title":"InnoDB Checkpoint Age"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#pages-flushed-adaptive","text":"Adaptive Flush Flushes pages from Flush List based on the need to advance Checkpoint (driven by Redo Generation Rate) and by maintaining number of dirty pages within set limit.","title":"Pages Flushed (Adaptive)"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#adaptive-flush-batches-executed","text":"","title":"Adaptive Flush Batches Executed"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#pages-per-batch-adaptive","text":"Pages Flushed Per Adaptive Batch.","title":"Pages Per Batch (Adaptive)"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#neighbor-flushing","text":"To optimize IO for rotational Media InnoDB may flush neighbor pages. It can cause significant wasted IO for flash storage. Generally for flash you should run with innodb_flush_neighbors=0 but otherwise this shows how much IO you\u2019re wasting.","title":"Neighbor Flushing"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#pages-flushed-lru","text":"Flushing from the tail of the LRU list is needed to keep readily-available free pages for new data to be read when data does not fit in the buffer pool.","title":"Pages Flushed (LRU)"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#lru-flush-batches-executed","text":"","title":"LRU Flush Batches Executed"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#pages-per-batch-lru","text":"Pages Flushed Per Neighbor.","title":"Pages Per Batch (LRU)"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#lsn-age-flush-batch-target","text":"Target for Pages to Flush due to LSN Age.","title":"LSN Age Flush Batch Target"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#pages-flushed-neighbor","text":"Number of Neighbor pages flushed (If neighbor flushing is enabled) from Flush List and LRU List Combined.","title":"Pages Flushed (Neighbor)"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#neighbor-flush-batches-executed","text":"","title":"Neighbor Flush Batches Executed"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#pages-per-batch-neighbor","text":"Pages Flushed Per Neighbor.","title":"Pages Per Batch (Neighbor)"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#sync-flush-waits","text":"If InnoDB could not keep up with Checkpoint Flushing and had to trigger Sync flush. This should never happen.","title":"Sync Flush Waits"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#pages-flushed-background","text":"Pages Flushed by Background Flush which is activated when server is considered to be idle.","title":"Pages Flushed (Background)"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#background-flush-batches-executed","text":"","title":"Background Flush Batches Executed"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#pages-per-batch-background","text":"Pages Flushed Per Background Batch.","title":"Pages Per Batch (Background)"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#redo-generation-rate","text":"Rate at which LSN (Redo) is Created. It may not match how much data is written to log files due to block size rounding.","title":"Redo Generation Rate"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-flushing-by-type","text":"","title":"InnoDB Flushing by Type"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#pages-evicted-lru","text":"This correspond to number of clean pages which were evicted (made free) from the tail of LRU buffer.","title":"Pages Evicted (LRU)"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#page-eviction-batches","text":"","title":"Page Eviction Batches"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#pages-evicted-per-batch","text":"","title":"Pages Evicted per Batch"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#max-log-space-used","text":"","title":"Max Log Space Used"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#single-page-flushes","text":"Single Page flushes happen in rare case, then clean page could not be found in LRU list. It should be zero for most workloads.","title":"Single Page Flushes"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#single-page-flush-pages-scanned","text":"","title":"Single Page Flush Pages Scanned"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#pages-scanned-per-single-page-flush","text":"","title":"Pages Scanned Per Single Page Flush"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-io-capacity","text":"Estimated number of IOPS storage system can provide. Is used to scale background activities. Do not set it to actual storage capacity.","title":"InnoDB IO Capacity"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-io-capacity-max","text":"InnoDB IO Capacity to use when falling behind and need to catch up with Flushing.","title":"InnoDB IO Capacity Max"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-logging","text":"","title":"InnoDB Logging"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#total-log-space","text":"Number of InnoDB Log Files Multiplied by Their Size.","title":"Total Log Space"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#log-buffer-size","text":"InnoDB Log Buffer Size The size of buffer InnoDB uses for buffering writes to log files.","title":"Log Buffer Size"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#at-transaction-commit","text":"What to do with Log file At Transaction Commit. Do nothing and wait for timeout to flush the data from Log Buffer, Flush it to OS Cache but not FSYNC or Flush only.","title":"At Transaction Commit"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#flush-transaction-log-every","text":"Every Specified Number of Seconds Flush Transaction Log.","title":"Flush Transaction Log Every"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-write-ahead-block-size","text":"This variable can be seen as minimum IO alignment InnoDB will use for Redo log file. High Values cause waste, low values can make IO less efficient.","title":"InnoDB Write Ahead Block Size"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#log-write-amplification","text":"How much Writes to Log Are Amplified compared to how much Redo is Generated.","title":"Log Write Amplification"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#log-fsync-rate","text":"","title":"Log Fsync Rate"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#redo-generated-per-trx","text":"Amount of Redo Generated Per Write Transaction. This is a good indicator of transaction size.","title":"Redo Generated per Trx"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-log-file-usage-hourly","text":"InnoDB Log File Usage Hourly Along with the buffer pool size, innodb_log_file_size is the most important setting when we are working with InnoDB. This graph shows how much data was written to InnoDB\u2019s redo logs over each hour. When the InnoDB log files are full, InnoDB needs to flush the modified pages from memory to disk. The rules of the thumb is to keep one hour of traffic in those logs and let the checkpointing perform its work as smooth as possible. If you don\u2019t do this, InnoDB will do synchronous flushing at the worst possible time, i.e., when you are busiest. This graph can help guide you in setting the correct innodb_log_file_size .","title":"InnoDB Log File Usage Hourly"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#log-padding-written","text":"Amount of Log Padding Written.","title":"Log Padding Written"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-log-file-size","text":"","title":"InnoDB Log File Size"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-log-files","text":"Number of InnoDB Redo Log Files.","title":"InnoDB Log Files"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#log-bandwidth","text":"","title":"Log Bandwidth"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#redo-generation-rate_1","text":"Rate at which LSN (Redo) is Created. It may not match how much data is written to log files due to block size rounding.","title":"Redo Generation Rate"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-group-commit-batch-size","text":"The InnoDB Group Commit Batch Size graph shows how many bytes were written to the InnoDB log files per attempt to write. If many threads are committing at the same time, one of them will write the log entries of all the waiting threads and flush the file. Such process reduces the number of disk operations needed and enlarge the batch size.","title":"InnoDB Group Commit Batch Size"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-locking","text":"","title":"InnoDB Locking"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#lock-wait-timeout","text":"InnoDB Lock Wait Timeout How long to wait for row lock before timing out.","title":"Lock Wait Timeout"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-deadlock-detection","text":"If Disabled InnoDB Will not detect deadlocks but rely on timeouts.","title":"InnoDB Deadlock Detection"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-auto-increment-lock-mode","text":"Will Define How much locking will come from working with Auto Increment Columns.","title":"InnoDB Auto Increment Lock Mode"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#rollback-on-timeout","text":"Whenever to rollback all transaction on timeout or just last statement.","title":"Rollback on Timeout"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#row-lock-blocking","text":"Percent of Active Sections which are blocked due to waiting on InnoDB Row Locks.","title":"Row Lock Blocking"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#row-writes-per-trx_1","text":"Rows Written Per Transactions which modify rows. This is better indicator of transaction write size than looking at all transactions which did not do any writes as well.","title":"Row Writes per Trx"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#rollbacks_1","text":"Percent of Transaction Rollbacks (as portion of read-write transactions).","title":"Rollbacks"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-row-lock-wait-activity","text":"","title":"InnoDB Row Lock Wait Activity"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-row-lock-wait-time","text":"","title":"InnoDB Row Lock Wait Time"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-row-lock-wait-load","text":"Average Number of Sessions blocked from proceeding due to waiting on row level lock.","title":"InnoDB Row Lock Wait Load"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-row-locks-activity","text":"","title":"InnoDB Row Locks Activity"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-table-lock-activity","text":"","title":"InnoDB Table Lock Activity"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#current-locks","text":"","title":"Current Locks"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-undo-space-and-purging","text":"","title":"InnoDB Undo Space and Purging"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#undo-tablespaces","text":"","title":"Undo Tablespaces"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#max-undo-log-size","text":"","title":"Max Undo Log Size"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-undo-log-truncate","text":"","title":"InnoDB Undo Log Truncate"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#purge-threads","text":"","title":"Purge Threads"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#max-purge-lag","text":"Maximum number of Unpurged Transactions, if this number exceeded delay will be introduced to incoming DDL statements.","title":"Max Purge Lag"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#max-purge-lag-delay","text":"","title":"Max Purge Lag Delay"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#current-purge-delay","text":"The Delay Injected due to Purge Thread(s) unable to keep up with purge progress.","title":"Current Purge Delay"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#rollback-segments","text":"","title":"Rollback Segments"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-purge-activity","text":"The InnoDB Purge Performance graph shows metrics about the page purging process. The purge process removed the undo entries from the history list and cleanup the pages of the old versions of modified rows and effectively remove deleted rows.","title":"InnoDB Purge Activity"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#transactions-and-undo-records","text":"","title":"Transactions and Undo Records"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-undo-space-usage","text":"The InnoDB Undo Space Usage graph shows the amount of space used by the Undo segment. If the amount of space grows too much, look for long running transactions holding read views opened in the InnoDB status.","title":"InnoDB Undo Space Usage"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#transaction-history","text":"","title":"Transaction History"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-purge-throttling","text":"","title":"InnoDB Purge Throttling"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#records-per-undo-log-page","text":"How Many Undo Operations Are Handled Per Each Undo Log Page.","title":"Records Per Undo Log Page"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#purge-invoked","text":"How Frequently Purge Operation is Invoked.","title":"Purge Invoked"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#ops-per-purge","text":"Home Many Purge Actions are done Per invocation.","title":"Ops Per Purge"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#undo-slots-used","text":"Number of Undo Slots Used.","title":"Undo Slots Used"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#max-transaction-history-length","text":"","title":"Max Transaction History Length"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#purge-batch-size","text":"","title":"Purge Batch Size"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#rseg-truncate-frequency","text":"","title":"Rseg Truncate Frequency"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-page-operations","text":"","title":"InnoDB Page Operations"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-page-splits-and-merges","text":"The InnoDB Page Splits graph shows the InnoDB page maintenance activity related to splitting and merging pages. When an InnoDB page, other than the top most leaf page, has too much data to accept a row update or a row insert, it has to be split in two. Similarly, if an InnoDB page, after a row update or delete operation, ends up being less than half full, an attempt is made to merge the page with a neighbor page. If the resulting page size is larger than the InnoDB page size, the operation fails. If your workload causes a large number of page splits, try lowering the innodb_fill_factor variable (5.7+).","title":"InnoDB Page Splits and Merges"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#page-merge-success-ratio","text":"","title":"Page Merge Success Ratio"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-page-reorg-attempts","text":"The InnoDB Page Reorgs graph shows information about the page reorganization operations. When a page receives an update or an insert that affect the offset of other rows in the page, a reorganization is needed. If the reorganization process finds out there is not enough room in the page, the page will be split. Page reorganization can only fail for compressed pages.","title":"InnoDB Page Reorg Attempts"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-page-reorgs-failures","text":"The InnoDB Page Reorgs graph shows information about the page reorganization operations. When a page receives an update or an insert that affect the offset of other rows in the page, a reorganization is needed. If the reorganization process finds out there is not enough room in the page, the page will be split. Page reorganization can only fail for compressed pages.","title":"InnoDB Page Reorgs Failures"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-fill-factor","text":"The portion of the page to fill then doing sorted Index Build. Lowering this value will worsen space utilization but will reduce need to split pages when new data is inserted in the index.","title":"InnoDB Fill Factor"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-adaptive-hash-index","text":"","title":"InnoDB Adaptive Hash Index"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#adaptive-hash-index-enabled","text":"Adaptive Hash Index helps to optimize index Look-ups but can be severe hotspot for some workloads.","title":"Adaptive Hash Index Enabled"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#adaptive-hash-index-partitions","text":"How many Partitions Used for Adaptive Hash Index (to reduce contention).","title":"Adaptive Hash Index Partitions"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#percent-of-pages-hashed","text":"Number of Pages Added to AHI vs Number of Pages Added to Buffer Pool.","title":"Percent of Pages Hashed"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#ahi-miss-ratio","text":"Percent of Searches which could not be resolved through AHI.","title":"AHI Miss Ratio"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#rows-added-per-page","text":"Number of Rows \u201cHashed\u201d Per Each Page which needs to be added to AHI.","title":"Rows Added Per Page"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#ahi-roi","text":"How Many Successful Searches using AHI are performed per each row maintenance operation.","title":"AHI ROI"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-ahi-usage","text":"The InnoDB AHI Usage graph shows the search operations on the InnoDB adaptive hash index and its efficiency. The adaptive hash index is a search hash designed to speed access to InnoDB pages in memory. If the Hit Ratio is small, the working data set is larger than the buffer pool, the AHI should likely be disabled.","title":"InnoDB AHI Usage"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-ahi-miss-ratio","text":"","title":"InnoDB AHI Miss Ratio"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-ahi-churn-rows","text":"","title":"InnoDB AHI Churn - Rows"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-ahi-churn-pages","text":"","title":"InnoDB AHI Churn - Pages"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-change-buffer","text":"","title":"InnoDB Change Buffer"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#change-buffer-max-size","text":"The Maximum Size of Change Buffer (as Percent of Buffer Pool Size).","title":"Change Buffer Max Size"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#change-buffer-max-size_1","text":"The Maximum Size of Change Buffer (Bytes).","title":"Change Buffer Max Size"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-change-buffer-merge-load","text":"Number of Average of Active Merge Buffer Operations in Process.","title":"InnoDB Change Buffer Merge Load"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-contention","text":"","title":"InnoDB Contention"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-thread-concurrency","text":"If Enabled limits number of Threads allowed inside InnoDB Kernel at the same time.","title":"InnoDB Thread Concurrency"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-commit-concurrency","text":"If Enabled limits number of Threads allowed inside InnoDB Kernel at the same time during Commit Stage.","title":"InnoDB Commit Concurrency"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-thread-sleep-delay","text":"The Time the thread will Sleep before Re-Entering InnoDB Kernel if high contention.","title":"InnoDB Thread Sleep Delay"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-adaptive-max-sleep-delay","text":"If Set to Non-Zero Value InnoDB Thread Sleep Delay will be adjusted automatically depending on the load up to the value specified by this variable.","title":"InnoDB Adaptive Max Sleep Delay"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-concurrency-tickets","text":"Number of low level operations InnoDB can do after it entered InnoDB kernel before it is forced to exit and yield to another thread waiting.","title":"InnoDB Concurrency Tickets"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-spin-wait-delay","text":"","title":"InnoDB Spin Wait Delay"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-spin-wait-pause-multiplier","text":"","title":"InnoDB Spin Wait Pause Multiplier"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-sync-spin-loops","text":"","title":"InnoDB Sync Spin Loops"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-contention-os-waits","text":"The InnoDB Contention - OS Waits graph shows the number of time an OS wait operation was required while waiting to get the lock. This happens once the spin rounds are exhausted.","title":"InnoDB Contention - OS Waits"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-contention-spin-rounds","text":"The InnoDB Contention - Spin Rounds graph shows the number of spin rounds executed to get a lock. A spin round is a fast retry to get the lock in a loop.","title":"InnoDB Contention - Spin Rounds"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-misc","text":"","title":"InnoDB Misc"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-main-thread-utilization","text":"The InnoDB Main Thread Utilization graph shows the portion of time the InnoDB main thread spent at various task.","title":"InnoDB Main Thread Utilization"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-activity_1","text":"The InnoDB Activity graph shows a measure of the activity of the InnoDB threads.","title":"InnoDB Activity"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-dedicated-server","text":"InnoDB automatically optimized for Dedicated Server Environment (auto scaling cache and some other variables).","title":"InnoDB Dedicated Server"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-sort-buffer-size","text":"This Buffer is used for Building InnoDB Indexes using Sort algorithm.","title":"InnoDB Sort Buffer Size"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-stats-auto-recalc","text":"","title":"InnoDB Stats Auto Recalc"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#update-stats-when-metadata-queried","text":"Refresh InnoDB Statistics when meta-data queries by SHOW TABLE STATUS or INFORMATION_SCHEMA queries. If Enabled can cause severe performance issues.","title":"Update Stats when Metadata Queried"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#index-condition-pushdown-icp","text":"Index Condition Pushdown (ICP) is an optimization for the case where MySQL retrieves rows from a table using an index. Without ICP, the storage engine traverses the index to locate rows in the base table and returns them to the MySQL server which evaluates the WHERE condition for the rows. With ICP enabled, and if parts of the WHERE condition can be evaluated by using only columns from the index, the MySQL server pushes this part of the WHERE condition down to the storage engine. The storage engine then evaluates the pushed index condition by using the index entry and only if this is satisfied is the row read from the table. ICP can reduce the number of times the storage engine must access the base table and the number of times the MySQL server must access the storage engine.","title":"Index Condition Pushdown (ICP)"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-persistent-statistics","text":"","title":"InnoDB Persistent Statistics"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-persistent-sample-pages","text":"Number of Pages To Sample if Persistent Statistics are Enabled.","title":"InnoDB Persistent Sample Pages"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-transient-sample-pages","text":"Number of Pages To Sample if Persistent Statistics are Disabled.","title":"InnoDB Transient Sample Pages"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-online-operations-mariadb","text":"","title":"InnoDB Online Operations (MariaDB)"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-defragmentation","text":"The InnoDB Defragmentation graph shows the status information related to the InnoDB online defragmentation feature of MariaDB for the optimize table command. To enable this feature, the variable innodb-defragment must be set to 1 in the configuration file. Currently available only on a MariaDB server.","title":"InnoDB Defragmentation"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-online-ddl","text":"The InnoDB Online DDL graph shows the state of the online DDL (alter table) operations in InnoDB. The progress metric is estimate of the percentage of the rows processed by the online DDL. Currently available only on a MariaDB server.","title":"InnoDB Online DDL"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#mysql-summary","text":"","title":"MySQL Summary"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#mysql-uptime","text":"MySQL Uptime The amount of time since the last restart of the MySQL server process.","title":"MySQL Uptime"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#current-qps","text":"Current QPS Based on the queries reported by MySQL\u2019s SHOW STATUS command, it is the number of statements executed by the server within the last second. This variable includes statements executed within stored programs, unlike the Questions variable. It does not count COM_PING or COM_STATISTICS commands.","title":"Current QPS"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#file-handlers-used","text":"","title":"File Handlers Used"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#table-open-cache-miss-ratio","text":"","title":"Table Open Cache Miss Ratio"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#table-open-cache-size","text":"","title":"Table Open Cache Size"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#table-definition-cache-size","text":"","title":"Table Definition Cache Size"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#mysql-connections","text":"Max Connections Max Connections is the maximum permitted number of simultaneous client connections. By default, this is 151. Increasing this value increases the number of file descriptors that mysqld requires. If the required number of descriptors are not available, the server reduces the value of Max Connections. mysqld actually permits Max Connections + 1 clients to connect. The extra connection is reserved for use by accounts that have the SUPER privilege, such as root. Max Used Connections is the maximum number of connections that have been in use simultaneously since the server started. Connections is the number of connection attempts (successful or not) to the MySQL server.","title":"MySQL Connections"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#mysql-client-thread-activity","text":"MySQL Active Threads Threads Connected is the number of open connections, while Threads Running is the number of threads not sleeping.","title":"MySQL Client Thread Activity"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#mysql-handlers","text":"MySQL Handlers Handler statistics are internal statistics on how MySQL is selecting, updating, inserting, and modifying rows, tables, and indexes. This is in fact the layer between the Storage Engine and MySQL. read_rnd_next is incremented when the server performs a full table scan and this is a counter you don\u2019t really want to see with a high value. read_key is incremented when a read is done with an index. read_next is incremented when the storage engine is asked to \u2018read the next index entry\u2019. A high value means a lot of index scans are being done.","title":"MySQL Handlers"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#top-command-counters","text":"Top Command Counters The Com_{{ xxx }} statement counter variables indicate the number of times each xxx statement has been executed. There is one status variable for each type of statement. For example, Com_delete and Com_update count DELETE and UPDATE statements, respectively. Com_delete_multi and Com_update_multi are similar but apply to DELETE and UPDATE statements that use multiple-table syntax.","title":"Top Command Counters"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#mysql-network-traffic","text":"MySQL Network Traffic Here we can see how much network traffic is generated by MySQL. Outbound is network traffic sent from MySQL and Inbound is network traffic MySQL has received.","title":"MySQL Network Traffic"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#node-summary","text":"","title":"Node Summary"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#system-uptime","text":"The parameter shows how long a system has been up and running without a shut down or restart.","title":"System Uptime"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#load-average","text":"The system load is a measurement of the computational work the system is performing. Each running process either using or waiting for CPU resources adds 1 to the load.","title":"Load Average"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#ram","text":"RAM (Random Access Memory) is the hardware in a computing device where the operating system, application programs and data in current use are kept so they can be quickly reached by the device\u2019s processor.","title":"RAM"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#memory-available","text":"Percent of Memory Available On Modern Linux Kernels amount of Memory Available for application is not the same as Free+Cached+Buffers.","title":"Memory Available"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#virtual-memory","text":"RAM + SWAP","title":"Virtual Memory"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#disk-space","text":"Sum of disk space on all partitions. It can be significantly over-reported in some installations.","title":"Disk Space"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#min-space-available","text":"Lowest percent of the disk space available.","title":"Min Space Available"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#cpu-usage","text":"The CPU time is measured in clock ticks or seconds. It is useful to measure CPU time as a percentage of the CPU\u2019s capacity, which is called the CPU usage.","title":"CPU Usage"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#cpu-saturation-and-max-core-usage","text":"When a system is running with maximum CPU utilization, the transmitting and receiving threads must all share the available CPU. This will cause data to be queued more frequently to cope with the lack of CPU. CPU Saturation may be measured as the length of a wait queue, or the time spent waiting on the queue.","title":"CPU Saturation and Max Core Usage"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#disk-io-and-swap-activity","text":"Disk I/O includes read or write or input/output operations involving a physical disk. It is the speed with which the data transfer takes place between the hard disk drive and RAM. Swap Activity is memory management that involves swapping sections of memory to and from physical storage.","title":"Disk I/O and Swap Activity"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#network-traffic","text":"Network traffic refers to the amount of data moving across a network at a given point in time.","title":"Network Traffic"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html","text":"MySQL Instance Summary \u00b6 MySQL Connections \u00b6 Max Connections \u00b6 Max Connections is the maximum permitted number of simultaneous client connections. By default, this is 151. Increasing this value increases the number of file descriptors that mysqld requires. If the required number of descriptors are not available, the server reduces the value of Max Connections. mysqld actually permits Max Connections + 1 clients to connect. The extra connection is reserved for use by accounts that have the SUPER privilege, such as root. Max Used Connections is the maximum number of connections that have been in use simultaneously since the server started. Connections is the number of connection attempts (successful or not) to the MySQL server. MySQL Aborted Connections \u00b6 Aborted Connections \u00b6 When a given host connects to MySQL and the connection is interrupted in the middle (for example due to bad credentials), MySQL keeps that info in a system table (since 5.6 this table is exposed in performance_schema ). If the amount of failed requests without a successful connection reaches the value of max_connect_errors , mysqld assumes that something is wrong and blocks the host from further connection. To allow connections from that host again, you need to issue the FLUSH HOSTS statement. MySQL Client Thread Activity \u00b6 MySQL Active Threads \u00b6 Threads Connected is the number of open connections, while Threads Running is the number of threads not sleeping. MySQL Thread Cache \u00b6 MySQL Thread Cache \u00b6 The thread_cache_size variable sets how many threads the server should cache to reuse. When a client disconnects, the client\u2019s threads are put in the cache if the cache is not full. It is auto-sized in MySQL 5.6.8 and above (capped to 100). Requests for threads are satisfied by reusing threads taken from the cache if possible, and only when the cache is empty is a new thread created. threads_created : The number of threads created to handle connections. threads_cached : The number of threads in the thread cache. MySQL Slow Queries \u00b6 MySQL Slow Queries \u00b6 Slow queries are defined as queries being slower than the long_query_time setting. For example, if you have long_query_time set to 3, all queries that take longer than 3 seconds to complete will show on this graph. MySQL Select Types \u00b6 MySQL Select Types \u00b6 As with most relational databases, selecting based on indexes is more efficient than scanning an entire table\u2019s data. Here we see the counters for selects not done with indexes. Select Scan is how many queries caused full table scans, in which all the data in the table had to be read and either discarded or returned. Select Range is how many queries used a range scan, which means MySQL scanned all rows in a given range. Select Full Join is the number of joins that are not joined on an index, this is usually a huge performance hit. MySQL Sorts \u00b6 MySQL Sorts \u00b6 Due to a query\u2019s structure, order, or other requirements, MySQL sorts the rows before returning them. For example, if a table is ordered 1 to 10 but you want the results reversed, MySQL then has to sort the rows to return 10 to 1. This graph also shows when sorts had to scan a whole table or a given range of a table to return the results and which could not have been sorted via an index. MySQL Table Locks \u00b6 Table Locks \u00b6 MySQL takes a number of different locks for varying reasons. In this graph we see how many Table level locks MySQL has requested from the storage engine. In the case of InnoDB, many times the locks could actually be row locks as it only takes table level locks in a few specific cases. It is most useful to compare Locks Immediate and Locks Waited. If Locks waited is rising, it means you have lock contention. Otherwise, Locks Immediate rising and falling is normal activity. MySQL Questions \u00b6 MySQL Questions \u00b6 The number of statements executed by the server. This includes only statements sent to the server by clients and not statements executed within stored programs, unlike the Queries used in the QPS calculation. This variable does not count the following commands: COM_PING COM_STATISTICS COM_STMT_PREPARE COM_STMT_CLOSE COM_STMT_RESET MySQL Network Traffic \u00b6 MySQL Network Traffic \u00b6 Here we can see how much network traffic is generated by MySQL. Outbound is network traffic sent from MySQL and Inbound is network traffic MySQL has received. MySQL Network Usage Hourly \u00b6 MySQL Network Usage Hourly \u00b6 Here we can see how much network traffic is generated by MySQL per hour. You can use the bar graph to compare data sent by MySQL and data received by MySQL. MySQL Internal Memory Overview \u00b6 System Memory : Total Memory for the system. InnoDB Buffer Pool Data : InnoDB maintains a storage area called the buffer pool for caching data and indexes in memory. TokuDB Cache Size : Similar in function to the InnoDB Buffer Pool, TokuDB will allocate 50% of the installed RAM for its own cache. Key Buffer Size : Index blocks for MyISAM tables are buffered and are shared by all threads. key_buffer_size is the size of the buffer used for index blocks. Adaptive Hash Index Size : When InnoDB notices that some index values are being accessed very frequently, it builds a hash index for them in memory on top of B-Tree indexes. Query Cache Size : The query cache stores the text of a SELECT statement together with the corresponding result that was sent to the client. The query cache has huge scalability problems in that only one thread can do an operation in the query cache at the same time. InnoDB Dictionary Size : The data dictionary is InnoDB\u2019s internal catalog of tables. InnoDB stores the data dictionary on disk, and loads entries into memory while the server is running. InnoDB Log Buffer Size : The MySQL InnoDB log buffer allows transactions to run without having to write the log to disk before the transactions commit. Top Command Counters \u00b6 Top Command Counters \u00b6 The Com_xxx statement counter variables indicate the number of times each xxx statement has been executed. There is one status variable for each type of statement. For example, Com_delete and Com_update count DELETE and UPDATE statements, respectively. Com_delete_multi and Com_update_multi are similar but apply to DELETE and UPDATE statements that use multiple-table syntax. Top Command Counters Hourly \u00b6 Top Command Counters Hourly \u00b6 The Com_xxx statement counter variables indicate the number of times each xxx statement has been executed. There is one status variable for each type of statement. For example, Com_delete and Com_update count DELETE and UPDATE statements, respectively. Com_delete_multi and Com_update_multi are similar but apply to DELETE and UPDATE statements that use multiple-table syntax. MySQL Handlers \u00b6 MySQL Handlers \u00b6 Handler statistics are internal statistics on how MySQL is selecting, updating, inserting, and modifying rows, tables, and indexes. This is in fact the layer between the Storage Engine and MySQL. read_rnd_next is incremented when the server performs a full table scan and this is a counter you don\u2019t really want to see with a high value. read_key is incremented when a read is done with an index. read_next is incremented when the storage engine is asked to \u2018read the next index entry\u2019. A high value means a lot of index scans are being done. MySQL Query Cache Memory \u00b6 MySQL Query Cache Memory \u00b6 The query cache has huge scalability problems in that only one thread can do an operation in the query cache at the same time. This serialization is true not only for SELECTs, but also for INSERT/UPDATE/DELETE. This also means that the larger the query_cache_size is set to, the slower those operations become. In concurrent environments, the MySQL Query Cache quickly becomes a contention point, decreasing performance. MariaDB and AWS Aurora have done work to try and eliminate the query cache contention in their flavors of MySQL, while MySQL 8.0 has eliminated the query cache feature. The recommended settings for most environments is to set: query_cache_type=0 query_cache_size=0 Tip While you can dynamically change these values, to completely remove the contention point you have to restart the database. MySQL Query Cache Activity \u00b6 MySQL Query Cache Activity \u00b6 The query cache has huge scalability problems in that only one thread can do an operation in the query cache at the same time. This serialization is true not only for SELECTs, but also for INSERT/UPDATE/DELETE. This also means that the larger the query_cache_size is set to, the slower those operations become. In concurrent environments, the MySQL Query Cache quickly becomes a contention point, decreasing performance. MariaDB and AWS Aurora have done work to try and eliminate the query cache contention in their flavors of MySQL, while MySQL 8.0 has eliminated the query cache feature. The recommended settings for most environments is to set: query_cache_type=0 query_cache_size=0 Tip While you can dynamically change these values, to completely remove the contention point you have to restart the database. MySQL Table Open Cache Status \u00b6 MySQL Table Open Cache Status \u00b6 The recommendation is to set the table_open_cache_instances to a loose correlation to virtual CPUs, keeping in mind that more instances means the cache is split more times. If you have a cache set to 500 but it has 10 instances, each cache will only have 50 cached. The table_definition_cache and table_open_cache can be left as default as they are auto-sized MySQL 5.6 and above (i.e., do not set them to any value). MySQL Open Tables \u00b6 MySQL Open Tables \u00b6 The recommendation is to set the table_open_cache_instances to a loose correlation to virtual CPUs, keeping in mind that more instances means the cache is split more times. If you have a cache set to 500 but it has 10 instances, each cache will only have 50 cached. The table_definition_cache and table_open_cache can be left as default as they are auto-sized MySQL 5.6 and above (i.e., do not set them to any value). MySQL Table Definition Cache \u00b6 MySQL Table Definition Cache \u00b6 The recommendation is to set the table_open_cache_instances to a loose correlation to virtual CPUs, keeping in mind that more instances means the cache is split more times. If you have a cache set to 500 but it has 10 instances, each cache will only have 50 cached. The table_definition_cache and table_open_cache can be left as default as they are auto-sized MySQL 5.6 and above (i.e., do not set them to any value).","title":"MySQL Instance Summary"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#mysql-instance-summary","text":"","title":"MySQL Instance Summary"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#mysql-connections","text":"","title":"MySQL Connections"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#max-connections","text":"Max Connections is the maximum permitted number of simultaneous client connections. By default, this is 151. Increasing this value increases the number of file descriptors that mysqld requires. If the required number of descriptors are not available, the server reduces the value of Max Connections. mysqld actually permits Max Connections + 1 clients to connect. The extra connection is reserved for use by accounts that have the SUPER privilege, such as root. Max Used Connections is the maximum number of connections that have been in use simultaneously since the server started. Connections is the number of connection attempts (successful or not) to the MySQL server.","title":"Max Connections"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#mysql-aborted-connections","text":"","title":"MySQL Aborted Connections"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#aborted-connections","text":"When a given host connects to MySQL and the connection is interrupted in the middle (for example due to bad credentials), MySQL keeps that info in a system table (since 5.6 this table is exposed in performance_schema ). If the amount of failed requests without a successful connection reaches the value of max_connect_errors , mysqld assumes that something is wrong and blocks the host from further connection. To allow connections from that host again, you need to issue the FLUSH HOSTS statement.","title":"Aborted Connections"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#mysql-client-thread-activity","text":"","title":"MySQL Client Thread Activity"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#mysql-active-threads","text":"Threads Connected is the number of open connections, while Threads Running is the number of threads not sleeping.","title":"MySQL Active Threads"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#mysql-thread-cache","text":"","title":"MySQL Thread Cache"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#mysql-thread-cache_1","text":"The thread_cache_size variable sets how many threads the server should cache to reuse. When a client disconnects, the client\u2019s threads are put in the cache if the cache is not full. It is auto-sized in MySQL 5.6.8 and above (capped to 100). Requests for threads are satisfied by reusing threads taken from the cache if possible, and only when the cache is empty is a new thread created. threads_created : The number of threads created to handle connections. threads_cached : The number of threads in the thread cache.","title":"MySQL Thread Cache"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#mysql-slow-queries","text":"","title":"MySQL Slow Queries"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#mysql-slow-queries_1","text":"Slow queries are defined as queries being slower than the long_query_time setting. For example, if you have long_query_time set to 3, all queries that take longer than 3 seconds to complete will show on this graph.","title":"MySQL Slow Queries"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#mysql-select-types","text":"","title":"MySQL Select Types"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#mysql-select-types_1","text":"As with most relational databases, selecting based on indexes is more efficient than scanning an entire table\u2019s data. Here we see the counters for selects not done with indexes. Select Scan is how many queries caused full table scans, in which all the data in the table had to be read and either discarded or returned. Select Range is how many queries used a range scan, which means MySQL scanned all rows in a given range. Select Full Join is the number of joins that are not joined on an index, this is usually a huge performance hit.","title":"MySQL Select Types"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#mysql-sorts","text":"","title":"MySQL Sorts"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#mysql-sorts_1","text":"Due to a query\u2019s structure, order, or other requirements, MySQL sorts the rows before returning them. For example, if a table is ordered 1 to 10 but you want the results reversed, MySQL then has to sort the rows to return 10 to 1. This graph also shows when sorts had to scan a whole table or a given range of a table to return the results and which could not have been sorted via an index.","title":"MySQL Sorts"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#mysql-table-locks","text":"","title":"MySQL Table Locks"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#table-locks","text":"MySQL takes a number of different locks for varying reasons. In this graph we see how many Table level locks MySQL has requested from the storage engine. In the case of InnoDB, many times the locks could actually be row locks as it only takes table level locks in a few specific cases. It is most useful to compare Locks Immediate and Locks Waited. If Locks waited is rising, it means you have lock contention. Otherwise, Locks Immediate rising and falling is normal activity.","title":"Table Locks"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#mysql-questions","text":"","title":"MySQL Questions"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#mysql-questions_1","text":"The number of statements executed by the server. This includes only statements sent to the server by clients and not statements executed within stored programs, unlike the Queries used in the QPS calculation. This variable does not count the following commands: COM_PING COM_STATISTICS COM_STMT_PREPARE COM_STMT_CLOSE COM_STMT_RESET","title":"MySQL Questions"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#mysql-network-traffic","text":"","title":"MySQL Network Traffic"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#mysql-network-traffic_1","text":"Here we can see how much network traffic is generated by MySQL. Outbound is network traffic sent from MySQL and Inbound is network traffic MySQL has received.","title":"MySQL Network Traffic"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#mysql-network-usage-hourly","text":"","title":"MySQL Network Usage Hourly"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#mysql-network-usage-hourly_1","text":"Here we can see how much network traffic is generated by MySQL per hour. You can use the bar graph to compare data sent by MySQL and data received by MySQL.","title":"MySQL Network Usage Hourly"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#mysql-internal-memory-overview","text":"System Memory : Total Memory for the system. InnoDB Buffer Pool Data : InnoDB maintains a storage area called the buffer pool for caching data and indexes in memory. TokuDB Cache Size : Similar in function to the InnoDB Buffer Pool, TokuDB will allocate 50% of the installed RAM for its own cache. Key Buffer Size : Index blocks for MyISAM tables are buffered and are shared by all threads. key_buffer_size is the size of the buffer used for index blocks. Adaptive Hash Index Size : When InnoDB notices that some index values are being accessed very frequently, it builds a hash index for them in memory on top of B-Tree indexes. Query Cache Size : The query cache stores the text of a SELECT statement together with the corresponding result that was sent to the client. The query cache has huge scalability problems in that only one thread can do an operation in the query cache at the same time. InnoDB Dictionary Size : The data dictionary is InnoDB\u2019s internal catalog of tables. InnoDB stores the data dictionary on disk, and loads entries into memory while the server is running. InnoDB Log Buffer Size : The MySQL InnoDB log buffer allows transactions to run without having to write the log to disk before the transactions commit.","title":"MySQL Internal Memory Overview"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#top-command-counters","text":"","title":"Top Command Counters"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#top-command-counters_1","text":"The Com_xxx statement counter variables indicate the number of times each xxx statement has been executed. There is one status variable for each type of statement. For example, Com_delete and Com_update count DELETE and UPDATE statements, respectively. Com_delete_multi and Com_update_multi are similar but apply to DELETE and UPDATE statements that use multiple-table syntax.","title":"Top Command Counters"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#top-command-counters-hourly","text":"","title":"Top Command Counters Hourly"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#top-command-counters-hourly_1","text":"The Com_xxx statement counter variables indicate the number of times each xxx statement has been executed. There is one status variable for each type of statement. For example, Com_delete and Com_update count DELETE and UPDATE statements, respectively. Com_delete_multi and Com_update_multi are similar but apply to DELETE and UPDATE statements that use multiple-table syntax.","title":"Top Command Counters Hourly"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#mysql-handlers","text":"","title":"MySQL Handlers"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#mysql-handlers_1","text":"Handler statistics are internal statistics on how MySQL is selecting, updating, inserting, and modifying rows, tables, and indexes. This is in fact the layer between the Storage Engine and MySQL. read_rnd_next is incremented when the server performs a full table scan and this is a counter you don\u2019t really want to see with a high value. read_key is incremented when a read is done with an index. read_next is incremented when the storage engine is asked to \u2018read the next index entry\u2019. A high value means a lot of index scans are being done.","title":"MySQL Handlers"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#mysql-query-cache-memory","text":"","title":"MySQL Query Cache Memory"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#mysql-query-cache-memory_1","text":"The query cache has huge scalability problems in that only one thread can do an operation in the query cache at the same time. This serialization is true not only for SELECTs, but also for INSERT/UPDATE/DELETE. This also means that the larger the query_cache_size is set to, the slower those operations become. In concurrent environments, the MySQL Query Cache quickly becomes a contention point, decreasing performance. MariaDB and AWS Aurora have done work to try and eliminate the query cache contention in their flavors of MySQL, while MySQL 8.0 has eliminated the query cache feature. The recommended settings for most environments is to set: query_cache_type=0 query_cache_size=0 Tip While you can dynamically change these values, to completely remove the contention point you have to restart the database.","title":"MySQL Query Cache Memory"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#mysql-query-cache-activity","text":"","title":"MySQL Query Cache Activity"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#mysql-query-cache-activity_1","text":"The query cache has huge scalability problems in that only one thread can do an operation in the query cache at the same time. This serialization is true not only for SELECTs, but also for INSERT/UPDATE/DELETE. This also means that the larger the query_cache_size is set to, the slower those operations become. In concurrent environments, the MySQL Query Cache quickly becomes a contention point, decreasing performance. MariaDB and AWS Aurora have done work to try and eliminate the query cache contention in their flavors of MySQL, while MySQL 8.0 has eliminated the query cache feature. The recommended settings for most environments is to set: query_cache_type=0 query_cache_size=0 Tip While you can dynamically change these values, to completely remove the contention point you have to restart the database.","title":"MySQL Query Cache Activity"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#mysql-table-open-cache-status","text":"","title":"MySQL Table Open Cache Status"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#mysql-table-open-cache-status_1","text":"The recommendation is to set the table_open_cache_instances to a loose correlation to virtual CPUs, keeping in mind that more instances means the cache is split more times. If you have a cache set to 500 but it has 10 instances, each cache will only have 50 cached. The table_definition_cache and table_open_cache can be left as default as they are auto-sized MySQL 5.6 and above (i.e., do not set them to any value).","title":"MySQL Table Open Cache Status"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#mysql-open-tables","text":"","title":"MySQL Open Tables"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#mysql-open-tables_1","text":"The recommendation is to set the table_open_cache_instances to a loose correlation to virtual CPUs, keeping in mind that more instances means the cache is split more times. If you have a cache set to 500 but it has 10 instances, each cache will only have 50 cached. The table_definition_cache and table_open_cache can be left as default as they are auto-sized MySQL 5.6 and above (i.e., do not set them to any value).","title":"MySQL Open Tables"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#mysql-table-definition-cache","text":"","title":"MySQL Table Definition Cache"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#mysql-table-definition-cache_1","text":"The recommendation is to set the table_open_cache_instances to a loose correlation to virtual CPUs, keeping in mind that more instances means the cache is split more times. If you have a cache set to 500 but it has 10 instances, each cache will only have 50 cached. The table_definition_cache and table_open_cache can be left as default as they are auto-sized MySQL 5.6 and above (i.e., do not set them to any value).","title":"MySQL Table Definition Cache"},{"location":"details/dashboards/dashboard-mysql-instances-compare.html","text":"MySQL Instances Compare \u00b6 No description","title":"MySQL Instances Compare"},{"location":"details/dashboards/dashboard-mysql-instances-compare.html#mysql-instances-compare","text":"No description","title":"MySQL Instances Compare"},{"location":"details/dashboards/dashboard-mysql-instances-overview.html","text":"MySQL Instances Overview \u00b6 No description","title":"MySQL Instances Overview"},{"location":"details/dashboards/dashboard-mysql-instances-overview.html#mysql-instances-overview","text":"No description","title":"MySQL Instances Overview"},{"location":"details/dashboards/dashboard-mysql-myisam-aria-details.html","text":"MySQL MyISAM/Aria Details \u00b6 MyISAM Key Buffer Performance \u00b6 The Key Read Ratio ( Key_reads / Key_read_requests ) ratio should normally be less than 0.01. The Key Write Ratio ( Key_writes / Key_write_requests ) ratio is usually near 1 if you are using mostly updates and deletes, but might be much smaller if you tend to do updates that affect many rows at the same time or if you are using the DELAY_KEY_WRITE table option. Aria Pagecache Reads/Writes \u00b6 This graph is similar to InnoDB buffer pool reads/writes. aria-pagecache-buffer-size is the main cache for the Aria storage engine. If you see high reads/writes (physical IO), i.e. reads are close to read requests and/or writes are close to write requests you may need to increase the aria-pagecache-buffer-size (may need to decrease other buffers: key_buffer_size , innodb_buffer_pool_size , etc.) Aria Transaction Log Syncs \u00b6 This is similar to InnoDB log file syncs. If you see lots of log syncs and want to relax the durability settings you can change aria_checkpoint_interval (in seconds) from 30 (default) to a higher number. It is good to look at the disk IO dashboard as well. Aria Pagecache Blocks \u00b6 This graph shows the utilization for the Aria pagecache. This is similar to InnoDB buffer pool graph. If you see all blocks are used you may consider increasing aria-pagecache-buffer-size (may need to decrease other buffers: key_buffer_size , innodb_buffer_pool_size , etc.)","title":"MySQL MyISAM/Aria Details"},{"location":"details/dashboards/dashboard-mysql-myisam-aria-details.html#mysql-myisamaria-details","text":"","title":"MySQL MyISAM/Aria Details"},{"location":"details/dashboards/dashboard-mysql-myisam-aria-details.html#myisam-key-buffer-performance","text":"The Key Read Ratio ( Key_reads / Key_read_requests ) ratio should normally be less than 0.01. The Key Write Ratio ( Key_writes / Key_write_requests ) ratio is usually near 1 if you are using mostly updates and deletes, but might be much smaller if you tend to do updates that affect many rows at the same time or if you are using the DELAY_KEY_WRITE table option.","title":"MyISAM Key Buffer Performance"},{"location":"details/dashboards/dashboard-mysql-myisam-aria-details.html#aria-pagecache-readswrites","text":"This graph is similar to InnoDB buffer pool reads/writes. aria-pagecache-buffer-size is the main cache for the Aria storage engine. If you see high reads/writes (physical IO), i.e. reads are close to read requests and/or writes are close to write requests you may need to increase the aria-pagecache-buffer-size (may need to decrease other buffers: key_buffer_size , innodb_buffer_pool_size , etc.)","title":"Aria Pagecache Reads/Writes"},{"location":"details/dashboards/dashboard-mysql-myisam-aria-details.html#aria-transaction-log-syncs","text":"This is similar to InnoDB log file syncs. If you see lots of log syncs and want to relax the durability settings you can change aria_checkpoint_interval (in seconds) from 30 (default) to a higher number. It is good to look at the disk IO dashboard as well.","title":"Aria Transaction Log Syncs"},{"location":"details/dashboards/dashboard-mysql-myisam-aria-details.html#aria-pagecache-blocks","text":"This graph shows the utilization for the Aria pagecache. This is similar to InnoDB buffer pool graph. If you see all blocks are used you may consider increasing aria-pagecache-buffer-size (may need to decrease other buffers: key_buffer_size , innodb_buffer_pool_size , etc.)","title":"Aria Pagecache Blocks"},{"location":"details/dashboards/dashboard-mysql-myrocks-details.html","text":"MySQL MyRocks Details \u00b6 The MyRocks storage engine developed by Facebook based on the RocksDB storage engine is applicable to systems which primarily interact with the database by writing data to it rather than reading from it. RocksDB also features a good level of compression, higher than that of the InnoDB storage engine, which makes it especially valuable when optimizing the usage of hard drives. PMM collects statistics on the MyRocks storage engine for MySQL in the Metrics Monitor information for this dashboard comes from the Information Schema tables. Metrics \u00b6 MyRocks cache MyRocks cache data bytes R/W MyRocks cache index hit rate MyRocks cache index MyRocks cache filter hit rate MyRocks cache filter MyRocks cache data bytes inserted MyRocks bloom filter MyRocks memtable MyRocks memtable size MyRocks number of keys MyRocks cache L0/L1 MyRocks number of DB ops MyRocks R/W MyRocks bytes read by iterations MyRocks write ops MyRocks WAL MyRocks number reseeks in iterations RocksDB row operations MyRocks file operations RocksDB stalls RocksDB stops/slowdowns","title":"MySQL MyRocks Details"},{"location":"details/dashboards/dashboard-mysql-myrocks-details.html#mysql-myrocks-details","text":"The MyRocks storage engine developed by Facebook based on the RocksDB storage engine is applicable to systems which primarily interact with the database by writing data to it rather than reading from it. RocksDB also features a good level of compression, higher than that of the InnoDB storage engine, which makes it especially valuable when optimizing the usage of hard drives. PMM collects statistics on the MyRocks storage engine for MySQL in the Metrics Monitor information for this dashboard comes from the Information Schema tables.","title":"MySQL MyRocks Details"},{"location":"details/dashboards/dashboard-mysql-myrocks-details.html#metrics","text":"MyRocks cache MyRocks cache data bytes R/W MyRocks cache index hit rate MyRocks cache index MyRocks cache filter hit rate MyRocks cache filter MyRocks cache data bytes inserted MyRocks bloom filter MyRocks memtable MyRocks memtable size MyRocks number of keys MyRocks cache L0/L1 MyRocks number of DB ops MyRocks R/W MyRocks bytes read by iterations MyRocks write ops MyRocks WAL MyRocks number reseeks in iterations RocksDB row operations MyRocks file operations RocksDB stalls RocksDB stops/slowdowns","title":"Metrics"},{"location":"details/dashboards/dashboard-mysql-performance-schema-details.html","text":"MySQL Performance Schema Details \u00b6 The MySQL Performance Schema dashboard helps determine the efficiency of communicating with Performance Schema. This dashboard contains the following metrics: Performance Schema file IO (events) Performance Schema file IO (load) Performance Schema file IO (Bytes) Performance Schema waits (events) Performance Schema waits (load) Index access operations (load) Table access operations (load) Performance Schema SQL and external locks (events) Performance Schema SQL and external locks (seconds)","title":"MySQL Performance Schema Details"},{"location":"details/dashboards/dashboard-mysql-performance-schema-details.html#mysql-performance-schema-details","text":"The MySQL Performance Schema dashboard helps determine the efficiency of communicating with Performance Schema. This dashboard contains the following metrics: Performance Schema file IO (events) Performance Schema file IO (load) Performance Schema file IO (Bytes) Performance Schema waits (events) Performance Schema waits (load) Index access operations (load) Table access operations (load) Performance Schema SQL and external locks (events) Performance Schema SQL and external locks (seconds)","title":"MySQL Performance Schema Details"},{"location":"details/dashboards/dashboard-mysql-query-response-time-details.html","text":"MySQL Query Response Time Details \u00b6 Average Query Response Time \u00b6 The Average Query Response Time graph shows information collected using the Response Time Distribution plugin sourced from table INFORMATION_SCHEMA.QUERY_RESPONSE_TIME . It computes this value across all queries by taking the sum of seconds divided by the count of queries. Query Response Time Distribution \u00b6 Query response time counts (operations) are grouped into three buckets: 100 ms - 1 s 1 s - 10 s > 10 s Average Query Response Time \u00b6 Available only in Percona Server for MySQL , provides visibility of the split of READ vs WRITE query response time. Read Query Response Time Distribution \u00b6 Available only in Percona Server for MySQL, illustrates READ query response time counts (operations) grouped into three buckets: 100 ms - 1 s 1 s - 10 s > 10 s Write Query Response Time Distribution \u00b6 Available only in Percona Server for MySQL, illustrates WRITE query response time counts (operations) grouped into three buckets: 100 ms - 1 s 1 s - 10 s > 10 s","title":"MySQL Query Response Time Details"},{"location":"details/dashboards/dashboard-mysql-query-response-time-details.html#mysql-query-response-time-details","text":"","title":"MySQL Query Response Time Details"},{"location":"details/dashboards/dashboard-mysql-query-response-time-details.html#average-query-response-time","text":"The Average Query Response Time graph shows information collected using the Response Time Distribution plugin sourced from table INFORMATION_SCHEMA.QUERY_RESPONSE_TIME . It computes this value across all queries by taking the sum of seconds divided by the count of queries.","title":"Average Query Response Time"},{"location":"details/dashboards/dashboard-mysql-query-response-time-details.html#query-response-time-distribution","text":"Query response time counts (operations) are grouped into three buckets: 100 ms - 1 s 1 s - 10 s > 10 s","title":"Query Response Time Distribution"},{"location":"details/dashboards/dashboard-mysql-query-response-time-details.html#average-query-response-time_1","text":"Available only in Percona Server for MySQL , provides visibility of the split of READ vs WRITE query response time.","title":"Average Query Response Time"},{"location":"details/dashboards/dashboard-mysql-query-response-time-details.html#read-query-response-time-distribution","text":"Available only in Percona Server for MySQL, illustrates READ query response time counts (operations) grouped into three buckets: 100 ms - 1 s 1 s - 10 s > 10 s","title":"Read Query Response Time Distribution"},{"location":"details/dashboards/dashboard-mysql-query-response-time-details.html#write-query-response-time-distribution","text":"Available only in Percona Server for MySQL, illustrates WRITE query response time counts (operations) grouped into three buckets: 100 ms - 1 s 1 s - 10 s > 10 s","title":"Write Query Response Time Distribution"},{"location":"details/dashboards/dashboard-mysql-replication-summary.html","text":"MySQL Replication Summary \u00b6 IO Thread Running \u00b6 This metric shows if the IO Thread is running or not. It only applies to a secondary host. SQL Thread is a process that runs on a secondary host in the replication environment. It reads the events from the local relay log file and applies them to the secondary server. Depending on the format of the binary log it can read query statements in plain text and re-execute them or it can read raw data and apply them to the local host. Possible values \u00b6 Yes The thread is running and is connected to a replication primary No The thread is not running because it is not launched yet or because an error has occurred connecting to the primary host Connecting The thread is running but is not connected to a replication primary No value The host is not configured to be a replication secondary IO Thread Running is one of the parameters that the command SHOW SLAVE STATUS returns. SQL Thread Running \u00b6 This metric shows if the SQL thread is running or not. It only applies to a secondary host. Possible values \u00b6 Yes SQL Thread is running and is applying events from the relay log to the local secondary host No SQL Thread is not running because it is not launched yet or because of an error occurred while applying an event to the local secondary host Replication Error No \u00b6 This metric shows the number of the last error in the SQL Thread encountered which caused replication to stop. One of the more common errors is Error: 1022 Duplicate Key Entry . In such a case replication is attempting to update a row that already exists on the secondary. The SQL Thread will stop replication to avoid data corruption. Read only \u00b6 This metric indicates whether the host is configured to be in Read Only mode or not. Possible values \u00b6 Yes The secondary host permits no client updates except from users who have the SUPER privilege or the REPLICATION SLAVE privilege. This kind of configuration is typically used for secondary hosts in a replication environment to avoid a user can inadvertently or voluntarily modify data causing inconsistencies and stopping the replication process. No The secondary host is not configured in Read Only mode. MySQL Replication Delay \u00b6 This metric shows the number of seconds the secondary host is delayed in replication applying events compared to when the primary host applied them, denoted by the Seconds_Behind_Master value, and only applies to a secondary host. Since the replication process applies the data modifications on the secondary asynchronously, it could happen that the secondary replicates events after some time. The main reasons are: Network round trip time - high latency links will lead to non-zero replication lag values. Single threaded nature of replication channels - primary servers have the advantage of applying changes in parallel, whereas secondary ones are only able to apply changes in serial, thus limiting their throughput. In some cases Group Commit can help but is not always applicable. High number of changed rows or computationally expensive SQL - depending on the replication format ( ROW vs STATEMENT ), significant changes to the database through high volume of rows modified, or expensive CPU will all contribute to secondary servers lagging behind the primary. Generally adding more CPU or Disk resources can alleviate replication lag issues, up to a point. Binlog Size \u00b6 This metric shows the overall size of the binary log files, which can exist on both primary and secondary servers. The binary log (also known as the binlog ) contains events that describe database changes: CREATE TABLE , ALTER TABLE , updates, inserts, deletes and other statements or database changes. The binlog file is read by secondaries via their IO Thread process to replicate database changes modification on the data and on the table structures. There can be more than one binlog file depending on the binlog rotation policy (for example using the configuration variables max_binlog_size and expire_logs_days ) or because of server reboots. When planning the disk space, take care of the overall dimension of binlog files and adopt a good rotation policy or think about having a separate mount point or disk to store the binlog data. Binlog Data Written Hourly \u00b6 This metric shows the amount of data written hourly to the binlog files during the last 24 hours. This metric can give you an idea of how big is your application in terms of data writes (creation, modification, deletion). Binlog Count \u00b6 This metric shows the overall count of binary log files, on both primary and secondary servers. Binlogs Created Hourly \u00b6 This metric shows the number of binlog files created hourly during the last 24 hours. Relay Log Space \u00b6 This metric shows the overall size of the relay log files. It only applies to a secondary host. The relay log consists of a set of numbered files containing the events to be executed on the secondary host to replicate database changes. The relay log has the same format as the binlog. There can be multiple relay log files depending on the rotation policy adopted (using the configuration variable max_relay_log_size ). As soon as the SQL thread completes to execute all events in the relay log file, the file is deleted. If this metric contains a high value, the variable max_relay_log_file is high too. Generally, this not a serious issue. If the value of this metric is constantly increased, the secondary is delaying too much in applying the events. Treat this metric in the same way as the MySQL Replication Delay metric. Relay Log Written Hourly \u00b6 This metric shows the amount of data written hourly into relay log files during the last 24 hours.","title":"MySQL Replication Summary"},{"location":"details/dashboards/dashboard-mysql-replication-summary.html#mysql-replication-summary","text":"","title":"MySQL Replication Summary"},{"location":"details/dashboards/dashboard-mysql-replication-summary.html#io-thread-running","text":"This metric shows if the IO Thread is running or not. It only applies to a secondary host. SQL Thread is a process that runs on a secondary host in the replication environment. It reads the events from the local relay log file and applies them to the secondary server. Depending on the format of the binary log it can read query statements in plain text and re-execute them or it can read raw data and apply them to the local host.","title":"IO Thread Running"},{"location":"details/dashboards/dashboard-mysql-replication-summary.html#possible-values","text":"Yes The thread is running and is connected to a replication primary No The thread is not running because it is not launched yet or because an error has occurred connecting to the primary host Connecting The thread is running but is not connected to a replication primary No value The host is not configured to be a replication secondary IO Thread Running is one of the parameters that the command SHOW SLAVE STATUS returns.","title":"Possible values"},{"location":"details/dashboards/dashboard-mysql-replication-summary.html#sql-thread-running","text":"This metric shows if the SQL thread is running or not. It only applies to a secondary host.","title":"SQL Thread Running"},{"location":"details/dashboards/dashboard-mysql-replication-summary.html#possible-values_1","text":"Yes SQL Thread is running and is applying events from the relay log to the local secondary host No SQL Thread is not running because it is not launched yet or because of an error occurred while applying an event to the local secondary host","title":"Possible values"},{"location":"details/dashboards/dashboard-mysql-replication-summary.html#replication-error-no","text":"This metric shows the number of the last error in the SQL Thread encountered which caused replication to stop. One of the more common errors is Error: 1022 Duplicate Key Entry . In such a case replication is attempting to update a row that already exists on the secondary. The SQL Thread will stop replication to avoid data corruption.","title":"Replication Error No"},{"location":"details/dashboards/dashboard-mysql-replication-summary.html#read-only","text":"This metric indicates whether the host is configured to be in Read Only mode or not.","title":"Read only"},{"location":"details/dashboards/dashboard-mysql-replication-summary.html#possible-values_2","text":"Yes The secondary host permits no client updates except from users who have the SUPER privilege or the REPLICATION SLAVE privilege. This kind of configuration is typically used for secondary hosts in a replication environment to avoid a user can inadvertently or voluntarily modify data causing inconsistencies and stopping the replication process. No The secondary host is not configured in Read Only mode.","title":"Possible values"},{"location":"details/dashboards/dashboard-mysql-replication-summary.html#mysql-replication-delay","text":"This metric shows the number of seconds the secondary host is delayed in replication applying events compared to when the primary host applied them, denoted by the Seconds_Behind_Master value, and only applies to a secondary host. Since the replication process applies the data modifications on the secondary asynchronously, it could happen that the secondary replicates events after some time. The main reasons are: Network round trip time - high latency links will lead to non-zero replication lag values. Single threaded nature of replication channels - primary servers have the advantage of applying changes in parallel, whereas secondary ones are only able to apply changes in serial, thus limiting their throughput. In some cases Group Commit can help but is not always applicable. High number of changed rows or computationally expensive SQL - depending on the replication format ( ROW vs STATEMENT ), significant changes to the database through high volume of rows modified, or expensive CPU will all contribute to secondary servers lagging behind the primary. Generally adding more CPU or Disk resources can alleviate replication lag issues, up to a point.","title":"MySQL Replication Delay"},{"location":"details/dashboards/dashboard-mysql-replication-summary.html#binlog-size","text":"This metric shows the overall size of the binary log files, which can exist on both primary and secondary servers. The binary log (also known as the binlog ) contains events that describe database changes: CREATE TABLE , ALTER TABLE , updates, inserts, deletes and other statements or database changes. The binlog file is read by secondaries via their IO Thread process to replicate database changes modification on the data and on the table structures. There can be more than one binlog file depending on the binlog rotation policy (for example using the configuration variables max_binlog_size and expire_logs_days ) or because of server reboots. When planning the disk space, take care of the overall dimension of binlog files and adopt a good rotation policy or think about having a separate mount point or disk to store the binlog data.","title":"Binlog Size"},{"location":"details/dashboards/dashboard-mysql-replication-summary.html#binlog-data-written-hourly","text":"This metric shows the amount of data written hourly to the binlog files during the last 24 hours. This metric can give you an idea of how big is your application in terms of data writes (creation, modification, deletion).","title":"Binlog Data Written Hourly"},{"location":"details/dashboards/dashboard-mysql-replication-summary.html#binlog-count","text":"This metric shows the overall count of binary log files, on both primary and secondary servers.","title":"Binlog Count"},{"location":"details/dashboards/dashboard-mysql-replication-summary.html#binlogs-created-hourly","text":"This metric shows the number of binlog files created hourly during the last 24 hours.","title":"Binlogs Created Hourly"},{"location":"details/dashboards/dashboard-mysql-replication-summary.html#relay-log-space","text":"This metric shows the overall size of the relay log files. It only applies to a secondary host. The relay log consists of a set of numbered files containing the events to be executed on the secondary host to replicate database changes. The relay log has the same format as the binlog. There can be multiple relay log files depending on the rotation policy adopted (using the configuration variable max_relay_log_size ). As soon as the SQL thread completes to execute all events in the relay log file, the file is deleted. If this metric contains a high value, the variable max_relay_log_file is high too. Generally, this not a serious issue. If the value of this metric is constantly increased, the secondary is delaying too much in applying the events. Treat this metric in the same way as the MySQL Replication Delay metric.","title":"Relay Log Space"},{"location":"details/dashboards/dashboard-mysql-replication-summary.html#relay-log-written-hourly","text":"This metric shows the amount of data written hourly into relay log files during the last 24 hours.","title":"Relay Log Written Hourly"},{"location":"details/dashboards/dashboard-mysql-table-details.html","text":"MySQL Table Details \u00b6 Largest Tables \u00b6 Largest Tables by Row Count The estimated number of rows in the table from information_schema.tables . Largest Tables by Size The size of the table components from information_schema.tables . Pie \u00b6 Total Database Size The total size of the database: as data + index size, so freeable one. Most Fragmented Tables by Freeable Size The list of 5 most fragmented tables ordered by their freeable size Table Activity \u00b6 The next two graphs are available only for Percona Server and MariaDB and require userstat variable turned on. Rows read \u00b6 The number of rows read from the table, shown for the top 5 tables. Rows Changed \u00b6 The number of rows changed in the table, shown for the top 5 tables. Auto Increment Usage \u00b6 The current value of an auto_increment column from information_schema , shown for the top 10 tables.","title":"MySQL Table Details"},{"location":"details/dashboards/dashboard-mysql-table-details.html#mysql-table-details","text":"","title":"MySQL Table Details"},{"location":"details/dashboards/dashboard-mysql-table-details.html#largest-tables","text":"Largest Tables by Row Count The estimated number of rows in the table from information_schema.tables . Largest Tables by Size The size of the table components from information_schema.tables .","title":"Largest Tables"},{"location":"details/dashboards/dashboard-mysql-table-details.html#pie","text":"Total Database Size The total size of the database: as data + index size, so freeable one. Most Fragmented Tables by Freeable Size The list of 5 most fragmented tables ordered by their freeable size","title":"Pie"},{"location":"details/dashboards/dashboard-mysql-table-details.html#table-activity","text":"The next two graphs are available only for Percona Server and MariaDB and require userstat variable turned on.","title":"Table Activity"},{"location":"details/dashboards/dashboard-mysql-table-details.html#rows-read","text":"The number of rows read from the table, shown for the top 5 tables.","title":"Rows read"},{"location":"details/dashboards/dashboard-mysql-table-details.html#rows-changed","text":"The number of rows changed in the table, shown for the top 5 tables.","title":"Rows Changed"},{"location":"details/dashboards/dashboard-mysql-table-details.html#auto-increment-usage","text":"The current value of an auto_increment column from information_schema , shown for the top 10 tables.","title":"Auto Increment Usage"},{"location":"details/dashboards/dashboard-mysql-tokudb-details.html","text":"MySQL TokuDB Details \u00b6 No description","title":"MySQL TokuDB Details"},{"location":"details/dashboards/dashboard-mysql-tokudb-details.html#mysql-tokudb-details","text":"No description","title":"MySQL TokuDB Details"},{"location":"details/dashboards/dashboard-mysql-user-details.html","text":"MySQL User Details \u00b6 This dashboard requires Percona Server for MySQL 5.1+ or MariaDB 10.1/10.2 with XtraDB. Also userstat should be enabled, for example with the SET GLOBAL userstat=1 statement. See Setting up MySQL . Data is displayed for the 5 top users. Top Users by Connections Created The number of times user\u2019s connections connected using SSL to the server. Top Users by Traffic The number of bytes sent to the user\u2019s connections. Top Users by Rows Fetched/Read The number of rows fetched by the user\u2019s connections. Top Users by Rows Updated The number of rows updated by the user\u2019s connections. Top Users by Busy Time The cumulative number of seconds there was activity on connections from the user. Top Users by CPU Time The cumulative CPU time elapsed, in seconds, while servicing connections of the user.","title":"MySQL User Details"},{"location":"details/dashboards/dashboard-mysql-user-details.html#mysql-user-details","text":"This dashboard requires Percona Server for MySQL 5.1+ or MariaDB 10.1/10.2 with XtraDB. Also userstat should be enabled, for example with the SET GLOBAL userstat=1 statement. See Setting up MySQL . Data is displayed for the 5 top users. Top Users by Connections Created The number of times user\u2019s connections connected using SSL to the server. Top Users by Traffic The number of bytes sent to the user\u2019s connections. Top Users by Rows Fetched/Read The number of rows fetched by the user\u2019s connections. Top Users by Rows Updated The number of rows updated by the user\u2019s connections. Top Users by Busy Time The cumulative number of seconds there was activity on connections from the user. Top Users by CPU Time The cumulative CPU time elapsed, in seconds, while servicing connections of the user.","title":"MySQL User Details"},{"location":"details/dashboards/dashboard-mysql-wait-event-analyses-details.html","text":"MySQL Wait Event Analyses Details \u00b6 This dashboard helps to analyze Performance Schema wait events. It plots the following metrics for the chosen (one or more) wait events: Count - Performance Schema Waits Load - Performance Schema Waits Avg Wait Time - Performance Schema Waits","title":"MySQL Wait Event Analyses Details"},{"location":"details/dashboards/dashboard-mysql-wait-event-analyses-details.html#mysql-wait-event-analyses-details","text":"This dashboard helps to analyze Performance Schema wait events. It plots the following metrics for the chosen (one or more) wait events: Count - Performance Schema Waits Load - Performance Schema Waits Avg Wait Time - Performance Schema Waits","title":"MySQL Wait Event Analyses Details"},{"location":"details/dashboards/dashboard-network-details.html","text":"Network Details \u00b6 Last Hour Statistic \u00b6 This section reports the inbound speed , outbound speed , traffic errors and drops , and retransmit rate . Network Traffic \u00b6 This section contains the Network traffic and network utilization hourly metrics. Network Traffic Details \u00b6 This section offers the following metrics: Network traffic by packets Network traffic errors Network traffic drop Network traffic multicast Network Netstat TCP \u00b6 This section offers the following metrics: Timeout value used for retransmitting Min TCP retransmission timeout Max TCP retransmission timeout Netstat: TCP TCP segments Network Netstat UDP \u00b6 In this section, you can find the following metrics: Netstat: UDP UDP Lite The graphs in the UDP Lite metric give statistics about: InDatagrams Packets received OutDatagrams Packets sent InCsumErrors Datagrams with checksum errors InErrors Datagrams that could not be delivered to an application RcvbufErrors Datagrams for which not enough socket buffer memory to receive SndbufErrors Datagrams for which not enough socket buffer memory to transmit NoPorts Datagrams received on a port with no listener ICMP \u00b6 This section has the following metrics: ICMP Errors Messages/Redirects Echos Timestamps/Mask Requests ICMP Errors \u00b6 InErrors Messages which the entity received but determined as having ICMP-specific errors (bad ICMP checksums, bad length, etc.) OutErrors Messages which this entity did not send due to problems discovered within ICMP, such as a lack of buffers InDestUnreachs Destination Unreachable messages received OutDestUnreachs Destination Unreachable messages sent InType3 Destination unreachable OutType3 Destination unreachable InCsumErrors Messages with ICMP checksum errors InTimeExcds Time Exceeded messages received Messages/Redirects \u00b6 InMsgs Messages which the entity received. Note that this counter includes all those counted by icmpInErrors InRedirects Redirect messages received OutMsgs Messages which this entity attempted to send. Note that this counter includes all those counted by icmpOutErrors OutRedirects Redirect messages sent. For a host, this object will always be zero, since hosts do not send redirects Echos \u00b6 InEchoReps Echo Reply messages received InEchos Echo (request) messages received OutEchoReps Echo Reply messages sent OutEchos Echo (request) messages sent Timestamps/Mask Requests \u00b6 InAddrMaskReps Address Mask Reply messages received InAddrMasks Address Mask Request messages received OutAddrMaskReps Address Mask Reply messages sent OutAddrMasks Address Mask Request messages sent InTimestampReps Timestamp Reply messages received InTimestamps Timestamp Request messages received OutTimestampReps Timestamp Reply messages sent OutTimestamps Timestamp Request messages sent","title":"Network Details"},{"location":"details/dashboards/dashboard-network-details.html#network-details","text":"","title":"Network Details"},{"location":"details/dashboards/dashboard-network-details.html#last-hour-statistic","text":"This section reports the inbound speed , outbound speed , traffic errors and drops , and retransmit rate .","title":"Last Hour Statistic"},{"location":"details/dashboards/dashboard-network-details.html#network-traffic","text":"This section contains the Network traffic and network utilization hourly metrics.","title":"Network Traffic"},{"location":"details/dashboards/dashboard-network-details.html#network-traffic-details","text":"This section offers the following metrics: Network traffic by packets Network traffic errors Network traffic drop Network traffic multicast","title":"Network Traffic Details"},{"location":"details/dashboards/dashboard-network-details.html#network-netstat-tcp","text":"This section offers the following metrics: Timeout value used for retransmitting Min TCP retransmission timeout Max TCP retransmission timeout Netstat: TCP TCP segments","title":"Network Netstat TCP"},{"location":"details/dashboards/dashboard-network-details.html#network-netstat-udp","text":"In this section, you can find the following metrics: Netstat: UDP UDP Lite The graphs in the UDP Lite metric give statistics about: InDatagrams Packets received OutDatagrams Packets sent InCsumErrors Datagrams with checksum errors InErrors Datagrams that could not be delivered to an application RcvbufErrors Datagrams for which not enough socket buffer memory to receive SndbufErrors Datagrams for which not enough socket buffer memory to transmit NoPorts Datagrams received on a port with no listener","title":"Network Netstat UDP"},{"location":"details/dashboards/dashboard-network-details.html#icmp","text":"This section has the following metrics: ICMP Errors Messages/Redirects Echos Timestamps/Mask Requests","title":"ICMP"},{"location":"details/dashboards/dashboard-network-details.html#icmp-errors","text":"InErrors Messages which the entity received but determined as having ICMP-specific errors (bad ICMP checksums, bad length, etc.) OutErrors Messages which this entity did not send due to problems discovered within ICMP, such as a lack of buffers InDestUnreachs Destination Unreachable messages received OutDestUnreachs Destination Unreachable messages sent InType3 Destination unreachable OutType3 Destination unreachable InCsumErrors Messages with ICMP checksum errors InTimeExcds Time Exceeded messages received","title":"ICMP Errors"},{"location":"details/dashboards/dashboard-network-details.html#messagesredirects","text":"InMsgs Messages which the entity received. Note that this counter includes all those counted by icmpInErrors InRedirects Redirect messages received OutMsgs Messages which this entity attempted to send. Note that this counter includes all those counted by icmpOutErrors OutRedirects Redirect messages sent. For a host, this object will always be zero, since hosts do not send redirects","title":"Messages/Redirects"},{"location":"details/dashboards/dashboard-network-details.html#echos","text":"InEchoReps Echo Reply messages received InEchos Echo (request) messages received OutEchoReps Echo Reply messages sent OutEchos Echo (request) messages sent","title":"Echos"},{"location":"details/dashboards/dashboard-network-details.html#timestampsmask-requests","text":"InAddrMaskReps Address Mask Reply messages received InAddrMasks Address Mask Request messages received OutAddrMaskReps Address Mask Reply messages sent OutAddrMasks Address Mask Request messages sent InTimestampReps Timestamp Reply messages received InTimestamps Timestamp Request messages received OutTimestampReps Timestamp Reply messages sent OutTimestamps Timestamp Request messages sent","title":"Timestamps/Mask Requests"},{"location":"details/dashboards/dashboard-node-summary.html","text":"Node Summary \u00b6 System Summary \u00b6 The output from pt-summary , one of the Percona Toolkit utilities . CPU Usage \u00b6 The CPU time is measured in clock ticks or seconds. It is useful to measure CPU time as a percentage of the CPU\u2019s capacity, which is called the CPU usage. CPU Saturation and Max Core Usage \u00b6 When a system is running with maximum CPU utilization, the transmitting and receiving threads must all share the available CPU. This will cause data to be queued more frequently to cope with the lack of CPU. CPU Saturation may be measured as the length of a wait queue, or the time spent waiting on the queue. Interrupts and Context Switches \u00b6 Interrupt is an input signal to the processor indicating an event that needs immediate attention. An interrupt signal alerts the processor and serves as a request for the processor to interrupt the currently executing code, so that the event can be processed in a timely manner. Context switch is the process of storing the state of a process or thread, so that it can be restored and resume execution at a later point. This allows multiple processes to share a single CPU, and is an essential feature of a multitasking operating system. Processes \u00b6 No description Memory Utilization \u00b6 No description Virtual Memory Utilization \u00b6 No description Swap Space \u00b6 No description Swap Activity \u00b6 Swap Activity is memory management that involves swapping sections of memory to and from physical storage. I/O Activity \u00b6 Disk I/O includes read or write or input/output operations involving a physical disk. It is the speed with which the data transfer takes place between the hard disk drive and RAM. Global File Descriptors Usage \u00b6 No description Disk IO Latency \u00b6 Shows average latency for Reads and Writes IO Devices. Higher than typical latency for highly loaded storage indicates saturation (overload) and is frequent cause of performance problems. Higher than normal latency also can indicate internal storage problems. Disk IO Load \u00b6 Shows how much disk was loaded for reads or writes as average number of outstanding requests at different period of time. High disk load is a good measure of actual storage utilization. Different storage types handle load differently - some will show latency increases on low loads others can handle higher load with no problems. Network Traffic \u00b6 Network traffic refers to the amount of data moving across a network at a given point in time. Network Utilization Hourly \u00b6 No description Local Network Errors \u00b6 Total Number of Local Network Interface Transmit Errors, Receive Errors and Drops. Should be Zero TCP Retransmission \u00b6 Retransmission, essentially identical with Automatic repeat request (ARQ), is the resending of packets which have been either damaged or lost. Retransmission is one of the basic mechanisms used by protocols operating over a packet switched computer network to provide reliable communication (such as that provided by a reliable byte stream, for example TCP).","title":"Node Summary"},{"location":"details/dashboards/dashboard-node-summary.html#node-summary","text":"","title":"Node Summary"},{"location":"details/dashboards/dashboard-node-summary.html#system-summary","text":"The output from pt-summary , one of the Percona Toolkit utilities .","title":"System Summary"},{"location":"details/dashboards/dashboard-node-summary.html#cpu-usage","text":"The CPU time is measured in clock ticks or seconds. It is useful to measure CPU time as a percentage of the CPU\u2019s capacity, which is called the CPU usage.","title":"CPU Usage"},{"location":"details/dashboards/dashboard-node-summary.html#cpu-saturation-and-max-core-usage","text":"When a system is running with maximum CPU utilization, the transmitting and receiving threads must all share the available CPU. This will cause data to be queued more frequently to cope with the lack of CPU. CPU Saturation may be measured as the length of a wait queue, or the time spent waiting on the queue.","title":"CPU Saturation and Max Core Usage"},{"location":"details/dashboards/dashboard-node-summary.html#interrupts-and-context-switches","text":"Interrupt is an input signal to the processor indicating an event that needs immediate attention. An interrupt signal alerts the processor and serves as a request for the processor to interrupt the currently executing code, so that the event can be processed in a timely manner. Context switch is the process of storing the state of a process or thread, so that it can be restored and resume execution at a later point. This allows multiple processes to share a single CPU, and is an essential feature of a multitasking operating system.","title":"Interrupts and Context Switches"},{"location":"details/dashboards/dashboard-node-summary.html#processes","text":"No description","title":"Processes"},{"location":"details/dashboards/dashboard-node-summary.html#memory-utilization","text":"No description","title":"Memory Utilization"},{"location":"details/dashboards/dashboard-node-summary.html#virtual-memory-utilization","text":"No description","title":"Virtual Memory Utilization"},{"location":"details/dashboards/dashboard-node-summary.html#swap-space","text":"No description","title":"Swap Space"},{"location":"details/dashboards/dashboard-node-summary.html#swap-activity","text":"Swap Activity is memory management that involves swapping sections of memory to and from physical storage.","title":"Swap Activity"},{"location":"details/dashboards/dashboard-node-summary.html#io-activity","text":"Disk I/O includes read or write or input/output operations involving a physical disk. It is the speed with which the data transfer takes place between the hard disk drive and RAM.","title":"I/O Activity"},{"location":"details/dashboards/dashboard-node-summary.html#global-file-descriptors-usage","text":"No description","title":"Global File Descriptors Usage"},{"location":"details/dashboards/dashboard-node-summary.html#disk-io-latency","text":"Shows average latency for Reads and Writes IO Devices. Higher than typical latency for highly loaded storage indicates saturation (overload) and is frequent cause of performance problems. Higher than normal latency also can indicate internal storage problems.","title":"Disk IO Latency"},{"location":"details/dashboards/dashboard-node-summary.html#disk-io-load","text":"Shows how much disk was loaded for reads or writes as average number of outstanding requests at different period of time. High disk load is a good measure of actual storage utilization. Different storage types handle load differently - some will show latency increases on low loads others can handle higher load with no problems.","title":"Disk IO Load"},{"location":"details/dashboards/dashboard-node-summary.html#network-traffic","text":"Network traffic refers to the amount of data moving across a network at a given point in time.","title":"Network Traffic"},{"location":"details/dashboards/dashboard-node-summary.html#network-utilization-hourly","text":"No description","title":"Network Utilization Hourly"},{"location":"details/dashboards/dashboard-node-summary.html#local-network-errors","text":"Total Number of Local Network Interface Transmit Errors, Receive Errors and Drops. Should be Zero","title":"Local Network Errors"},{"location":"details/dashboards/dashboard-node-summary.html#tcp-retransmission","text":"Retransmission, essentially identical with Automatic repeat request (ARQ), is the resending of packets which have been either damaged or lost. Retransmission is one of the basic mechanisms used by protocols operating over a packet switched computer network to provide reliable communication (such as that provided by a reliable byte stream, for example TCP).","title":"TCP Retransmission"},{"location":"details/dashboards/dashboard-node-temperature-details.html","text":"Node Temperature Details \u00b6 The Node Temperature Details dashboard exposes hardware monitoring and sensor data obtained through the sysfs virtual file system of the node. Hardware monitoring devices attached to the CPU and/or other chips on the motherboard let you monitor the hardware health of a system. Most modern systems include several of such devices. The actual list can include temperature sensors, voltage sensors, fan speed sensors, and various additional features, such as the ability to control the rotation speed of the fans. CPU Cores Temperatures \u00b6 Presents data taken from the temperature sensors of the CPU Chips Temperatures \u00b6 Presents data taken from the temperature sensors connected to other system controllers Fan Rotation Speeds \u00b6 Fan rotation speeds reported in RPM (rotations per minute). Fan Power Usage \u00b6 Describes the pulse width modulation of the PWN-equipped fans. PWM operates like a switch that constantly cycles on and off, thereby regulating the amount of power the fan gains: 100% makes it rotate at full speed, while lower percentage slows rotation down proportionally.","title":"Node Temperature Details"},{"location":"details/dashboards/dashboard-node-temperature-details.html#node-temperature-details","text":"The Node Temperature Details dashboard exposes hardware monitoring and sensor data obtained through the sysfs virtual file system of the node. Hardware monitoring devices attached to the CPU and/or other chips on the motherboard let you monitor the hardware health of a system. Most modern systems include several of such devices. The actual list can include temperature sensors, voltage sensors, fan speed sensors, and various additional features, such as the ability to control the rotation speed of the fans.","title":"Node Temperature Details"},{"location":"details/dashboards/dashboard-node-temperature-details.html#cpu-cores-temperatures","text":"Presents data taken from the temperature sensors of the CPU","title":"CPU Cores Temperatures"},{"location":"details/dashboards/dashboard-node-temperature-details.html#chips-temperatures","text":"Presents data taken from the temperature sensors connected to other system controllers","title":"Chips Temperatures"},{"location":"details/dashboards/dashboard-node-temperature-details.html#fan-rotation-speeds","text":"Fan rotation speeds reported in RPM (rotations per minute).","title":"Fan Rotation Speeds"},{"location":"details/dashboards/dashboard-node-temperature-details.html#fan-power-usage","text":"Describes the pulse width modulation of the PWN-equipped fans. PWM operates like a switch that constantly cycles on and off, thereby regulating the amount of power the fan gains: 100% makes it rotate at full speed, while lower percentage slows rotation down proportionally.","title":"Fan Power Usage"},{"location":"details/dashboards/dashboard-nodes-compare.html","text":"Nodes Compare \u00b6 This dashboard lets you compare a wide range of parameters. Parameters of the same type are shown side by side for all servers, grouped into the following sections: System Information CPU Memory Disk Partitions Disk Performance Network The System Information section shows the System Info summary of each server, as well as System Uptime , CPU Cores , RAM , Saturation Metrics , and Load Average gauges. The CPU section offers the CPU Usage , Interrupts , and Context Switches metrics. In the Memory section, you can find the Memory Usage , Swap Usage , and Swap Activity metrics. The Disk Partitions section encapsulates two metrics, Mountpoint Usage and Free Space . The Disk Performance section contains the I/O Activity , Disk Operations , Disk Bandwidth , Disk IO Utilization , Disk Latency , and Disk Load metrics. Finally, Network section shows Network Traffic , and Network Utilization Hourly metrics.","title":"Nodes Compare"},{"location":"details/dashboards/dashboard-nodes-compare.html#nodes-compare","text":"This dashboard lets you compare a wide range of parameters. Parameters of the same type are shown side by side for all servers, grouped into the following sections: System Information CPU Memory Disk Partitions Disk Performance Network The System Information section shows the System Info summary of each server, as well as System Uptime , CPU Cores , RAM , Saturation Metrics , and Load Average gauges. The CPU section offers the CPU Usage , Interrupts , and Context Switches metrics. In the Memory section, you can find the Memory Usage , Swap Usage , and Swap Activity metrics. The Disk Partitions section encapsulates two metrics, Mountpoint Usage and Free Space . The Disk Performance section contains the I/O Activity , Disk Operations , Disk Bandwidth , Disk IO Utilization , Disk Latency , and Disk Load metrics. Finally, Network section shows Network Traffic , and Network Utilization Hourly metrics.","title":"Nodes Compare"},{"location":"details/dashboards/dashboard-nodes-overview.html","text":"Nodes Overview \u00b6 The Nodes Overview dashboard provides details about the efficiency of work of the following components. Each component is represented as a section in the dashboard. CPU Memory & Swap Disk Network The CPU section offers the CPU Usage , CPU Saturation and Max Core Usage , Interrupts and Context Switches , and Processes metrics. In the Memory section, you can find the Memory Utilization , Virtual Memory Utilization , Swap Space , and Swap Activity metrics. The Disk section contains the I/O Activity , Global File Descriptors Usage , Disk IO Latency , and Disk IO Load metrics. In the Network section, you can find the Network Traffic , Network Utilization Hourly , Local Network Errors , and TCP Retransmission metrics.","title":"Nodes Overview"},{"location":"details/dashboards/dashboard-nodes-overview.html#nodes-overview","text":"The Nodes Overview dashboard provides details about the efficiency of work of the following components. Each component is represented as a section in the dashboard. CPU Memory & Swap Disk Network The CPU section offers the CPU Usage , CPU Saturation and Max Core Usage , Interrupts and Context Switches , and Processes metrics. In the Memory section, you can find the Memory Utilization , Virtual Memory Utilization , Swap Space , and Swap Activity metrics. The Disk section contains the I/O Activity , Global File Descriptors Usage , Disk IO Latency , and Disk IO Load metrics. In the Network section, you can find the Network Traffic , Network Utilization Hourly , Local Network Errors , and TCP Retransmission metrics.","title":"Nodes Overview"},{"location":"details/dashboards/dashboard-numa-details.html","text":"NUMA Details \u00b6 For each node, this dashboard shows metrics related to Non-uniform memory access (NUMA). Memory Usage \u00b6 Remotes over time the total, used, and free memory. Free Memory Percent \u00b6 Shows the free memory as the ratio to the total available memory. NUMA Memory Usage Types \u00b6 Dirty Memory waiting to be written back to disk Bounce Memory used for block device bounce buffers Mapped Files which have been mapped, such as libraries KernelStack The memory the kernel stack uses. This is not reclaimable. NUMA Allocation Hits \u00b6 Memory successfully allocated on this node as intended. NUMA Allocation Missed \u00b6 Memory missed is allocated on a node despite the process preferring some different node. Memory foreign is intended for a node, but actually allocated on some different node. Anonymous Memory \u00b6 Active Anonymous memory that has been used more recently and usually not swapped out. Inactive Anonymous memory that has not been used recently and can be swapped out. NUMA File (PageCache) \u00b6 Active(file) Pagecache memory that has been used more recently and usually not reclaimed until needed. Inactive(file) Pagecache memory that can be reclaimed without huge performance impact. Shared Memory \u00b6 Shmem Total used shared memory (shared between several processes, thus including RAM disks, SYS-V-IPC and BSD like SHMEM). HugePages Statistics \u00b6 Total Number of hugepages being allocated by the kernel (Defined with vm.nr_hugepages ). Free The number of hugepages not being allocated by a process Surp The number of hugepages in the pool above the value in vm.nr_hugepages . The maximum number of surplus hugepages is controlled by vm.nr_overcommit_hugepages . Local Processes \u00b6 Memory allocated on a node while a process was running on it. Remote Processes \u00b6 Memory allocated on a node while a process was running on some other node. Slab Memory \u00b6 Slab Allocation is a memory management mechanism intended for the efficient memory allocation of kernel objects. SReclaimable The part of the Slab that might be reclaimed (such as caches). SUnreclaim The part of the Slab that can\u2019t be reclaimed under memory pressure","title":"NUMA Details"},{"location":"details/dashboards/dashboard-numa-details.html#numa-details","text":"For each node, this dashboard shows metrics related to Non-uniform memory access (NUMA).","title":"NUMA Details"},{"location":"details/dashboards/dashboard-numa-details.html#memory-usage","text":"Remotes over time the total, used, and free memory.","title":"Memory Usage"},{"location":"details/dashboards/dashboard-numa-details.html#free-memory-percent","text":"Shows the free memory as the ratio to the total available memory.","title":"Free Memory Percent"},{"location":"details/dashboards/dashboard-numa-details.html#numa-memory-usage-types","text":"Dirty Memory waiting to be written back to disk Bounce Memory used for block device bounce buffers Mapped Files which have been mapped, such as libraries KernelStack The memory the kernel stack uses. This is not reclaimable.","title":"NUMA Memory Usage Types"},{"location":"details/dashboards/dashboard-numa-details.html#numa-allocation-hits","text":"Memory successfully allocated on this node as intended.","title":"NUMA Allocation Hits"},{"location":"details/dashboards/dashboard-numa-details.html#numa-allocation-missed","text":"Memory missed is allocated on a node despite the process preferring some different node. Memory foreign is intended for a node, but actually allocated on some different node.","title":"NUMA Allocation Missed"},{"location":"details/dashboards/dashboard-numa-details.html#anonymous-memory","text":"Active Anonymous memory that has been used more recently and usually not swapped out. Inactive Anonymous memory that has not been used recently and can be swapped out.","title":"Anonymous Memory"},{"location":"details/dashboards/dashboard-numa-details.html#numa-file-pagecache","text":"Active(file) Pagecache memory that has been used more recently and usually not reclaimed until needed. Inactive(file) Pagecache memory that can be reclaimed without huge performance impact.","title":"NUMA File (PageCache)"},{"location":"details/dashboards/dashboard-numa-details.html#shared-memory","text":"Shmem Total used shared memory (shared between several processes, thus including RAM disks, SYS-V-IPC and BSD like SHMEM).","title":"Shared Memory"},{"location":"details/dashboards/dashboard-numa-details.html#hugepages-statistics","text":"Total Number of hugepages being allocated by the kernel (Defined with vm.nr_hugepages ). Free The number of hugepages not being allocated by a process Surp The number of hugepages in the pool above the value in vm.nr_hugepages . The maximum number of surplus hugepages is controlled by vm.nr_overcommit_hugepages .","title":"HugePages Statistics"},{"location":"details/dashboards/dashboard-numa-details.html#local-processes","text":"Memory allocated on a node while a process was running on it.","title":"Local Processes"},{"location":"details/dashboards/dashboard-numa-details.html#remote-processes","text":"Memory allocated on a node while a process was running on some other node.","title":"Remote Processes"},{"location":"details/dashboards/dashboard-numa-details.html#slab-memory","text":"Slab Allocation is a memory management mechanism intended for the efficient memory allocation of kernel objects. SReclaimable The part of the Slab that might be reclaimed (such as caches). SUnreclaim The part of the Slab that can\u2019t be reclaimed under memory pressure","title":"Slab Memory"},{"location":"details/dashboards/dashboard-postgresql-instance-summary.html","text":"PostgreSQL Instance Summary \u00b6 Number of Temp Files \u00b6 Cumulative number of temporary files created by queries in this database since service start. All temporary files are counted, regardless of why the temporary file was created (e.g., sorting or hashing), and regardless of the log_temp_files setting. Size of Temp Files \u00b6 Cumulative amount of data written to temporary files by queries in this database since service start. All temporary files are counted, regardless of why the temporary file was created, and regardless of the log_temp_files setting. Temp Files Activity \u00b6 Number of temporary files created by queries in this database. All temporary files are counted, regardless of why the temporary file was created (e.g., sorting or hashing), and regardless of the log_temp_files setting. Temp Files Utilization \u00b6 Total amount of data written to temporary files by queries in this database. All temporary files are counted, regardless of why the temporary file was created, and regardless of the log_temp_files setting. Canceled Queries \u00b6 Based on pg_stat_database_conflicts view","title":"PostgreSQL Instance Summary"},{"location":"details/dashboards/dashboard-postgresql-instance-summary.html#postgresql-instance-summary","text":"","title":"PostgreSQL Instance Summary"},{"location":"details/dashboards/dashboard-postgresql-instance-summary.html#number-of-temp-files","text":"Cumulative number of temporary files created by queries in this database since service start. All temporary files are counted, regardless of why the temporary file was created (e.g., sorting or hashing), and regardless of the log_temp_files setting.","title":"Number of Temp Files"},{"location":"details/dashboards/dashboard-postgresql-instance-summary.html#size-of-temp-files","text":"Cumulative amount of data written to temporary files by queries in this database since service start. All temporary files are counted, regardless of why the temporary file was created, and regardless of the log_temp_files setting.","title":"Size of Temp Files"},{"location":"details/dashboards/dashboard-postgresql-instance-summary.html#temp-files-activity","text":"Number of temporary files created by queries in this database. All temporary files are counted, regardless of why the temporary file was created (e.g., sorting or hashing), and regardless of the log_temp_files setting.","title":"Temp Files Activity"},{"location":"details/dashboards/dashboard-postgresql-instance-summary.html#temp-files-utilization","text":"Total amount of data written to temporary files by queries in this database. All temporary files are counted, regardless of why the temporary file was created, and regardless of the log_temp_files setting.","title":"Temp Files Utilization"},{"location":"details/dashboards/dashboard-postgresql-instance-summary.html#canceled-queries","text":"Based on pg_stat_database_conflicts view","title":"Canceled Queries"},{"location":"details/dashboards/dashboard-postgresql-instances-compare.html","text":"PostgreSQL Instances Compare \u00b6 No description","title":"PostgreSQL Instances Compare"},{"location":"details/dashboards/dashboard-postgresql-instances-compare.html#postgresql-instances-compare","text":"No description","title":"PostgreSQL Instances Compare"},{"location":"details/dashboards/dashboard-postgresql-instances-overview.html","text":"PostgreSQL Instances Overview \u00b6 Connected \u00b6 Reports whether PMM Server can connect to the PostgreSQL instance. Version \u00b6 The version of the PostgreSQL instance. Shared Buffers \u00b6 Defines the amount of memory the database server uses for shared memory buffers. Default is 128MB . Guidance on tuning is 25% of RAM, but generally doesn\u2019t exceed 40% . Disk-Page Buffers \u00b6 The setting wal_buffers defines how much memory is used for caching the write-ahead log entries. Generally this value is small ( 3% of shared_buffers value), but it may need to be modified for heavily loaded servers. Memory Size for each Sort \u00b6 The parameter work_mem defines the amount of memory assigned for internal sort operations and hash tables before writing to temporary disk files. The default is 4MB . Disk Cache Size \u00b6 PostgreSQL\u2019s effective_cache_size variable tunes how much RAM you expect to be available for disk caching. Generally adding Linux free+cached will give you a good idea. This value is used by the query planner whether plans will fit in memory, and when defined too low, can lead to some plans rejecting certain indexes. Autovacuum \u00b6 Whether autovacuum process is enabled or not. Generally the solution is to vacuum more often, not less. PostgreSQL Connections \u00b6 Max Connections The maximum number of client connections allowed. Change this value with care as there are some memory resources that are allocated on a per-client basis, so setting max_connections higher will generally increase overall PostgreSQL memory usage. Connections The number of connection attempts (successful or not) to the PostgreSQL server. Active Connections The number of open connections to the PostgreSQL server. PostgreSQL Tuples \u00b6 Tuples The total number of rows processed by PostgreSQL server: fetched, returned, inserted, updated, and deleted. Read Tuple Activity The number of rows read from the database: as returned so fetched ones. Tuples Changed per 5 min The number of rows changed in the last 5 minutes: inserted, updated, and deleted ones. PostgreSQL Transactions \u00b6 Transactions The total number of transactions that have been either been committed or rolled back. Duration of Transactions Maximum duration in seconds any active transaction has been running. Temp Files \u00b6 Number of Temp Files The number of temporary files created by queries. Size of Temp files The total amount of data written to temporary files by queries in bytes. All temporary files are taken into account by these two gauges, regardless of why the temporary file was created (e.g., sorting or hashing), and regardless of the log_temp_files setting. Conflicts and Locks \u00b6 Conflicts/Deadlocks The number of queries canceled due to conflicts with recovery in the database (due to dropped tablespaces, lock timeouts, old snapshots, pinned buffers, or deadlocks). Number of Locks The number of deadlocks detected by PostgreSQL. Buffers and Blocks Operations \u00b6 Operations with Blocks The time spent reading and writing data file blocks by back ends, in milliseconds. Tip Capturing read and write time statistics is possible only if track_io_timing setting is enabled. This can be done either in configuration file or with the following query executed on the running system: ALTER SYSTEM SET track_io_timing = ON ; SELECT pg_reload_conf (); Buffers The number of buffers allocated by PostgreSQL. Canceled Queries \u00b6 The number of queries that have been canceled due to dropped tablespaces, lock timeouts, old snapshots, pinned buffers, and deadlocks. Data shown by this gauge are based on the pg_stat_database_conflicts view. Cache Hit Ratio \u00b6 The number of times disk blocks were found already in the buffer cache, so that a read was not necessary. This only includes hits in the PostgreSQL buffer cache, not the operating system\u2019s file system cache. Checkpoint Stats \u00b6 The total amount of time that has been spent in the portion of checkpoint processing where files are either written or synchronized to disk, in milliseconds. PostgreSQL Settings \u00b6 The list of all settings of the PostgreSQL server. System Summary \u00b6 This section contains the following system parameters of the PostgreSQL server: CPU Usage, CPU Saturation and Max Core Usage, Disk I/O Activity, and Network Traffic.","title":"PostgreSQL Dashboards"},{"location":"details/dashboards/dashboard-postgresql-instances-overview.html#postgresql-instances-overview","text":"","title":"PostgreSQL Instances Overview"},{"location":"details/dashboards/dashboard-postgresql-instances-overview.html#connected","text":"Reports whether PMM Server can connect to the PostgreSQL instance.","title":"Connected"},{"location":"details/dashboards/dashboard-postgresql-instances-overview.html#version","text":"The version of the PostgreSQL instance.","title":"Version"},{"location":"details/dashboards/dashboard-postgresql-instances-overview.html#shared-buffers","text":"Defines the amount of memory the database server uses for shared memory buffers. Default is 128MB . Guidance on tuning is 25% of RAM, but generally doesn\u2019t exceed 40% .","title":"Shared Buffers"},{"location":"details/dashboards/dashboard-postgresql-instances-overview.html#disk-page-buffers","text":"The setting wal_buffers defines how much memory is used for caching the write-ahead log entries. Generally this value is small ( 3% of shared_buffers value), but it may need to be modified for heavily loaded servers.","title":"Disk-Page Buffers"},{"location":"details/dashboards/dashboard-postgresql-instances-overview.html#memory-size-for-each-sort","text":"The parameter work_mem defines the amount of memory assigned for internal sort operations and hash tables before writing to temporary disk files. The default is 4MB .","title":"Memory Size for each Sort"},{"location":"details/dashboards/dashboard-postgresql-instances-overview.html#disk-cache-size","text":"PostgreSQL\u2019s effective_cache_size variable tunes how much RAM you expect to be available for disk caching. Generally adding Linux free+cached will give you a good idea. This value is used by the query planner whether plans will fit in memory, and when defined too low, can lead to some plans rejecting certain indexes.","title":"Disk Cache Size"},{"location":"details/dashboards/dashboard-postgresql-instances-overview.html#autovacuum","text":"Whether autovacuum process is enabled or not. Generally the solution is to vacuum more often, not less.","title":"Autovacuum"},{"location":"details/dashboards/dashboard-postgresql-instances-overview.html#postgresql-connections","text":"Max Connections The maximum number of client connections allowed. Change this value with care as there are some memory resources that are allocated on a per-client basis, so setting max_connections higher will generally increase overall PostgreSQL memory usage. Connections The number of connection attempts (successful or not) to the PostgreSQL server. Active Connections The number of open connections to the PostgreSQL server.","title":"PostgreSQL Connections"},{"location":"details/dashboards/dashboard-postgresql-instances-overview.html#postgresql-tuples","text":"Tuples The total number of rows processed by PostgreSQL server: fetched, returned, inserted, updated, and deleted. Read Tuple Activity The number of rows read from the database: as returned so fetched ones. Tuples Changed per 5 min The number of rows changed in the last 5 minutes: inserted, updated, and deleted ones.","title":"PostgreSQL Tuples"},{"location":"details/dashboards/dashboard-postgresql-instances-overview.html#postgresql-transactions","text":"Transactions The total number of transactions that have been either been committed or rolled back. Duration of Transactions Maximum duration in seconds any active transaction has been running.","title":"PostgreSQL Transactions"},{"location":"details/dashboards/dashboard-postgresql-instances-overview.html#temp-files","text":"Number of Temp Files The number of temporary files created by queries. Size of Temp files The total amount of data written to temporary files by queries in bytes. All temporary files are taken into account by these two gauges, regardless of why the temporary file was created (e.g., sorting or hashing), and regardless of the log_temp_files setting.","title":"Temp Files"},{"location":"details/dashboards/dashboard-postgresql-instances-overview.html#conflicts-and-locks","text":"Conflicts/Deadlocks The number of queries canceled due to conflicts with recovery in the database (due to dropped tablespaces, lock timeouts, old snapshots, pinned buffers, or deadlocks). Number of Locks The number of deadlocks detected by PostgreSQL.","title":"Conflicts and Locks"},{"location":"details/dashboards/dashboard-postgresql-instances-overview.html#buffers-and-blocks-operations","text":"Operations with Blocks The time spent reading and writing data file blocks by back ends, in milliseconds. Tip Capturing read and write time statistics is possible only if track_io_timing setting is enabled. This can be done either in configuration file or with the following query executed on the running system: ALTER SYSTEM SET track_io_timing = ON ; SELECT pg_reload_conf (); Buffers The number of buffers allocated by PostgreSQL.","title":"Buffers and Blocks Operations"},{"location":"details/dashboards/dashboard-postgresql-instances-overview.html#canceled-queries","text":"The number of queries that have been canceled due to dropped tablespaces, lock timeouts, old snapshots, pinned buffers, and deadlocks. Data shown by this gauge are based on the pg_stat_database_conflicts view.","title":"Canceled Queries"},{"location":"details/dashboards/dashboard-postgresql-instances-overview.html#cache-hit-ratio","text":"The number of times disk blocks were found already in the buffer cache, so that a read was not necessary. This only includes hits in the PostgreSQL buffer cache, not the operating system\u2019s file system cache.","title":"Cache Hit Ratio"},{"location":"details/dashboards/dashboard-postgresql-instances-overview.html#checkpoint-stats","text":"The total amount of time that has been spent in the portion of checkpoint processing where files are either written or synchronized to disk, in milliseconds.","title":"Checkpoint Stats"},{"location":"details/dashboards/dashboard-postgresql-instances-overview.html#postgresql-settings","text":"The list of all settings of the PostgreSQL server.","title":"PostgreSQL Settings"},{"location":"details/dashboards/dashboard-postgresql-instances-overview.html#system-summary","text":"This section contains the following system parameters of the PostgreSQL server: CPU Usage, CPU Saturation and Max Core Usage, Disk I/O Activity, and Network Traffic.","title":"System Summary"},{"location":"details/dashboards/dashboard-postgresql-vacuum-monitoring-experimental.html","text":"Experimental PostgreSQL Vacuum Monitoring \u00b6 Disclaimer This is an Experimental Dashboard that is not part of the official Percona Monitoring and Management (PMM) deployment and might be updated. We ship this Dashboard to obtain feedback from our users. This dashbaord provides timely insights into the autovacuum process in PostgreSQL. This dashboard contains the following: Dead tuples - Identifies the number of dead rows in each table even though the rows are physically removed from the table. Last time vacuum ran - Tracks the last time a vacuum or autovacuum process successfully ran on each of your tables. Number of rows modified since last Analyze - The number of rows changed since the last time ANALYZE ran. Manual vacuum events - Tracks the number of times a manual vacuum was run on each table. Table disk usage - Tracking the disk space used by each table is crucial as it enables you to gauge expected changes in the query performance over time - but it can also help you detect potential vacuuming-related issues.","title":"Experimental PostgreSQL Vacuum Monitoring"},{"location":"details/dashboards/dashboard-postgresql-vacuum-monitoring-experimental.html#experimental-postgresql-vacuum-monitoring","text":"Disclaimer This is an Experimental Dashboard that is not part of the official Percona Monitoring and Management (PMM) deployment and might be updated. We ship this Dashboard to obtain feedback from our users. This dashbaord provides timely insights into the autovacuum process in PostgreSQL. This dashboard contains the following: Dead tuples - Identifies the number of dead rows in each table even though the rows are physically removed from the table. Last time vacuum ran - Tracks the last time a vacuum or autovacuum process successfully ran on each of your tables. Number of rows modified since last Analyze - The number of rows changed since the last time ANALYZE ran. Manual vacuum events - Tracks the number of times a manual vacuum was run on each table. Table disk usage - Tracking the disk space used by each table is crucial as it enables you to gauge expected changes in the query performance over time - but it can also help you detect potential vacuuming-related issues.","title":"Experimental PostgreSQL Vacuum Monitoring"},{"location":"details/dashboards/dashboard-processes-details.html","text":"Processes Details \u00b6 The Processes Details dashboard displays Linux process information - PIDs, Threads, and Processes. The dashboard shows how many processes/threads are either in the kernel run queue (runnable state) or in the blocked queue (waiting for I/O). When the number of process in the runnable state is constantly higher than the number of CPU cores available, the load is CPU bound. When the number of process blocked waiting for I/O is large, the load is disk bound. The running average of the sum of these two quantities is the basis of the loadavg metric. The dashboard consists of two parts: the first section describes metrics for all hosts, and the second part provides charts for each host. Charts for all hosts, available in the first section, are the following ones: States of Processes Number of PIDs Percentage of Max PIDs Limit Number of Threads Percentage of Max Threads Limit Runnable Processes Blocked Processes Waiting for I/O Sleeping Processes Running Processes Disk Sleep Processes Stopped Processes Zombie Processes Dead Processes The following charts are present in the second part, available for each host: Processes States of Processes Number of PIDs Percentage of Max PIDs Limit Number of Threads Percentage of Max Threads Limit Number of PIDs \u00b6 No description Percentage of Max PIDs Limit \u00b6 No description Number of Threads \u00b6 No description Percentage of Max Threads Limit \u00b6 No description Runnable Processes \u00b6 Processes \u00b6 The Processes graph shows how many processes/threads are either in the kernel run queue (runnable state) or in the blocked queue (waiting for I/O). When the number of process in the runnable state is constantly higher than the number of CPU cores available, the load is CPU bound. When the number of process blocked waiting for I/O is large, the load is disk bound. The running average of the sum of these two quantities is the basis of the loadavg metric. Blocked Processes Waiting for I/O \u00b6 Processes \u00b6 The Processes graph shows how many processes/threads are either in the kernel run queue (runnable state) or in the blocked queue (waiting for I/O). When the number of process in the runnable state is constantly higher than the number of CPU cores available, the load is CPU bound. When the number of process blocked waiting for I/O is large, the load is disk bound. The running average of the sum of these two quantities is the basis of the loadavg metric. Sleeping Processes \u00b6 No description Running Processes \u00b6 No description Disk Sleep Processes \u00b6 No description Stopped Processes \u00b6 No description Zombie Processes \u00b6 No description Dead Processes \u00b6 No description","title":"Processes Details"},{"location":"details/dashboards/dashboard-processes-details.html#processes-details","text":"The Processes Details dashboard displays Linux process information - PIDs, Threads, and Processes. The dashboard shows how many processes/threads are either in the kernel run queue (runnable state) or in the blocked queue (waiting for I/O). When the number of process in the runnable state is constantly higher than the number of CPU cores available, the load is CPU bound. When the number of process blocked waiting for I/O is large, the load is disk bound. The running average of the sum of these two quantities is the basis of the loadavg metric. The dashboard consists of two parts: the first section describes metrics for all hosts, and the second part provides charts for each host. Charts for all hosts, available in the first section, are the following ones: States of Processes Number of PIDs Percentage of Max PIDs Limit Number of Threads Percentage of Max Threads Limit Runnable Processes Blocked Processes Waiting for I/O Sleeping Processes Running Processes Disk Sleep Processes Stopped Processes Zombie Processes Dead Processes The following charts are present in the second part, available for each host: Processes States of Processes Number of PIDs Percentage of Max PIDs Limit Number of Threads Percentage of Max Threads Limit","title":"Processes Details"},{"location":"details/dashboards/dashboard-processes-details.html#number-of-pids","text":"No description","title":"Number of PIDs"},{"location":"details/dashboards/dashboard-processes-details.html#percentage-of-max-pids-limit","text":"No description","title":"Percentage of Max PIDs Limit"},{"location":"details/dashboards/dashboard-processes-details.html#number-of-threads","text":"No description","title":"Number of Threads"},{"location":"details/dashboards/dashboard-processes-details.html#percentage-of-max-threads-limit","text":"No description","title":"Percentage of Max Threads Limit"},{"location":"details/dashboards/dashboard-processes-details.html#runnable-processes","text":"","title":"Runnable Processes"},{"location":"details/dashboards/dashboard-processes-details.html#processes","text":"The Processes graph shows how many processes/threads are either in the kernel run queue (runnable state) or in the blocked queue (waiting for I/O). When the number of process in the runnable state is constantly higher than the number of CPU cores available, the load is CPU bound. When the number of process blocked waiting for I/O is large, the load is disk bound. The running average of the sum of these two quantities is the basis of the loadavg metric.","title":"Processes"},{"location":"details/dashboards/dashboard-processes-details.html#blocked-processes-waiting-for-io","text":"","title":"Blocked Processes Waiting for I/O"},{"location":"details/dashboards/dashboard-processes-details.html#processes_1","text":"The Processes graph shows how many processes/threads are either in the kernel run queue (runnable state) or in the blocked queue (waiting for I/O). When the number of process in the runnable state is constantly higher than the number of CPU cores available, the load is CPU bound. When the number of process blocked waiting for I/O is large, the load is disk bound. The running average of the sum of these two quantities is the basis of the loadavg metric.","title":"Processes"},{"location":"details/dashboards/dashboard-processes-details.html#sleeping-processes","text":"No description","title":"Sleeping Processes"},{"location":"details/dashboards/dashboard-processes-details.html#running-processes","text":"No description","title":"Running Processes"},{"location":"details/dashboards/dashboard-processes-details.html#disk-sleep-processes","text":"No description","title":"Disk Sleep Processes"},{"location":"details/dashboards/dashboard-processes-details.html#stopped-processes","text":"No description","title":"Stopped Processes"},{"location":"details/dashboards/dashboard-processes-details.html#zombie-processes","text":"No description","title":"Zombie Processes"},{"location":"details/dashboards/dashboard-processes-details.html#dead-processes","text":"No description","title":"Dead Processes"},{"location":"details/dashboards/dashboard-prometheus-exporter-status.html","text":"Prometheus Exporter Status \u00b6 The Prometheus Exporter Status dashboard reports the consumption of resources by the Prometheus exporters used by PMM. For each exporter, this dashboard reveals the following information: CPU usage Memory usage File descriptors used Exporter uptime","title":"Prometheus Dashboards"},{"location":"details/dashboards/dashboard-prometheus-exporter-status.html#prometheus-exporter-status","text":"The Prometheus Exporter Status dashboard reports the consumption of resources by the Prometheus exporters used by PMM. For each exporter, this dashboard reveals the following information: CPU usage Memory usage File descriptors used Exporter uptime","title":"Prometheus Exporter Status"},{"location":"details/dashboards/dashboard-prometheus-exporters-overview.html","text":"Prometheus Exporters Overview \u00b6 Prometheus Exporters Summary \u00b6 This section provides a summary of how exporters are used across the selected hosts. It includes the average usage of CPU and memory as well as the number of hosts being monitored and the total number of running exporters. Avg CPU Usage per Host Shows the average CPU usage in percent per host for all exporters. Avg Memory Usage per Host Shows the Exporters average Memory usage per host. Monitored Hosts Shows the number of monitored hosts that are running Exporters. Exporters Running Shows the total number of Exporters running with this PMM Server instance. The CPU usage and memory usage do not include the additional CPU and memory usage required to produce metrics by the application or operating system. Prometheus Exporters Resource Usage by Node \u00b6 This section shows how resources, such as CPU and memory, are being used by the exporters for the selected hosts. CPU Usage Plots the Exporters\u2019 CPU usage across each monitored host (by default, All hosts). Memory Usage Plots the Exporters\u2019 Memory usage across each monitored host (by default, All hosts). Prometheus Exporters Resource Usage by Type \u00b6 This section shows how resources, such as CPU and memory, are being used by the exporters for host types: MySQL, MongoDB, ProxySQL, and the system. CPU Cores Used Shows the Exporters\u2019 CPU Cores used for each type of Exporter. Memory Usage Shows the Exporters\u2019 memory used for each type of Exporter. List of Hosts \u00b6 At the bottom, this dashboard shows details for each running host. CPU Used Show the CPU usage as a percentage for all Exporters. Mem Used Shows total Memory Used by Exporters. Exporters Running Shows the number of Exporters running. RAM Shows the total amount of RAM of the host. Virtual CPUs Shows the total number of virtual CPUs on the host. You can click the value of the CPU Used , Memory Used , or Exporters Running columns to open the Prometheus Exporter Status dashboard for further analysis. See also Percona blog: Understand Your Prometheus Exporters with Percona Monitoring and Management (PMM)","title":"Prometheus Exporters Overview"},{"location":"details/dashboards/dashboard-prometheus-exporters-overview.html#prometheus-exporters-overview","text":"","title":"Prometheus Exporters Overview"},{"location":"details/dashboards/dashboard-prometheus-exporters-overview.html#prometheus-exporters-summary","text":"This section provides a summary of how exporters are used across the selected hosts. It includes the average usage of CPU and memory as well as the number of hosts being monitored and the total number of running exporters. Avg CPU Usage per Host Shows the average CPU usage in percent per host for all exporters. Avg Memory Usage per Host Shows the Exporters average Memory usage per host. Monitored Hosts Shows the number of monitored hosts that are running Exporters. Exporters Running Shows the total number of Exporters running with this PMM Server instance. The CPU usage and memory usage do not include the additional CPU and memory usage required to produce metrics by the application or operating system.","title":"Prometheus Exporters Summary"},{"location":"details/dashboards/dashboard-prometheus-exporters-overview.html#prometheus-exporters-resource-usage-by-node","text":"This section shows how resources, such as CPU and memory, are being used by the exporters for the selected hosts. CPU Usage Plots the Exporters\u2019 CPU usage across each monitored host (by default, All hosts). Memory Usage Plots the Exporters\u2019 Memory usage across each monitored host (by default, All hosts).","title":"Prometheus Exporters Resource Usage by Node"},{"location":"details/dashboards/dashboard-prometheus-exporters-overview.html#prometheus-exporters-resource-usage-by-type","text":"This section shows how resources, such as CPU and memory, are being used by the exporters for host types: MySQL, MongoDB, ProxySQL, and the system. CPU Cores Used Shows the Exporters\u2019 CPU Cores used for each type of Exporter. Memory Usage Shows the Exporters\u2019 memory used for each type of Exporter.","title":"Prometheus Exporters Resource Usage by Type"},{"location":"details/dashboards/dashboard-prometheus-exporters-overview.html#list-of-hosts","text":"At the bottom, this dashboard shows details for each running host. CPU Used Show the CPU usage as a percentage for all Exporters. Mem Used Shows total Memory Used by Exporters. Exporters Running Shows the number of Exporters running. RAM Shows the total amount of RAM of the host. Virtual CPUs Shows the total number of virtual CPUs on the host. You can click the value of the CPU Used , Memory Used , or Exporters Running columns to open the Prometheus Exporter Status dashboard for further analysis. See also Percona blog: Understand Your Prometheus Exporters with Percona Monitoring and Management (PMM)","title":"List of Hosts"},{"location":"details/dashboards/dashboard-proxysql-instance-summary.html","text":"ProxySQL Instance Summary \u00b6 Network Traffic \u00b6 Network traffic refers to the amount of data moving across a network at a given point in time.","title":"ProxySQL Dashboards"},{"location":"details/dashboards/dashboard-proxysql-instance-summary.html#proxysql-instance-summary","text":"","title":"ProxySQL Instance Summary"},{"location":"details/dashboards/dashboard-proxysql-instance-summary.html#network-traffic","text":"Network traffic refers to the amount of data moving across a network at a given point in time.","title":"Network Traffic"},{"location":"details/dashboards/dashboard-pxc-galera-cluster-summary-experimental.html","text":"Experimental PXC/Galera Cluster Summary \u00b6 Disclaimer This is an Experimental Dashboard that is not part of the official Percona Monitoring and Management (PMM) deployment and might be updated. We ship this Dashboard to obtain feedback from our users. Availability This experimental dashboard is available starting with PMM 2.29.0. The experimental PXC/Galera Cluster Summary dashboard provides a high level information about the clusters, resource utilization and its state for MySQL databases.","title":"Experimental PXC/Galera Cluster Summary"},{"location":"details/dashboards/dashboard-pxc-galera-cluster-summary-experimental.html#experimental-pxcgalera-cluster-summary","text":"Disclaimer This is an Experimental Dashboard that is not part of the official Percona Monitoring and Management (PMM) deployment and might be updated. We ship this Dashboard to obtain feedback from our users. Availability This experimental dashboard is available starting with PMM 2.29.0. The experimental PXC/Galera Cluster Summary dashboard provides a high level information about the clusters, resource utilization and its state for MySQL databases.","title":"Experimental PXC/Galera Cluster Summary"},{"location":"details/dashboards/dashboard-pxc-galera-cluster-summary.html","text":"PXC/Galera Cluster Summary \u00b6 No description","title":"PXC/Galera Cluster Summary"},{"location":"details/dashboards/dashboard-pxc-galera-cluster-summary.html#pxcgalera-cluster-summary","text":"No description","title":"PXC/Galera Cluster Summary"},{"location":"details/dashboards/dashboard-pxc-galera-node-summary.html","text":"PXC/Galera Node Summary \u00b6 Galera Replication Latency \u00b6 Shows figures for the replication latency on group communication. It measures latency from the time point when a message is sent out to the time point when a message is received. As replication is a group operation, this essentially gives you the slowest ACK and longest RTT in the cluster. Galera Replication Queues \u00b6 Shows the length of receive and send queues. Galera Cluster Size \u00b6 Shows the number of members currently connected to the cluster. Galera Flow Control \u00b6 Shows the number of FC_PAUSE events sent/received. They are sent by a node when its replication queue gets too full. If a node is sending out FC messages it indicates a problem. Galera Parallelization Efficiency \u00b6 Shows the average distances between highest and lowest seqno that are concurrently applied, committed and can be possibly applied in parallel (potential degree of parallelization). Galera Writing Conflicts \u00b6 Shows the number of local transactions being committed on this node that failed certification (some other node had a commit that conflicted with ours) \u2013 client received deadlock error on commit and also the number of local transactions in flight on this node that were aborted because they locked something an applier thread needed \u2013 deadlock error anywhere in an open transaction. Spikes in the graph may indicate writing to the same table potentially the same rows from 2 nodes. Available Downtime before SST Required \u00b6 Shows for how long the node can be taken out of the cluster before SST is required. SST is a full state transfer method. Galera Writeset Count \u00b6 Shows the count of transactions received from the cluster (any other node) and replicated to the cluster (from this node). Galera Writeset Size \u00b6 Shows the average transaction size received/replicated. Galera Writeset Traffic \u00b6 Shows the bytes of data received from the cluster (any other node) and replicated to the cluster (from this node). Galera Network Usage Hourly \u00b6 Shows the bytes of data received from the cluster (any other node) and replicated to the cluster (from this node).","title":"HA Dashboards"},{"location":"details/dashboards/dashboard-pxc-galera-node-summary.html#pxcgalera-node-summary","text":"","title":"PXC/Galera Node Summary"},{"location":"details/dashboards/dashboard-pxc-galera-node-summary.html#galera-replication-latency","text":"Shows figures for the replication latency on group communication. It measures latency from the time point when a message is sent out to the time point when a message is received. As replication is a group operation, this essentially gives you the slowest ACK and longest RTT in the cluster.","title":"Galera Replication Latency"},{"location":"details/dashboards/dashboard-pxc-galera-node-summary.html#galera-replication-queues","text":"Shows the length of receive and send queues.","title":"Galera Replication Queues"},{"location":"details/dashboards/dashboard-pxc-galera-node-summary.html#galera-cluster-size","text":"Shows the number of members currently connected to the cluster.","title":"Galera Cluster Size"},{"location":"details/dashboards/dashboard-pxc-galera-node-summary.html#galera-flow-control","text":"Shows the number of FC_PAUSE events sent/received. They are sent by a node when its replication queue gets too full. If a node is sending out FC messages it indicates a problem.","title":"Galera Flow Control"},{"location":"details/dashboards/dashboard-pxc-galera-node-summary.html#galera-parallelization-efficiency","text":"Shows the average distances between highest and lowest seqno that are concurrently applied, committed and can be possibly applied in parallel (potential degree of parallelization).","title":"Galera Parallelization Efficiency"},{"location":"details/dashboards/dashboard-pxc-galera-node-summary.html#galera-writing-conflicts","text":"Shows the number of local transactions being committed on this node that failed certification (some other node had a commit that conflicted with ours) \u2013 client received deadlock error on commit and also the number of local transactions in flight on this node that were aborted because they locked something an applier thread needed \u2013 deadlock error anywhere in an open transaction. Spikes in the graph may indicate writing to the same table potentially the same rows from 2 nodes.","title":"Galera Writing Conflicts"},{"location":"details/dashboards/dashboard-pxc-galera-node-summary.html#available-downtime-before-sst-required","text":"Shows for how long the node can be taken out of the cluster before SST is required. SST is a full state transfer method.","title":"Available Downtime before SST Required"},{"location":"details/dashboards/dashboard-pxc-galera-node-summary.html#galera-writeset-count","text":"Shows the count of transactions received from the cluster (any other node) and replicated to the cluster (from this node).","title":"Galera Writeset Count"},{"location":"details/dashboards/dashboard-pxc-galera-node-summary.html#galera-writeset-size","text":"Shows the average transaction size received/replicated.","title":"Galera Writeset Size"},{"location":"details/dashboards/dashboard-pxc-galera-node-summary.html#galera-writeset-traffic","text":"Shows the bytes of data received from the cluster (any other node) and replicated to the cluster (from this node).","title":"Galera Writeset Traffic"},{"location":"details/dashboards/dashboard-pxc-galera-node-summary.html#galera-network-usage-hourly","text":"Shows the bytes of data received from the cluster (any other node) and replicated to the cluster (from this node).","title":"Galera Network Usage Hourly"},{"location":"details/dashboards/dashboard-pxc-galera-nodes-compare.html","text":"PXC/Galera Nodes Compare \u00b6 $cluster - Galera Cluster Size \u00b6 Shows the number of members currently connected to the cluster.","title":"PXC/Galera Nodes Compare"},{"location":"details/dashboards/dashboard-pxc-galera-nodes-compare.html#pxcgalera-nodes-compare","text":"","title":"PXC/Galera Nodes Compare"},{"location":"details/dashboards/dashboard-pxc-galera-nodes-compare.html#cluster-galera-cluster-size","text":"Shows the number of members currently connected to the cluster.","title":"$cluster - Galera Cluster Size"},{"location":"details/dashboards/dashboard-victoriametrics-agents-overview.html","text":"VictoriaMetrics Agents Overview \u00b6 No description","title":"VictoriaMetrics Agents Overview"},{"location":"details/dashboards/dashboard-victoriametrics-agents-overview.html#victoriametrics-agents-overview","text":"No description","title":"VictoriaMetrics Agents Overview"},{"location":"details/dashboards/dashboard-victoriametrics.html","text":"VictoriaMetrics \u00b6 No description","title":"VictoriaMetrics"},{"location":"details/dashboards/dashboard-victoriametrics.html#victoriametrics","text":"No description","title":"VictoriaMetrics"},{"location":"details/develop-checks/index.html","text":"Developing Advisor checks \u00b6 PMM offers sets of checks that can detect common security threats, performance degradation, data loss and data corruption. As a developer, you can create custom checks to cover additional use cases, relevant to your specific database infrastructure. Check components \u00b6 A check is a combination of: A query for extracting data from the database. Python script for converting extracted data into check results. This is actually a Starlark script, which is a Python dialect that adds more imperative features than Python. The script\u2019s execution environment is sandboxed, and no I/O can be done from it. All checks are self-contained in the first phase, as well as in most of the planned phases. This means that extracted data is processed on the PMM side and not sent back to Percona Platform. Backend \u00b6 pmm-managed checks that the installation is opted-in for checks. pmm-managed downloads files with checks from Percona Platform. pmm-managed verifies file signatures using a list of hard-coded public keys. At least one signature should be correct. pmm-managed sends queries to pmm-agent and gathers results. pmm-managed executes check scripts that produce alert information. pmm-managed sends alerts to Alertmanager. Due to Alertmanager design, pmm-managed has to send and re-send alerts to it much more often than the frequency with which checks are executed. This expected behaviour is not important for using checks but is important for understanding how checks work. Currently, Prometheus is not involved. Frontend \u00b6 PMM uses Alertmanager API to get information about failed checks and show them on the UI: Advisor check format version 2 \u00b6 PMM 2.28 upgraded Advisor checks to version 2 to accommodate the following significant enhancements introduced in this release: Support for multiple queries Support for Victoria Metrics as a data source Database Family field to specify one of the supported database families: MYSQL, POSTGRESQL, MONGODB. The enhancements in version 2 enable you to create more intelligent advisor checks, that deliver more value to your connected PMM instances. If you are creating checks for PMM version 2.28 and newer, see Advisor checks v.2 for information on developing custom checks for PMM 2.28 and later. Advisor check format version 1 \u00b6 If you are creating checks for PMM version 2.27 and older, see Advisor checks v.1 , for information on creating custom checks for PMM 2.27 and older. Submit feedback \u00b6 We welcome your feedback on the current process for developing and debugging checks. Send us your comments over Slack or post a question on the Percona Forums .","title":"Develop checks"},{"location":"details/develop-checks/index.html#developing-advisor-checks","text":"PMM offers sets of checks that can detect common security threats, performance degradation, data loss and data corruption. As a developer, you can create custom checks to cover additional use cases, relevant to your specific database infrastructure.","title":"Developing Advisor checks"},{"location":"details/develop-checks/index.html#check-components","text":"A check is a combination of: A query for extracting data from the database. Python script for converting extracted data into check results. This is actually a Starlark script, which is a Python dialect that adds more imperative features than Python. The script\u2019s execution environment is sandboxed, and no I/O can be done from it. All checks are self-contained in the first phase, as well as in most of the planned phases. This means that extracted data is processed on the PMM side and not sent back to Percona Platform.","title":"Check components"},{"location":"details/develop-checks/index.html#backend","text":"pmm-managed checks that the installation is opted-in for checks. pmm-managed downloads files with checks from Percona Platform. pmm-managed verifies file signatures using a list of hard-coded public keys. At least one signature should be correct. pmm-managed sends queries to pmm-agent and gathers results. pmm-managed executes check scripts that produce alert information. pmm-managed sends alerts to Alertmanager. Due to Alertmanager design, pmm-managed has to send and re-send alerts to it much more often than the frequency with which checks are executed. This expected behaviour is not important for using checks but is important for understanding how checks work. Currently, Prometheus is not involved.","title":"Backend"},{"location":"details/develop-checks/index.html#frontend","text":"PMM uses Alertmanager API to get information about failed checks and show them on the UI:","title":"Frontend"},{"location":"details/develop-checks/index.html#advisor-check-format-version-2","text":"PMM 2.28 upgraded Advisor checks to version 2 to accommodate the following significant enhancements introduced in this release: Support for multiple queries Support for Victoria Metrics as a data source Database Family field to specify one of the supported database families: MYSQL, POSTGRESQL, MONGODB. The enhancements in version 2 enable you to create more intelligent advisor checks, that deliver more value to your connected PMM instances. If you are creating checks for PMM version 2.28 and newer, see Advisor checks v.2 for information on developing custom checks for PMM 2.28 and later.","title":"Advisor check format version 2"},{"location":"details/develop-checks/index.html#advisor-check-format-version-1","text":"If you are creating checks for PMM version 2.27 and older, see Advisor checks v.1 , for information on creating custom checks for PMM 2.27 and older.","title":"Advisor check format version 1"},{"location":"details/develop-checks/index.html#submit-feedback","text":"We welcome your feedback on the current process for developing and debugging checks. Send us your comments over Slack or post a question on the Percona Forums .","title":"Submit feedback"},{"location":"details/develop-checks/checks-v1.html","text":"Version 1 checks for PMM 2.27 and older \u00b6 Advisor checks created for PMM 2.27 and older use a slightly different structure than checks for 2.28. This is because, compared to 2.28 checks, 2.27 checks do not support: Multiple queries Victoria Metrics as a data source No database Family field Format for v.1 checks \u00b6 Checks for PMM 2.27 and older use the following format: Version 1 Checks Format --- checks : - version : 1 name : example summary : Example check description : This check is just an example. type : MONGODB_BUILDINFO script : | def check(docs): # for compatibility with PMM Server < 2.12 context = { \"format_version_num\": format_version_num, \"parse_version\": parse_version, } return check_context(docs, context) def check_context(docs, context): # `docs` is a frozen (deeply immutable) list of dicts where each dict represents a single document in result set. # `context` is a dict with additional functions. # # Global `print` and `fail` functions are available. # # `check_context` function is expected to return a list of dicts that are then converted to alerts; # in particular, that list can be empty. # Any other value (for example, string) is treated as script execution failure # (Starlark does not support Python exceptions); # it is recommended to use global function `fail` for that instead. format_version_num = context.get(\"format_version_num\", fail) parse_version = context.get(\"parse_version\", fail) print(\"first doc =\", repr(docs[0])) return [{ \"summary\": \"Example summary\", \"description\": \"Example description\", \"severity\": \"warning\", \"labels\": { \"version\": format_version_num(10203), } }] Realistic Example of Check in v.1 Format --- checks : - version : 1 name : mongodb_version summary : MongoDB Version description : This check returns warnings if MongoDB/PSMDB version is not the latest one. type : MONGODB_BUILDINFO script : |- LATEST_VERSIONS = { \"mongodb\": { \"3.6\": 30620, # https://docs.mongodb.com/manual/release-notes/3.6/ \"4.0\": 40020, # https://docs.mongodb.com/manual/release-notes/4.0/ \"4.2\": 40210, # https://docs.mongodb.com/manual/release-notes/4.2/ \"4.4\": 40401, # https://docs.mongodb.com/manual/release-notes/4.4/ }, \"percona\": { \"3.6\": 30620, # https://www.percona.com/downloads/percona-server-mongodb-3.6/ \"4.0\": 40020, # https://www.percona.com/downloads/percona-server-mongodb-4.0/ \"4.2\": 40209, # https://www.percona.com/downloads/percona-server-mongodb-4.2/ \"4.4\": 40401, # https://www.percona.com/downloads/percona-server-mongodb-4.4/ }, } def check(docs): # for compatibility with PMM Server < 2.12 context = { \"format_version_num\": format_version_num, \"parse_version\": parse_version, } return check_context(docs, context) def check_context(docs, context): # `docs` is a frozen (deeply immutable) list of dicts where each dict represents a single document in result set. # `context` is a dict with additional functions. # # Global `print` and `fail` functions are available. # # `check_context` function is expected to return a list of dicts that are then converted to alerts; # in particular, that list can be empty. # Any other value (for example, string) is treated as script execution failure # (Starlark does not support Python exceptions); # it is recommended to use global function `fail` for that instead. \"\"\" This check returns warnings if MongoDB/PSMDB version is not the latest one. \"\"\" format_version_num = context.get(\"format_version_num\", fail) parse_version = context.get(\"parse_version\", fail) if len(docs) != 1: return \"Unexpected number of documents\" info = docs[0] # extract information is_percona = 'psmdbVersion' in info # parse_version returns a dict with keys: major, minor, patch, rest, num version = parse_version(info[\"version\"]) print(\"version =\", repr(version)) num = version[\"num\"] mm = \"{}.{}\".format(version[\"major\"], version[\"minor\"]) results = [] if is_percona: latest = LATEST_VERSIONS[\"percona\"][mm] if latest > num: results.append({ \"summary\": \"Newer version of Percona Server for MongoDB is available\", \"description\": \"Current version is {}, latest available version is {}.\".format(format_version_num(num), format_version_num(latest)), \"severity\": \"warning\", \"labels\": { \"current\": format_version_num(num), \"latest\": format_version_num(latest), }, }) return results if True: # MongoDB latest = LATEST_VERSIONS[\"mongodb\"][mm] if latest > num: results.append({ \"summary\": \"Newer version of MongoDB is available\", \"description\": \"Current version is {}, latest available version is {}.\".format(format_version_num(num), format_version_num(latest)), \"severity\": \"warning\", \"labels\": { \"current\": format_version_num(num), \"latest\": format_version_num(latest), }, }) return results Security checks in PMM 2.26 and older \u00b6 PMM 2.26 and older included a set of security checks grouped under the Security Threat Tool option. With the 2.27 release, security checks have been renamed to Advisor checks, and the Security Threat Tool option in the PMM Settings was renamed to Advisors . Checks script \u00b6 The check script assumes that there is a function with a fixed name, that accepts a list of docs containing returned rows for SQL databases and documents for MongoDB. It returns zero, one, or several check results that are then converted to alerts. PMM 2.12.0 and earlier function name is check , while newer versions use name check_context . Both have the same meaning. Function signature \u00b6 The function signature should be check_context (docs, context), where docs is lists of docs (one doc represents one row for SQL DBMS and one document for MongoDB). Check severity levels \u00b6 You can label your advisor checks with one of the following available severity levels: Emergency , Alert , Critical , Error , Warning , Notice , Info , Debug . PMM groups failed checks by their severity, and displays them under Advisors Checks > Failed Checks . Check fields \u00b6 Checks can include the following fields: Version (integer, required): defines what other properties are expected, what types are supported, what is expected from the script and what it can expect from the execution environment, etc. Name (string, required): defines machine-readable name (ID). Summary (string, required): defines short human-readable description. Description (string, required): defines long human-readable description. Interval (string/enum, optional): defines running interval. Can be one of the predefined intervals in the UI: Standard, Frequent, Rare. Type (string/enum, required): defines the query type and the PMM Service type for which the advisor runs. Check the list of available types for version 1 checks in the table below. Script (string, required): contains a small Starlark program that processes query results, and returns check results. It is executed on the PMM Server side. Category (string, required): specifies a custom or a default advisor check category. For example: Performance, Security. Query (string, can be absent if the type defines the whole query by itself): The query is executed on the PMM Client side and contains query specific for the target DBMS. Check types \u00b6 Expand the table below for the list of checks types that you can use to define your query type and the PMM Service type for which the check will run. Check Types table Check type Description \u201cquery\u201d required (must be empty if \u201cNo\u201d) MYSQL_SHOW Executes \u2018SHOW \u2026\u2019 clause against MySQL database. Yes MYSQL_SELECT Executes \u2018SELECT \u2026\u2019 clause against MySQL database. Yes POSTGRESQL_SHOW Executes \u2018SHOW ALL\u2019 command against PosgreSQL database. No POSTGRESQL_SELECT Executes \u2018SELECT \u2026\u2019 clause against PosgreSQL database. Yes MONGODB_GETPARAMETER Executes db.adminCommand( { getParameter: \u201c*\u201d } ) against MongoDB\u2019s \u201cadmin\u201d database. For more information, see getParameter No MONGODB_BUILDINFO Executes db.adminCommand( { buildInfo: 1 } ) against MongoDB\u2019s \u201cadmin\u201d database. For more information, see buildInfo No Develop version 1 checks \u00b6 Development / Debugging Only Note that V1 check development in PMM 2.26/2.27 is currently for debugging only and NOT for production use! Future releases plan to include the option to run custom local checks in addition to hosted Percona Platform checks. To develop custom checks for PMM 2.26 and 2.27: Install the latest PMM Server and PMM Client builds following the installation instructions . Run PMM Server with special environment variables: PMM_DEBUG=1 to enable debug output that would be useful later; PERCONA_TEST_CHECKS_FILE=/srv/custom-checks.yml to use checks from the local files instead of downloading them from Percona Platform. PERCONA_TEST_CHECKS_DISABLE_START_DELAY=true to disable the default check execution start delay. This is currently set to one minute, so that checks run upon system start. PERCONA_TEST_CHECKS_RESEND_INTERVAL=2s to define the frequency for sending the SA-based alerts to Alertmanager. docker run -p 80:80 -p 443:443 --name pmm-server \\ -e PMM_DEBUG=1 \\ -e PERCONA_TEST_CHECKS_FILE=/srv/custom-checks.yml \\ -e PERCONA_TEST_CHECKS_DISABLE_START_DELAY=true \\ -e PERCONA_TEST_CHECKS_RESEND_INTERVAL=2s \\ perconalab/pmm-server:dev-latest Log in to Grafana with credentials admin/admin . Go to Configuration > Settings > Advanced Settings and enable the Security Threat Tool option. Create /srv/custom-checks.yml inside the pmm-server container with the content of your check. The checks will run according to the time interval defined on the UI. You can see the result of running the check on the home dashboard: Click on the number of failed checks to open the Failed Checks dashboard: Go into Docker container to output the logs of pmm-managed and read check logs: # get inside the container docker exec -it pmm-server bash # print and watch the logs supervisorctl tail -f pmm-managed","title":"Version 1 checks for PMM 2.27 and older"},{"location":"details/develop-checks/checks-v1.html#version-1-checks-for-pmm-227-and-older","text":"Advisor checks created for PMM 2.27 and older use a slightly different structure than checks for 2.28. This is because, compared to 2.28 checks, 2.27 checks do not support: Multiple queries Victoria Metrics as a data source No database Family field","title":"Version 1 checks for PMM 2.27 and older"},{"location":"details/develop-checks/checks-v1.html#format-for-v1-checks","text":"Checks for PMM 2.27 and older use the following format: Version 1 Checks Format --- checks : - version : 1 name : example summary : Example check description : This check is just an example. type : MONGODB_BUILDINFO script : | def check(docs): # for compatibility with PMM Server < 2.12 context = { \"format_version_num\": format_version_num, \"parse_version\": parse_version, } return check_context(docs, context) def check_context(docs, context): # `docs` is a frozen (deeply immutable) list of dicts where each dict represents a single document in result set. # `context` is a dict with additional functions. # # Global `print` and `fail` functions are available. # # `check_context` function is expected to return a list of dicts that are then converted to alerts; # in particular, that list can be empty. # Any other value (for example, string) is treated as script execution failure # (Starlark does not support Python exceptions); # it is recommended to use global function `fail` for that instead. format_version_num = context.get(\"format_version_num\", fail) parse_version = context.get(\"parse_version\", fail) print(\"first doc =\", repr(docs[0])) return [{ \"summary\": \"Example summary\", \"description\": \"Example description\", \"severity\": \"warning\", \"labels\": { \"version\": format_version_num(10203), } }] Realistic Example of Check in v.1 Format --- checks : - version : 1 name : mongodb_version summary : MongoDB Version description : This check returns warnings if MongoDB/PSMDB version is not the latest one. type : MONGODB_BUILDINFO script : |- LATEST_VERSIONS = { \"mongodb\": { \"3.6\": 30620, # https://docs.mongodb.com/manual/release-notes/3.6/ \"4.0\": 40020, # https://docs.mongodb.com/manual/release-notes/4.0/ \"4.2\": 40210, # https://docs.mongodb.com/manual/release-notes/4.2/ \"4.4\": 40401, # https://docs.mongodb.com/manual/release-notes/4.4/ }, \"percona\": { \"3.6\": 30620, # https://www.percona.com/downloads/percona-server-mongodb-3.6/ \"4.0\": 40020, # https://www.percona.com/downloads/percona-server-mongodb-4.0/ \"4.2\": 40209, # https://www.percona.com/downloads/percona-server-mongodb-4.2/ \"4.4\": 40401, # https://www.percona.com/downloads/percona-server-mongodb-4.4/ }, } def check(docs): # for compatibility with PMM Server < 2.12 context = { \"format_version_num\": format_version_num, \"parse_version\": parse_version, } return check_context(docs, context) def check_context(docs, context): # `docs` is a frozen (deeply immutable) list of dicts where each dict represents a single document in result set. # `context` is a dict with additional functions. # # Global `print` and `fail` functions are available. # # `check_context` function is expected to return a list of dicts that are then converted to alerts; # in particular, that list can be empty. # Any other value (for example, string) is treated as script execution failure # (Starlark does not support Python exceptions); # it is recommended to use global function `fail` for that instead. \"\"\" This check returns warnings if MongoDB/PSMDB version is not the latest one. \"\"\" format_version_num = context.get(\"format_version_num\", fail) parse_version = context.get(\"parse_version\", fail) if len(docs) != 1: return \"Unexpected number of documents\" info = docs[0] # extract information is_percona = 'psmdbVersion' in info # parse_version returns a dict with keys: major, minor, patch, rest, num version = parse_version(info[\"version\"]) print(\"version =\", repr(version)) num = version[\"num\"] mm = \"{}.{}\".format(version[\"major\"], version[\"minor\"]) results = [] if is_percona: latest = LATEST_VERSIONS[\"percona\"][mm] if latest > num: results.append({ \"summary\": \"Newer version of Percona Server for MongoDB is available\", \"description\": \"Current version is {}, latest available version is {}.\".format(format_version_num(num), format_version_num(latest)), \"severity\": \"warning\", \"labels\": { \"current\": format_version_num(num), \"latest\": format_version_num(latest), }, }) return results if True: # MongoDB latest = LATEST_VERSIONS[\"mongodb\"][mm] if latest > num: results.append({ \"summary\": \"Newer version of MongoDB is available\", \"description\": \"Current version is {}, latest available version is {}.\".format(format_version_num(num), format_version_num(latest)), \"severity\": \"warning\", \"labels\": { \"current\": format_version_num(num), \"latest\": format_version_num(latest), }, }) return results","title":"Format for v.1 checks"},{"location":"details/develop-checks/checks-v1.html#security-checks-in-pmm-226-and-older","text":"PMM 2.26 and older included a set of security checks grouped under the Security Threat Tool option. With the 2.27 release, security checks have been renamed to Advisor checks, and the Security Threat Tool option in the PMM Settings was renamed to Advisors .","title":"Security checks in PMM 2.26 and older"},{"location":"details/develop-checks/checks-v1.html#checks-script","text":"The check script assumes that there is a function with a fixed name, that accepts a list of docs containing returned rows for SQL databases and documents for MongoDB. It returns zero, one, or several check results that are then converted to alerts. PMM 2.12.0 and earlier function name is check , while newer versions use name check_context . Both have the same meaning.","title":"Checks script"},{"location":"details/develop-checks/checks-v1.html#function-signature","text":"The function signature should be check_context (docs, context), where docs is lists of docs (one doc represents one row for SQL DBMS and one document for MongoDB).","title":"Function signature"},{"location":"details/develop-checks/checks-v1.html#check-severity-levels","text":"You can label your advisor checks with one of the following available severity levels: Emergency , Alert , Critical , Error , Warning , Notice , Info , Debug . PMM groups failed checks by their severity, and displays them under Advisors Checks > Failed Checks .","title":"Check severity levels"},{"location":"details/develop-checks/checks-v1.html#check-fields","text":"Checks can include the following fields: Version (integer, required): defines what other properties are expected, what types are supported, what is expected from the script and what it can expect from the execution environment, etc. Name (string, required): defines machine-readable name (ID). Summary (string, required): defines short human-readable description. Description (string, required): defines long human-readable description. Interval (string/enum, optional): defines running interval. Can be one of the predefined intervals in the UI: Standard, Frequent, Rare. Type (string/enum, required): defines the query type and the PMM Service type for which the advisor runs. Check the list of available types for version 1 checks in the table below. Script (string, required): contains a small Starlark program that processes query results, and returns check results. It is executed on the PMM Server side. Category (string, required): specifies a custom or a default advisor check category. For example: Performance, Security. Query (string, can be absent if the type defines the whole query by itself): The query is executed on the PMM Client side and contains query specific for the target DBMS.","title":"Check fields"},{"location":"details/develop-checks/checks-v1.html#check-types","text":"Expand the table below for the list of checks types that you can use to define your query type and the PMM Service type for which the check will run. Check Types table Check type Description \u201cquery\u201d required (must be empty if \u201cNo\u201d) MYSQL_SHOW Executes \u2018SHOW \u2026\u2019 clause against MySQL database. Yes MYSQL_SELECT Executes \u2018SELECT \u2026\u2019 clause against MySQL database. Yes POSTGRESQL_SHOW Executes \u2018SHOW ALL\u2019 command against PosgreSQL database. No POSTGRESQL_SELECT Executes \u2018SELECT \u2026\u2019 clause against PosgreSQL database. Yes MONGODB_GETPARAMETER Executes db.adminCommand( { getParameter: \u201c*\u201d } ) against MongoDB\u2019s \u201cadmin\u201d database. For more information, see getParameter No MONGODB_BUILDINFO Executes db.adminCommand( { buildInfo: 1 } ) against MongoDB\u2019s \u201cadmin\u201d database. For more information, see buildInfo No","title":"Check types"},{"location":"details/develop-checks/checks-v1.html#develop-version-1-checks","text":"Development / Debugging Only Note that V1 check development in PMM 2.26/2.27 is currently for debugging only and NOT for production use! Future releases plan to include the option to run custom local checks in addition to hosted Percona Platform checks. To develop custom checks for PMM 2.26 and 2.27: Install the latest PMM Server and PMM Client builds following the installation instructions . Run PMM Server with special environment variables: PMM_DEBUG=1 to enable debug output that would be useful later; PERCONA_TEST_CHECKS_FILE=/srv/custom-checks.yml to use checks from the local files instead of downloading them from Percona Platform. PERCONA_TEST_CHECKS_DISABLE_START_DELAY=true to disable the default check execution start delay. This is currently set to one minute, so that checks run upon system start. PERCONA_TEST_CHECKS_RESEND_INTERVAL=2s to define the frequency for sending the SA-based alerts to Alertmanager. docker run -p 80:80 -p 443:443 --name pmm-server \\ -e PMM_DEBUG=1 \\ -e PERCONA_TEST_CHECKS_FILE=/srv/custom-checks.yml \\ -e PERCONA_TEST_CHECKS_DISABLE_START_DELAY=true \\ -e PERCONA_TEST_CHECKS_RESEND_INTERVAL=2s \\ perconalab/pmm-server:dev-latest Log in to Grafana with credentials admin/admin . Go to Configuration > Settings > Advanced Settings and enable the Security Threat Tool option. Create /srv/custom-checks.yml inside the pmm-server container with the content of your check. The checks will run according to the time interval defined on the UI. You can see the result of running the check on the home dashboard: Click on the number of failed checks to open the Failed Checks dashboard: Go into Docker container to output the logs of pmm-managed and read check logs: # get inside the container docker exec -it pmm-server bash # print and watch the logs supervisorctl tail -f pmm-managed","title":"Develop version 1 checks"},{"location":"details/develop-checks/checks-v2.html","text":"Version 2 advisor checks for PMM 2.28 and newer \u00b6 PMM 2.28 upgraded Advisor Checks to version 2, which uses a slightly different structure than version 1 checks, created in 2.7 and older. This is because, compared to version 1 checks, checks created in 2.28 and later offer additional support for: Multiple queries Victoria Metrics as a data source Database Family field Format for v.2 checks \u00b6 Advisor checks for PMM 2.28 and later use the following format: Version 2 Checks Format --- checks : - version : 2 name : exampleV2 summary : Check format V2 description : Checks something important interval : standard family : MYSQL category : configuration queries : - type : MYSQL_SHOW query : VARIABLES - type : METRICS_INSTANT query : mysql_global_status_uptime{service_name=~\"{{.ServiceName}}\"} - type : METRICS_INSTANT query : mysql_global_status_uptime{service_name=~\"{{.ServiceName}}\"} parameters : lookback : 5m - type : METRICS_RANGE query : avg by (node_name) (avg_over_time(node_load1{node_name=~\"{{.NodeName}}\"}[5m])) parameters : range : 15m step : 5m - type : METRICS_RANGE query : avg by (node_name) (avg_over_time(node_load1{node_name=~\"{{.NodeName}}\"}[5m])) parameters : lookback : 5m range : 15m step : 5m script : | def check_context(docs, context): # `docs` is a frozen (deeply immutable) list where each item represents single query results. Order of results # matches order of queries in check file. Each query result is list of dicts where each item where each dict # represents a single document in result set. # # `context` is a dict with additional functions. # # Global `print` and `fail` functions are available. # # `check_context` function is expected to return a list of dicts that are then converted to alerts; # in particular, that list can be empty. # Any other value (for example, string) is treated as script execution failure # (Starlark does not support Python exceptions); # it is recommended to use global function `fail` for that instead. results = [] for row in docs[0]: name, value = row[\"Variable_name\"], row[\"Value\"] if name == \"version\": results.append({ \"summary\": \"MySQL has version {}\".format(value), \"description\": \"Current version is {}\".format(value), \"read_more_url\": \"\", \"severity\": \"warning\", \"labels\": {}, }) uptimeNow = int(int(docs[1][0][\"value\"][1])/60) results.append({ \"summary\": \"MySQL uptime {} min\".format(uptimeNow), \"description\": \"Current uptime is {} min\".format(uptimeNow), \"read_more_url\": \"\", \"severity\": \"warning\", \"labels\": {}, }) uptimeFiveMinAgo = int(int(docs[2][0][\"value\"][1])/60) results.append({ \"summary\": \"MySQL uptime 5 min ago was {} min\".format(uptimeFiveMinAgo), \"description\": \"5 min ago uptime was {} min\".format(uptimeFiveMinAgo), \"read_more_url\": \"\", \"severity\": \"warning\", \"labels\": {}, }) dataPoints = [] for row in docs[3][0][\"values\"]: dataPoints.append(row[1]) results.append({ \"summary\": \"Node has load average for last 15 minutes {}\".format(dataPoints), \"description\": \"Data points {}\".format(dataPoints), \"read_more_url\": \"\", \"severity\": \"warning\", \"labels\": {}, }) dataPoints = [] for row in docs[4][0][\"values\"]: dataPoints.append(row[1]) results.append({ \"summary\": \"Five minutes ago node had load average for 15 minutes {}\".format(dataPoints), \"description\": \"Data points {}\".format(dataPoints), \"read_more_url\": \"\", \"severity\": \"warning\", \"labels\": {}, }) return results Checks script \u00b6 The check script assumes that there is a function with check_context , that accepts a list where each item represents result of a single query specified in check. Each result itself is a list of docs containing returned rows for SQL databases and documents for MongoDB. It returns zero, one, or several check results that are then converted to alerts. Check severity levels \u00b6 You can label your advisor checks with one of the following available severity levels: Emergency , Alert , Critical , Error , Warning , Notice , Info , Debug . PMM groups failed checks by their severity, and displays them under Advisors Checks > Failed Checks . Check fields \u00b6 Checks can include the following fields: Version (integer, required): defines what other properties are expected, what types are supported, what is expected from the script and what it can expect from the execution environment, etc. Name (string, required): defines machine-readable name (ID). Summary (string, required): defines short human-readable description. Description (string, required): defines long human-readable description. Interval (string/enum, optional): defines running interval. Can be one of the predefined intervals in the UI: Standard, Frequent, Rare. Script (string, required): contains a small Starlark program that processes query results, and returns check results. It is executed on the PMM Server side. Family (string, required): specifies one of the supported database families: MYSQL, POSTGRESQL, MONGODB. This field is only available for Advisor checks v.2, created for PMM 2.28 and later. Category (string, required): specifies a custom or a default advisor check category. For example: Performance, Security. Queries (array, required): contains items that specify queries. Type (string/enum, required): defines the query type. Check the list of available types in the table below. Query (string, can be absent if the type defines the whole query by itself): The query is executed on the PMM Client side and can contain multiple queries specific to the target DBMS. Parameters (key-value, can be absent if query doesn\u2019t have required parameters) Query types \u00b6 Expand the table below for the list of checks types that you can use to define your query type and the PMM Service type for which the check will run. Check Types table Check type Description \u201cquery\u201d required (must be empty if \u201cNo\u201d) MYSQL_SHOW Executes \u2018SHOW \u2026\u2019 clause against MySQL database. Yes MYSQL_SELECT Executes \u2018SELECT \u2026\u2019 clause against MySQL database. Yes POSTGRESQL_SHOW Executes \u2018SHOW ALL\u2019 command against PosgreSQL database. No POSTGRESQL_SELECT Executes \u2018SELECT \u2026\u2019 clause against PosgreSQL database. Yes MONGODB_GETPARAMETER Executes db.adminCommand( { getParameter: \u201c*\u201d } ) against MongoDB\u2019s \u201cadmin\u201d database. For more information, see getParameter No MONGODB_BUILDINFO Executes db.adminCommand( { buildInfo: 1 } ) against MongoDB\u2019s \u201cadmin\u201d database. For more information, see buildInfo No MONGODB_GETCMDLINEOPTS Executes db.adminCommand( { getCmdLineOpts: 1 } ) against MongoDB\u2019s \u201cadmin\u201d database. For more information, see getCmdLineOpts No MONGODB_REPLSETGETSTATUS Executes db.adminCommand( { replSetGetStatus: 1 } ) against MongoDB\u2019s \u201cadmin\u201d database. For more information, see replSetGetStatus No MONGODB_GETDIAGNOSTICDATA Executes db.adminCommand( { getDiagnosticData: 1 } ) against MongoDB\u2019s \u201cadmin\u201d database. For more information, see MongoDB Performance No METRICS_INSTANT Executes instant MetricsQL query. Query can use placeholders in query string {{.NodeName }} and {{}}.ServiceName}} . Both match target service/node names. To read more about instant queries see Prometheus docs . Yes METRICS_RANGE Executes range MetricsQL query. Query can use placeholders in query string {{.NodeName }} and {{}}.ServiceName}} . Both match target service/node names. To read more about range queries see Prometheus docs . Yes Query parameters \u00b6 METRICS_INSTANT lookback (duration, optional): specifies how far in past to look back to metrics history. If this parameter is not specified, then query executed on the latest data. Example values: 30s, 5m, 8h. METRICS_RANGE lookback (duration, optional): specifies how far in past to look back to metrics history. If this parameter is not specified, then query executed on the latest data. Example values: 30s, 5m, 8h. range (duration, required): specifies time window of the query. This parameter is equal to prometheus API . step (duration, required): query resolution. This parameter is equal to prometheus API . Develop version 2 checks \u00b6 Development / Debugging Only Note that V2 check development in PMM 2.28+ is currently for debugging only and NOT for production use! Future releases plan to include the option to run custom local checks in addition to hosted Percona Platform checks. To develop custom checks for PMM 2.28 and later: Install the latest PMM Server and PMM Client builds following the installation instructions . Run PMM Server with special environment variables: PMM_DEBUG=1 to enable debug output that would be useful later; PERCONA_TEST_CHECKS_FILE=/srv/custom-checks.yml to use checks from the local files instead of downloading them from Percona Platform. PERCONA_TEST_CHECKS_DISABLE_START_DELAY=true to disable the default check execution start delay. This is currently set to one minute, so that checks run upon system start. PERCONA_TEST_CHECKS_RESEND_INTERVAL=2s to define the frequency for sending the SA-based alerts to Alertmanager. docker run -p 80:80 -p 443:443 --name pmm-server \\ -e PMM_DEBUG=1 \\ -e PERCONA_TEST_CHECKS_FILE=/srv/custom-checks.yml \\ -e PERCONA_TEST_CHECKS_DISABLE_START_DELAY=true \\ -e PERCONA_TEST_CHECKS_RESEND_INTERVAL=2s \\ perconalab/pmm-server:dev-latest Log in to Grafana with credentials admin/admin . Go to Configuration > Settings > Advanced Settings and make sure the Advisors option is enabled. Create /srv/custom-checks.yml inside the pmm-server container with the content of your check. The checks will run according to the time interval defined on the UI. You can see the result of running the check on the home dashboard: Click on the number of failed checks to open the Failed Checks dashboard: Go into Docker container to output the logs of pmm-managed and read check logs: # get inside the container docker exec -it pmm-server bash # print and watch the logs supervisorctl tail -f pmm-managed","title":"Version 2 advisor checks for PMM 2.28 and newer"},{"location":"details/develop-checks/checks-v2.html#version-2-advisor-checks-for-pmm-228-and-newer","text":"PMM 2.28 upgraded Advisor Checks to version 2, which uses a slightly different structure than version 1 checks, created in 2.7 and older. This is because, compared to version 1 checks, checks created in 2.28 and later offer additional support for: Multiple queries Victoria Metrics as a data source Database Family field","title":"Version 2 advisor checks for PMM 2.28 and newer"},{"location":"details/develop-checks/checks-v2.html#format-for-v2-checks","text":"Advisor checks for PMM 2.28 and later use the following format: Version 2 Checks Format --- checks : - version : 2 name : exampleV2 summary : Check format V2 description : Checks something important interval : standard family : MYSQL category : configuration queries : - type : MYSQL_SHOW query : VARIABLES - type : METRICS_INSTANT query : mysql_global_status_uptime{service_name=~\"{{.ServiceName}}\"} - type : METRICS_INSTANT query : mysql_global_status_uptime{service_name=~\"{{.ServiceName}}\"} parameters : lookback : 5m - type : METRICS_RANGE query : avg by (node_name) (avg_over_time(node_load1{node_name=~\"{{.NodeName}}\"}[5m])) parameters : range : 15m step : 5m - type : METRICS_RANGE query : avg by (node_name) (avg_over_time(node_load1{node_name=~\"{{.NodeName}}\"}[5m])) parameters : lookback : 5m range : 15m step : 5m script : | def check_context(docs, context): # `docs` is a frozen (deeply immutable) list where each item represents single query results. Order of results # matches order of queries in check file. Each query result is list of dicts where each item where each dict # represents a single document in result set. # # `context` is a dict with additional functions. # # Global `print` and `fail` functions are available. # # `check_context` function is expected to return a list of dicts that are then converted to alerts; # in particular, that list can be empty. # Any other value (for example, string) is treated as script execution failure # (Starlark does not support Python exceptions); # it is recommended to use global function `fail` for that instead. results = [] for row in docs[0]: name, value = row[\"Variable_name\"], row[\"Value\"] if name == \"version\": results.append({ \"summary\": \"MySQL has version {}\".format(value), \"description\": \"Current version is {}\".format(value), \"read_more_url\": \"\", \"severity\": \"warning\", \"labels\": {}, }) uptimeNow = int(int(docs[1][0][\"value\"][1])/60) results.append({ \"summary\": \"MySQL uptime {} min\".format(uptimeNow), \"description\": \"Current uptime is {} min\".format(uptimeNow), \"read_more_url\": \"\", \"severity\": \"warning\", \"labels\": {}, }) uptimeFiveMinAgo = int(int(docs[2][0][\"value\"][1])/60) results.append({ \"summary\": \"MySQL uptime 5 min ago was {} min\".format(uptimeFiveMinAgo), \"description\": \"5 min ago uptime was {} min\".format(uptimeFiveMinAgo), \"read_more_url\": \"\", \"severity\": \"warning\", \"labels\": {}, }) dataPoints = [] for row in docs[3][0][\"values\"]: dataPoints.append(row[1]) results.append({ \"summary\": \"Node has load average for last 15 minutes {}\".format(dataPoints), \"description\": \"Data points {}\".format(dataPoints), \"read_more_url\": \"\", \"severity\": \"warning\", \"labels\": {}, }) dataPoints = [] for row in docs[4][0][\"values\"]: dataPoints.append(row[1]) results.append({ \"summary\": \"Five minutes ago node had load average for 15 minutes {}\".format(dataPoints), \"description\": \"Data points {}\".format(dataPoints), \"read_more_url\": \"\", \"severity\": \"warning\", \"labels\": {}, }) return results","title":"Format for v.2 checks"},{"location":"details/develop-checks/checks-v2.html#checks-script","text":"The check script assumes that there is a function with check_context , that accepts a list where each item represents result of a single query specified in check. Each result itself is a list of docs containing returned rows for SQL databases and documents for MongoDB. It returns zero, one, or several check results that are then converted to alerts.","title":"Checks script"},{"location":"details/develop-checks/checks-v2.html#check-severity-levels","text":"You can label your advisor checks with one of the following available severity levels: Emergency , Alert , Critical , Error , Warning , Notice , Info , Debug . PMM groups failed checks by their severity, and displays them under Advisors Checks > Failed Checks .","title":"Check severity levels"},{"location":"details/develop-checks/checks-v2.html#check-fields","text":"Checks can include the following fields: Version (integer, required): defines what other properties are expected, what types are supported, what is expected from the script and what it can expect from the execution environment, etc. Name (string, required): defines machine-readable name (ID). Summary (string, required): defines short human-readable description. Description (string, required): defines long human-readable description. Interval (string/enum, optional): defines running interval. Can be one of the predefined intervals in the UI: Standard, Frequent, Rare. Script (string, required): contains a small Starlark program that processes query results, and returns check results. It is executed on the PMM Server side. Family (string, required): specifies one of the supported database families: MYSQL, POSTGRESQL, MONGODB. This field is only available for Advisor checks v.2, created for PMM 2.28 and later. Category (string, required): specifies a custom or a default advisor check category. For example: Performance, Security. Queries (array, required): contains items that specify queries. Type (string/enum, required): defines the query type. Check the list of available types in the table below. Query (string, can be absent if the type defines the whole query by itself): The query is executed on the PMM Client side and can contain multiple queries specific to the target DBMS. Parameters (key-value, can be absent if query doesn\u2019t have required parameters)","title":"Check fields"},{"location":"details/develop-checks/checks-v2.html#query-types","text":"Expand the table below for the list of checks types that you can use to define your query type and the PMM Service type for which the check will run. Check Types table Check type Description \u201cquery\u201d required (must be empty if \u201cNo\u201d) MYSQL_SHOW Executes \u2018SHOW \u2026\u2019 clause against MySQL database. Yes MYSQL_SELECT Executes \u2018SELECT \u2026\u2019 clause against MySQL database. Yes POSTGRESQL_SHOW Executes \u2018SHOW ALL\u2019 command against PosgreSQL database. No POSTGRESQL_SELECT Executes \u2018SELECT \u2026\u2019 clause against PosgreSQL database. Yes MONGODB_GETPARAMETER Executes db.adminCommand( { getParameter: \u201c*\u201d } ) against MongoDB\u2019s \u201cadmin\u201d database. For more information, see getParameter No MONGODB_BUILDINFO Executes db.adminCommand( { buildInfo: 1 } ) against MongoDB\u2019s \u201cadmin\u201d database. For more information, see buildInfo No MONGODB_GETCMDLINEOPTS Executes db.adminCommand( { getCmdLineOpts: 1 } ) against MongoDB\u2019s \u201cadmin\u201d database. For more information, see getCmdLineOpts No MONGODB_REPLSETGETSTATUS Executes db.adminCommand( { replSetGetStatus: 1 } ) against MongoDB\u2019s \u201cadmin\u201d database. For more information, see replSetGetStatus No MONGODB_GETDIAGNOSTICDATA Executes db.adminCommand( { getDiagnosticData: 1 } ) against MongoDB\u2019s \u201cadmin\u201d database. For more information, see MongoDB Performance No METRICS_INSTANT Executes instant MetricsQL query. Query can use placeholders in query string {{.NodeName }} and {{}}.ServiceName}} . Both match target service/node names. To read more about instant queries see Prometheus docs . Yes METRICS_RANGE Executes range MetricsQL query. Query can use placeholders in query string {{.NodeName }} and {{}}.ServiceName}} . Both match target service/node names. To read more about range queries see Prometheus docs . Yes","title":"Query types"},{"location":"details/develop-checks/checks-v2.html#query-parameters","text":"METRICS_INSTANT lookback (duration, optional): specifies how far in past to look back to metrics history. If this parameter is not specified, then query executed on the latest data. Example values: 30s, 5m, 8h. METRICS_RANGE lookback (duration, optional): specifies how far in past to look back to metrics history. If this parameter is not specified, then query executed on the latest data. Example values: 30s, 5m, 8h. range (duration, required): specifies time window of the query. This parameter is equal to prometheus API . step (duration, required): query resolution. This parameter is equal to prometheus API .","title":"Query parameters"},{"location":"details/develop-checks/checks-v2.html#develop-version-2-checks","text":"Development / Debugging Only Note that V2 check development in PMM 2.28+ is currently for debugging only and NOT for production use! Future releases plan to include the option to run custom local checks in addition to hosted Percona Platform checks. To develop custom checks for PMM 2.28 and later: Install the latest PMM Server and PMM Client builds following the installation instructions . Run PMM Server with special environment variables: PMM_DEBUG=1 to enable debug output that would be useful later; PERCONA_TEST_CHECKS_FILE=/srv/custom-checks.yml to use checks from the local files instead of downloading them from Percona Platform. PERCONA_TEST_CHECKS_DISABLE_START_DELAY=true to disable the default check execution start delay. This is currently set to one minute, so that checks run upon system start. PERCONA_TEST_CHECKS_RESEND_INTERVAL=2s to define the frequency for sending the SA-based alerts to Alertmanager. docker run -p 80:80 -p 443:443 --name pmm-server \\ -e PMM_DEBUG=1 \\ -e PERCONA_TEST_CHECKS_FILE=/srv/custom-checks.yml \\ -e PERCONA_TEST_CHECKS_DISABLE_START_DELAY=true \\ -e PERCONA_TEST_CHECKS_RESEND_INTERVAL=2s \\ perconalab/pmm-server:dev-latest Log in to Grafana with credentials admin/admin . Go to Configuration > Settings > Advanced Settings and make sure the Advisors option is enabled. Create /srv/custom-checks.yml inside the pmm-server container with the content of your check. The checks will run according to the time interval defined on the UI. You can see the result of running the check on the home dashboard: Click on the number of failed checks to open the Failed Checks dashboard: Go into Docker container to output the logs of pmm-managed and read check logs: # get inside the container docker exec -it pmm-server bash # print and watch the logs supervisorctl tail -f pmm-managed","title":"Develop version 2 checks"},{"location":"how-to/index.html","text":"How to \u00b6 Configure via the PMM Settings page. Manage users via the PMM Users page. Upgrade PMM Server via the user interface. Secure your PMM installation. Optimize the performance of your PMM installation. Annotate charts to mark significant events. Share dashboards and panels to save or share. Extend Metrics with textfile collector. Troubleshoot","title":"How to"},{"location":"how-to/index.html#how-to","text":"Configure via the PMM Settings page. Manage users via the PMM Users page. Upgrade PMM Server via the user interface. Secure your PMM installation. Optimize the performance of your PMM installation. Annotate charts to mark significant events. Share dashboards and panels to save or share. Extend Metrics with textfile collector. Troubleshoot","title":"How to"},{"location":"how-to/account-info.html","text":"Check Percona Portal account information \u00b6 When you connect your PMM instances to Percona Platform, PMM gets access to: more alert templates Registered Advisor Checks for additional database checks Paid Advisor Checks for more advanced database health checks. Paid checks are available when you connect to Percona Platform with a customer account. You can check the list of available Paid Advisor checks in the Percona Platform documentation . When you connect with a customer accouny, PMM reveals two new tabs on the main menu, where you can check all the information available for your customer accounts: Entitlements and Support tickets :","title":"Check Percona Portal account information"},{"location":"how-to/account-info.html#check-percona-portal-account-information","text":"When you connect your PMM instances to Percona Platform, PMM gets access to: more alert templates Registered Advisor Checks for additional database checks Paid Advisor Checks for more advanced database health checks. Paid checks are available when you connect to Percona Platform with a customer account. You can check the list of available Paid Advisor checks in the Percona Platform documentation . When you connect with a customer accouny, PMM reveals two new tabs on the main menu, where you can check all the information available for your customer accounts: Entitlements and Support tickets :","title":"Check Percona Portal account information"},{"location":"how-to/advisors.html","text":"Work with Advisor checks \u00b6 Advisors are automated checks that you can run against connected databases to identify any potential security threats, configuration problems, performance concerns, policy non-compliance issues etc. Checks are grouped into advisors according to the functionality and recommendations they provide. Prerequisites for accessing Advisor checks \u00b6 All checks are hosted on Percona Platform. PMM Server automatically downloads them from here when the Advisors and Telemetry options are enabled in PMM under Configuration > Settings > Advanced Settings . Both these options are enabled by default. Advisor check tiers and Platform entitlements \u00b6 Depending on the entitlements available for your Percona Account, the set of advisor checks that PMM can download from the Percona Platform differs in terms of complexity and functionality. If your PMM instance is not connected to Percona Platform, PMM can only download the basic set of Anonymous Advisor checks. As soon as you connect your PMM instance to Percona Platform, has access to additional checks, available only for Registered PMM instances. If you are a Percona customer with a Percona Customer Portal account, you also get access to Paid Advisor checks, which offer more advanced database health information. \u200bTo see the complete list of available checks, see the Advisor Checks for PMM topic in the Percona Platform documentation. Enable/Disable \u00b6 To download the checks available for your Percona Account, the Advisors and Telemetry options have to be enabled under Configuration > Settings > Advanced Settings . These options are enabled by default so that PMM can run automatic advisor checks in the background. However, you can disable them at any time if you do not need to check the health and performance of your connected databases. Automatic checks \u00b6 Advisor checks can be executed manually or automatically. By default, PMM runs all the checks available for your PMM instances every 24 hours. Change run interval for automatic advisors \u00b6 You can change the standard 24-hours interval to a custom frequency for each advisor: Rare interval - 78 hours Standard interval (default) - 24 hours Frequent interval - 4 hours To change the frequency of an automatic advisor: Click Advisors . Select the All tab. Scroll through the list to find a specific check. In PMM 2.29 and later, you can also use the Filter section to search by Name, Description, Status, or Interval. !!! hint alert alert-success \u201cTip\u201d If you need to share filtered advisors results with your team members, send them the PMM URL. This saves your search criteria and results. Select the check and click the Interval icon in the Actions column. Chose an interval and click Save . Manual checks \u00b6 In addition to the automatic checks that run every 24 hours, you can also run checks manually, for ad-hoc assessments of your database health and performance. To manually run all checks or individual ones: Click Advisors on the main menu. Select the All tab. Click Run checks to run all the available advisors at once, or click Run next to each check that you want to run individually. Checks results \u00b6 The results are sent to PMM Server where you can review any failed checks on the Home Dashboard > Failed Advisors Checks panel. The summary count of failed checks is classified as: Critical , which also includes checks tagged as Alert and Emergency Error Warning Notice , which also includes checks tagges as Info and Debug To see more details about the available checks and any checks that failed, click the Advisors icon on the main menu. Check results data always remains on the PMM Server. This is not related to anonymous data sent for Telemetry purposes.","title":"Work with Advisor checks"},{"location":"how-to/advisors.html#work-with-advisor-checks","text":"Advisors are automated checks that you can run against connected databases to identify any potential security threats, configuration problems, performance concerns, policy non-compliance issues etc. Checks are grouped into advisors according to the functionality and recommendations they provide.","title":"Work with Advisor checks"},{"location":"how-to/advisors.html#prerequisites-for-accessing-advisor-checks","text":"All checks are hosted on Percona Platform. PMM Server automatically downloads them from here when the Advisors and Telemetry options are enabled in PMM under Configuration > Settings > Advanced Settings . Both these options are enabled by default.","title":"Prerequisites for accessing Advisor checks"},{"location":"how-to/advisors.html#advisor-check-tiers-and-platform-entitlements","text":"Depending on the entitlements available for your Percona Account, the set of advisor checks that PMM can download from the Percona Platform differs in terms of complexity and functionality. If your PMM instance is not connected to Percona Platform, PMM can only download the basic set of Anonymous Advisor checks. As soon as you connect your PMM instance to Percona Platform, has access to additional checks, available only for Registered PMM instances. If you are a Percona customer with a Percona Customer Portal account, you also get access to Paid Advisor checks, which offer more advanced database health information. \u200bTo see the complete list of available checks, see the Advisor Checks for PMM topic in the Percona Platform documentation.","title":"Advisor check tiers and Platform entitlements"},{"location":"how-to/advisors.html#enabledisable","text":"To download the checks available for your Percona Account, the Advisors and Telemetry options have to be enabled under Configuration > Settings > Advanced Settings . These options are enabled by default so that PMM can run automatic advisor checks in the background. However, you can disable them at any time if you do not need to check the health and performance of your connected databases.","title":"Enable/Disable"},{"location":"how-to/advisors.html#automatic-checks","text":"Advisor checks can be executed manually or automatically. By default, PMM runs all the checks available for your PMM instances every 24 hours.","title":"Automatic checks"},{"location":"how-to/advisors.html#change-run-interval-for-automatic-advisors","text":"You can change the standard 24-hours interval to a custom frequency for each advisor: Rare interval - 78 hours Standard interval (default) - 24 hours Frequent interval - 4 hours To change the frequency of an automatic advisor: Click Advisors . Select the All tab. Scroll through the list to find a specific check. In PMM 2.29 and later, you can also use the Filter section to search by Name, Description, Status, or Interval. !!! hint alert alert-success \u201cTip\u201d If you need to share filtered advisors results with your team members, send them the PMM URL. This saves your search criteria and results. Select the check and click the Interval icon in the Actions column. Chose an interval and click Save .","title":"Change run interval for automatic advisors"},{"location":"how-to/advisors.html#manual-checks","text":"In addition to the automatic checks that run every 24 hours, you can also run checks manually, for ad-hoc assessments of your database health and performance. To manually run all checks or individual ones: Click Advisors on the main menu. Select the All tab. Click Run checks to run all the available advisors at once, or click Run next to each check that you want to run individually.","title":"Manual checks"},{"location":"how-to/advisors.html#checks-results","text":"The results are sent to PMM Server where you can review any failed checks on the Home Dashboard > Failed Advisors Checks panel. The summary count of failed checks is classified as: Critical , which also includes checks tagged as Alert and Emergency Error Warning Notice , which also includes checks tagges as Info and Debug To see more details about the available checks and any checks that failed, click the Advisors icon on the main menu. Check results data always remains on the PMM Server. This is not related to anonymous data sent for Telemetry purposes.","title":"Checks results"},{"location":"how-to/annotate.html","text":"Annotate \u00b6 Annotations mark a moment in time. They are useful for marking system changes or other significant application events. They can be set globally or for specific nodes or services. You create them on the command line with the pmm-admin annotate command. Annotations show as a vertical dashed line on a dashboard graph. Reveal the annotation text by mousing over the caret indicator below the line. You turn annotations on or off with the PMM Annotations switch in the second row menu bar.","title":"Annotate"},{"location":"how-to/annotate.html#annotate","text":"Annotations mark a moment in time. They are useful for marking system changes or other significant application events. They can be set globally or for specific nodes or services. You create them on the command line with the pmm-admin annotate command. Annotations show as a vertical dashed line on a dashboard graph. Reveal the annotation text by mousing over the caret indicator below the line. You turn annotations on or off with the PMM Annotations switch in the second row menu bar.","title":"Annotate"},{"location":"how-to/configure.html","text":"Configure \u00b6 The Settings page is where you configure PMM. Open the Settings page from the main menu with Configuration \u2192 Settings . The page opens with the Metrics Resolution settings tab selected. On the left are the selector tabs: Configure Metrics resolution Advanced Settings Data Retention Telemetry Check for updates Advisors Public address DBaaS Alerting Microsoft Azure Monitoring Public Address {: #public-address-1 } SSH Key Alertmanager integration Percona Platform Connect PMM to Percona Platform Password Reset Password Forgotten Change Password after Login Tip Click Apply changes to save any changes made here. Metrics resolution \u00b6 Metrics are collected at three intervals representing low, medium and high resolutions. The Metrics Resolution settings tab contains a radio button with three fixed presets ( Rare , Standard and Frequent ) and one editable custom preset ( Custom ). Each preset is a group of low, medium and high resolutions. The values are in seconds. Time intervals and resolutions Short time intervals are high resolution metrics. Longer time intervals are low resolution. So: A low resolution interval increases the time between collection, resulting in low-resolution metrics and lower disk usage. A high resolution interval decreases the time between collection, resulting in high-resolution metrics and higher disk usage. The default values (in seconds) for the fixed presets and their resolution names are: Editable? Preset Low Medium High No Rare 300 180 60 No Standard 60 10 5 No Frequent 30 5 1 Yes Custom (defaults) 60 10 5 Values for the Custom preset can be entered as values, or changed with the arrows. If there is poor network connectivity between PMM Server and PMM Client, or between PMM Client and the database server being monitored, scraping every second may not be possible when the network latency is greater than 1 second. Advanced Settings \u00b6 Data Retention \u00b6 Data retention specifies how long data is stored by PMM Server. By default, time-series data is stored for 30 days. You can adjust the data retention time to balance your system\u2019s available disk space with your metrics history requirements. Telemetry \u00b6 The Telemetry switch enables gathering and sending basic anonymous data to Percona, which helps us to determine where to focus the development and what is the uptake for each release of PMM. Specifically, gathering this information helps determine if we need to release patches to legacy versions beyond support, determining when supporting a particular version is no longer necessary, and even understanding how the frequency of release encourages or deters adoption. The following information is gathered: PMM Server Integration Alerting feature enabled/disabled PMM Server Security Thread Tool feature enabled/disabled PMM Server Backup feature enabled/disabled PMM Server DBaaS feature enabled/disabled PMM Server Check Updates feature disabled Detailed information about the version of monitored MySQL services Monitored MongoDB services version Monitored PostgreSQL services version Total Grafana users Monitored nodes count Monitored services count Agents version Node type We do not gather anything that identify a system, but the following two points should be mentioned: The Country Code is evaluated from the submitting IP address before being discarded. We do create an \u201cinstance ID\u201d - a random string generated using UUID v4. This instance ID is generated to distinguish new instances from existing ones, for figuring out instance upgrades. The first telemetry reporting of a new PMM Server instance is delayed by 24 hours to allow enough time to disable the service for those that do not wish to share any information. The landing page for this service, check.percona.com , explains what this service is. Grafana\u2019s anonymous usage statistics is not managed by PMM. To activate it, you must change the PMM Server container configuration after each update. As well as via the PMM Settings page, you can also disable telemetry with the -e DISABLE_TELEMETRY=1 option in your docker run statement for the PMM Server. Telemetry is sent straight away; the 24 hour grace period is not honored. Check for updates \u00b6 When active, PMM will automatically check for updates and put a notification in the home page Updates dashboard if any are available. Advisors \u00b6 Advisors are sets of checks grouped by functionality that run a range of database health checks on a registered instance. The findings are reported on the Advisors > Failed Checks page, and an overview is displayed on the Dashboard in the Failed Advisor Checks panel. The Advisors option is enabled by default. Checks are refetched and rerun at intervals. See Working with Advisor checks . Public address \u00b6 The address or hostname PMM Server will be accessible at. Click Get from browser to have your browser detect and populate this field automatically. DBaaS \u00b6 Caution DBaaS functionality is a technical preview that must be turned on with a server feature flag. See DBaaS . Enables/disables DBaaS features on this server. Important Deactivating DBaaS does not suspend or remove running DB clusters. Alerting \u00b6 Enables Percona Alerting and reveals the Percona templated alerts option on the Alerting page. Microsoft Azure Monitoring \u00b6 Caution This is a technical preview feature. Activates Microsoft Azure monitoring. Public Address \u00b6 Public address for accessing DBaaS features on this server. SSH Key \u00b6 This section lets you upload your public SSH key to access the PMM Server via SSH (for example, when accessing PMM Server as a virtual appliance ). Enter your public key in the SSH Key field and click Apply SSH Key . Alertmanager integration \u00b6 Alertmanager manages alerts, de-duplicating, grouping, and routing them to the appropriate receiver or display component. This section lets you configure integration of VictoriaMetrics with an external Alertmanager. Tip If possible, use Integrated Alerting instead of Alertmanager. The Alertmanager URL field should contain the URL of the Alertmanager which would serve your PMM alerts. The Prometheus Alerting rules field is used to specify alerting rules in the YAML configuration format. Fill both fields and click the Apply Alertmanager settings button to proceed. Percona Platform \u00b6 This panel is where you connect your PMM server to your Percona Platform Account. Your Percona Platform Account is separate from your PMM User account. Connect PMM to Percona Platform \u00b6 To learn how to connect your PMM servers to Percona Platform and leverage Platform services that boost the monitoring capabilities of your PMM installations, see Integrate PMM with Percona Platform . Password Reset \u00b6 Password Forgotten \u00b6 In case you forgot your password, click on the Forgot password link on the login page. You will be redirected to a password reset page. Enter the email you are registered with in the field and click on Reset via Email . An email with a link to reset your password will be sent to you. Change Password after Login \u00b6 If you did not forget your password but you still want to change it, go to https://okta.percona.com/enduser/settings (make sure you are logged in). Insert you current password and the new password in the form to the bottom right of the page. If you cannot see the form, you will need to click on the Edit Profile green button (you will be prompted for you password). Click on Change Password . If everything goes well, you will see a confirmation message.","title":"Configure"},{"location":"how-to/configure.html#configure","text":"The Settings page is where you configure PMM. Open the Settings page from the main menu with Configuration \u2192 Settings . The page opens with the Metrics Resolution settings tab selected. On the left are the selector tabs: Configure Metrics resolution Advanced Settings Data Retention Telemetry Check for updates Advisors Public address DBaaS Alerting Microsoft Azure Monitoring Public Address {: #public-address-1 } SSH Key Alertmanager integration Percona Platform Connect PMM to Percona Platform Password Reset Password Forgotten Change Password after Login Tip Click Apply changes to save any changes made here.","title":"Configure"},{"location":"how-to/configure.html#metrics-resolution","text":"Metrics are collected at three intervals representing low, medium and high resolutions. The Metrics Resolution settings tab contains a radio button with three fixed presets ( Rare , Standard and Frequent ) and one editable custom preset ( Custom ). Each preset is a group of low, medium and high resolutions. The values are in seconds. Time intervals and resolutions Short time intervals are high resolution metrics. Longer time intervals are low resolution. So: A low resolution interval increases the time between collection, resulting in low-resolution metrics and lower disk usage. A high resolution interval decreases the time between collection, resulting in high-resolution metrics and higher disk usage. The default values (in seconds) for the fixed presets and their resolution names are: Editable? Preset Low Medium High No Rare 300 180 60 No Standard 60 10 5 No Frequent 30 5 1 Yes Custom (defaults) 60 10 5 Values for the Custom preset can be entered as values, or changed with the arrows. If there is poor network connectivity between PMM Server and PMM Client, or between PMM Client and the database server being monitored, scraping every second may not be possible when the network latency is greater than 1 second.","title":"Metrics resolution"},{"location":"how-to/configure.html#advanced-settings","text":"","title":"Advanced Settings"},{"location":"how-to/configure.html#data-retention","text":"Data retention specifies how long data is stored by PMM Server. By default, time-series data is stored for 30 days. You can adjust the data retention time to balance your system\u2019s available disk space with your metrics history requirements.","title":"Data Retention"},{"location":"how-to/configure.html#telemetry","text":"The Telemetry switch enables gathering and sending basic anonymous data to Percona, which helps us to determine where to focus the development and what is the uptake for each release of PMM. Specifically, gathering this information helps determine if we need to release patches to legacy versions beyond support, determining when supporting a particular version is no longer necessary, and even understanding how the frequency of release encourages or deters adoption. The following information is gathered: PMM Server Integration Alerting feature enabled/disabled PMM Server Security Thread Tool feature enabled/disabled PMM Server Backup feature enabled/disabled PMM Server DBaaS feature enabled/disabled PMM Server Check Updates feature disabled Detailed information about the version of monitored MySQL services Monitored MongoDB services version Monitored PostgreSQL services version Total Grafana users Monitored nodes count Monitored services count Agents version Node type We do not gather anything that identify a system, but the following two points should be mentioned: The Country Code is evaluated from the submitting IP address before being discarded. We do create an \u201cinstance ID\u201d - a random string generated using UUID v4. This instance ID is generated to distinguish new instances from existing ones, for figuring out instance upgrades. The first telemetry reporting of a new PMM Server instance is delayed by 24 hours to allow enough time to disable the service for those that do not wish to share any information. The landing page for this service, check.percona.com , explains what this service is. Grafana\u2019s anonymous usage statistics is not managed by PMM. To activate it, you must change the PMM Server container configuration after each update. As well as via the PMM Settings page, you can also disable telemetry with the -e DISABLE_TELEMETRY=1 option in your docker run statement for the PMM Server. Telemetry is sent straight away; the 24 hour grace period is not honored.","title":"Telemetry"},{"location":"how-to/configure.html#check-for-updates","text":"When active, PMM will automatically check for updates and put a notification in the home page Updates dashboard if any are available.","title":"Check for updates"},{"location":"how-to/configure.html#advisors","text":"Advisors are sets of checks grouped by functionality that run a range of database health checks on a registered instance. The findings are reported on the Advisors > Failed Checks page, and an overview is displayed on the Dashboard in the Failed Advisor Checks panel. The Advisors option is enabled by default. Checks are refetched and rerun at intervals. See Working with Advisor checks .","title":"Advisors"},{"location":"how-to/configure.html#public-address","text":"The address or hostname PMM Server will be accessible at. Click Get from browser to have your browser detect and populate this field automatically.","title":"Public address"},{"location":"how-to/configure.html#dbaas","text":"Caution DBaaS functionality is a technical preview that must be turned on with a server feature flag. See DBaaS . Enables/disables DBaaS features on this server. Important Deactivating DBaaS does not suspend or remove running DB clusters.","title":"DBaaS"},{"location":"how-to/configure.html#alerting","text":"Enables Percona Alerting and reveals the Percona templated alerts option on the Alerting page.","title":"Alerting"},{"location":"how-to/configure.html#microsoft-azure-monitoring","text":"Caution This is a technical preview feature. Activates Microsoft Azure monitoring.","title":"Microsoft Azure Monitoring"},{"location":"how-to/configure.html#public-address-1","text":"Public address for accessing DBaaS features on this server.","title":"Public Address"},{"location":"how-to/configure.html#ssh-key","text":"This section lets you upload your public SSH key to access the PMM Server via SSH (for example, when accessing PMM Server as a virtual appliance ). Enter your public key in the SSH Key field and click Apply SSH Key .","title":"SSH Key"},{"location":"how-to/configure.html#alertmanager-integration","text":"Alertmanager manages alerts, de-duplicating, grouping, and routing them to the appropriate receiver or display component. This section lets you configure integration of VictoriaMetrics with an external Alertmanager. Tip If possible, use Integrated Alerting instead of Alertmanager. The Alertmanager URL field should contain the URL of the Alertmanager which would serve your PMM alerts. The Prometheus Alerting rules field is used to specify alerting rules in the YAML configuration format. Fill both fields and click the Apply Alertmanager settings button to proceed.","title":"Alertmanager integration"},{"location":"how-to/configure.html#percona-platform","text":"This panel is where you connect your PMM server to your Percona Platform Account. Your Percona Platform Account is separate from your PMM User account.","title":"Percona Platform"},{"location":"how-to/configure.html#connect-pmm-to-percona-platform","text":"To learn how to connect your PMM servers to Percona Platform and leverage Platform services that boost the monitoring capabilities of your PMM installations, see Integrate PMM with Percona Platform .","title":"Connect PMM to Percona Platform"},{"location":"how-to/configure.html#password-reset","text":"","title":"Password Reset"},{"location":"how-to/configure.html#password-forgotten","text":"In case you forgot your password, click on the Forgot password link on the login page. You will be redirected to a password reset page. Enter the email you are registered with in the field and click on Reset via Email . An email with a link to reset your password will be sent to you.","title":"Password Forgotten"},{"location":"how-to/configure.html#change-password-after-login","text":"If you did not forget your password but you still want to change it, go to https://okta.percona.com/enduser/settings (make sure you are logged in). Insert you current password and the new password in the form to the bottom right of the page. If you cannot see the form, you will need to click on the Edit Profile green button (you will be prompted for you password). Click on Change Password . If everything goes well, you will see a confirmation message.","title":"Change Password after Login"},{"location":"how-to/extend-metrics.html","text":"Extend Metrics \u00b6 When you need a metric that\u2019s not present in the default list of node_exporter metrics you may be able to use the textfile collector. The textfile collector allows exporting of statistics from batch jobs. It can also be used to export static metrics, such as what role a machine has. Enable the textfile collector \u00b6 The collector is enabled by default. The following folders are used for different resolutions: Resolution Folder High /usr/local/percona/pmm2/collectors/textfile-collector/high-resolution Medium /usr/local/percona/pmm2/collectors/textfile-collector/medium-resolution Low /usr/local/percona/pmm2/collectors/textfile-collector/low-resolution The exporter parses all files in these directories that match the filename wildcard expression *.prom using a simple text-based exposition format . Metrics are stored on the PMM Server-side with additional labels related to this Node. Examples of shell commands for custom metrics \u00b6 To statically set roles for a machine using labels: echo 'node_role{role=\"my_monitored_server_1\"} 1' > /usr/local/percona/pmm2/collectors/textfile-collector/low-resolution/node_role.prom Here\u2019s an example of a cron job that automatically pushes logged-in users: $ cat /etc/cron.d/loggedin_users */1 * * * * root /usr/bin/who | /usr/bin/wc -l | sed -ne 's/^/node_loggedin_users /p' > /usr/local/percona/pmm2/collectors/textfile-collector/high-resolution/node_users.prom","title":"Extend Metrics"},{"location":"how-to/extend-metrics.html#extend-metrics","text":"When you need a metric that\u2019s not present in the default list of node_exporter metrics you may be able to use the textfile collector. The textfile collector allows exporting of statistics from batch jobs. It can also be used to export static metrics, such as what role a machine has.","title":"Extend Metrics"},{"location":"how-to/extend-metrics.html#enable-the-textfile-collector","text":"The collector is enabled by default. The following folders are used for different resolutions: Resolution Folder High /usr/local/percona/pmm2/collectors/textfile-collector/high-resolution Medium /usr/local/percona/pmm2/collectors/textfile-collector/medium-resolution Low /usr/local/percona/pmm2/collectors/textfile-collector/low-resolution The exporter parses all files in these directories that match the filename wildcard expression *.prom using a simple text-based exposition format . Metrics are stored on the PMM Server-side with additional labels related to this Node.","title":"Enable the textfile collector"},{"location":"how-to/extend-metrics.html#examples-of-shell-commands-for-custom-metrics","text":"To statically set roles for a machine using labels: echo 'node_role{role=\"my_monitored_server_1\"} 1' > /usr/local/percona/pmm2/collectors/textfile-collector/low-resolution/node_role.prom Here\u2019s an example of a cron job that automatically pushes logged-in users: $ cat /etc/cron.d/loggedin_users */1 * * * * root /usr/bin/who | /usr/bin/wc -l | sed -ne 's/^/node_loggedin_users /p' > /usr/local/percona/pmm2/collectors/textfile-collector/high-resolution/node_users.prom","title":"Examples of shell commands for custom metrics"},{"location":"how-to/integrate-platform.html","text":"Integrate PMM with Percona Platform \u00b6 Percona Platform brings together database distributions, support expertise, services, management, and automated insights. Connect your PMM servers to Percona Platform to boost the monitoring capabilities of your PMM installations and manage database deployments easier. In addition, you get access to PMM updates, automated insights, advanced advisor checks and more alert rule templates. Connect PMM to Percona Platform \u00b6 You can connect to Percona Platform with a Percona Account or via Google or GitHub authentication. If Percona Support has enabled a custom identity provider for your account, you can also log in using your company\u2019s credentials. We recommend that you connect with a Percona Account, as this gives you access to other Percona services, including Percona Platform, Percona Customer Portal, and Community Forum. If you don\u2019t have a Percona Account, you can create one on the Percona Platform homepage using the Don\u2019t have an account? Create one? link. Pre-requisites \u00b6 To ensure that PMM can establish a connection to Percona Platform: Upgrade to PMM 2.27.0 or later \u00b6 Before connecting your PMM server to Percona Platform, make sure you are using PMM version 2.27 or newer. Otherwise, upgrade your PMM installation beforehand. This is required because, starting with PMM 2.27, Percona Platform has replaced username/password authentication with access token authentication. Access-token authentication increases security and enables federated identity. This change did not affect existing connections to PMM Platform, which were not automatically terminated. For more information, see Install and set up PMM . Check that you are a member of an existing Platform organization \u00b6 Log in to Percona Platform using your Percona Account. If you are connecting via GitHub, make sure you set your email address as public in your GitHub account. If your email address is private instead, Percona Platform cannot access it to authenticate you. On the Getting Started page, check that the Create organization step shows an option to view your organization. Contact your account administrator or create a new organization for your Percona Account if this is the case. Set the public address of your PMM server \u00b6 PMM automatically detects and populates the public address of the PMM server when this is not set up. If you need to set it differently, go to Settings > Advanced Settings and edit the Public Address field. Connect PMM to Percona Platform \u00b6 To connect your PMM server to Percona Platform, copy your personal access token from Platform Portal and paste it into PMM. You will find your access token in Platform Portal as part of your user profile page. Token validity \u00b6 For security reasons, access tokens expire after 30 minutes. Make sure to paste the code before that, or generate a new one if it expires. To connect your PMM server to Percona Platform: 1. In PMM, go to Settings > Percona Platform tab to fill in the Connect PMM to Percona Portal form: The PMM server ID field is automatically populated with the ID identified for your PMM instance. Enter the name of your PMM instance and click Get token to go to Percona Platform Portal and generate your access token. Log into Percona Platform using your Percona Account (if you don\u2019t have an active current session). On the Profile Settings page , copy the code from the Percona Platform Access Token field. Back into PMM, paste the Access Token into the Percona Platform Access Token field, and click Connect . To confirm that you have successfully connected the server and check the list of all servers currently connected to an organization, go to Percona Platform > Dashboard tab and click View Instances next to the Connect your PMM step. Check Percona Portal entitlements \u00b6 After connecting to the Percona Platform, PMM has access to additional alert templates, Advisors checks, and account information. See (../how-to/account-info.md) Disconnect a PMM instance \u00b6 Disconnect a PMM instance when you want to unlink it from your Percona Platform organization or stop monitoring it there. To disconnect a PMM server, go to > Configuration > Settings > Percona Platform and click Disconnect . Disconnecting instances as an Admin \u00b6 In situations where you are not able to disconnect servers yourself, ask your PMM Admin to disconnect the server for you. For example, you may not be able to disconnect servers when PMM is moved to a network segment without outbound connections to public networks. Availability This feature is available starting with PMM 2.29.0. If you cannot disconnect servers yourself, ask your PMM Admin to disconnect the server for you. For example, you may not be able to disconnect servers when PMM is moved to a network segment without outbound connections to public networks. If you are a PMM Admin, you can terminate any connections to Percona Platform, even if you are not logged into PMM with a Percona Account. However, we recommend logging in with a Percona Account before disconnecting servers, as this will automatically remove the disconnected servers from Percona Platform as well. If you do disconnect servers without being connected with a Percona Account, you\u2019ll have to manually remove the unavailable servers from Percona Platform. This ensures that your list of connected PMM instances stays up-to-date in Percona Platform. To do this, go to PMM instances , and remove any servers that you have already disconnected from PMM. Sign into PMM with your Percona Account \u00b6 Once you\u2019ve successfully connected your PMM instance to the Percona Platform, you can also sign into PMM using your Percona Account: Log out of your existing PMM session. On the PMM login screen, click Sign in with Percona Account . If you have an active Percona Account session on the same browser, PMM will log you in automatically. Otherwise, enter your Percona Account credentials to start a new session.","title":"Integrate with Percona Platform"},{"location":"how-to/integrate-platform.html#integrate-pmm-with-percona-platform","text":"Percona Platform brings together database distributions, support expertise, services, management, and automated insights. Connect your PMM servers to Percona Platform to boost the monitoring capabilities of your PMM installations and manage database deployments easier. In addition, you get access to PMM updates, automated insights, advanced advisor checks and more alert rule templates.","title":"Integrate PMM with Percona Platform"},{"location":"how-to/integrate-platform.html#connect-pmm-to-percona-platform","text":"You can connect to Percona Platform with a Percona Account or via Google or GitHub authentication. If Percona Support has enabled a custom identity provider for your account, you can also log in using your company\u2019s credentials. We recommend that you connect with a Percona Account, as this gives you access to other Percona services, including Percona Platform, Percona Customer Portal, and Community Forum. If you don\u2019t have a Percona Account, you can create one on the Percona Platform homepage using the Don\u2019t have an account? Create one? link.","title":"Connect PMM to Percona Platform"},{"location":"how-to/integrate-platform.html#pre-requisites","text":"To ensure that PMM can establish a connection to Percona Platform:","title":"Pre-requisites"},{"location":"how-to/integrate-platform.html#upgrade-to-pmm-2270-or-later","text":"Before connecting your PMM server to Percona Platform, make sure you are using PMM version 2.27 or newer. Otherwise, upgrade your PMM installation beforehand. This is required because, starting with PMM 2.27, Percona Platform has replaced username/password authentication with access token authentication. Access-token authentication increases security and enables federated identity. This change did not affect existing connections to PMM Platform, which were not automatically terminated. For more information, see Install and set up PMM .","title":"Upgrade to PMM 2.27.0 or later"},{"location":"how-to/integrate-platform.html#check-that-you-are-a-member-of-an-existing-platform-organization","text":"Log in to Percona Platform using your Percona Account. If you are connecting via GitHub, make sure you set your email address as public in your GitHub account. If your email address is private instead, Percona Platform cannot access it to authenticate you. On the Getting Started page, check that the Create organization step shows an option to view your organization. Contact your account administrator or create a new organization for your Percona Account if this is the case.","title":"Check that you are a member of an existing Platform organization"},{"location":"how-to/integrate-platform.html#set-the-public-address-of-your-pmm-server","text":"PMM automatically detects and populates the public address of the PMM server when this is not set up. If you need to set it differently, go to Settings > Advanced Settings and edit the Public Address field.","title":"Set the public address of your PMM server"},{"location":"how-to/integrate-platform.html#connect-pmm-to-percona-platform_1","text":"To connect your PMM server to Percona Platform, copy your personal access token from Platform Portal and paste it into PMM. You will find your access token in Platform Portal as part of your user profile page.","title":"Connect PMM to Percona Platform"},{"location":"how-to/integrate-platform.html#token-validity","text":"For security reasons, access tokens expire after 30 minutes. Make sure to paste the code before that, or generate a new one if it expires. To connect your PMM server to Percona Platform: 1. In PMM, go to Settings > Percona Platform tab to fill in the Connect PMM to Percona Portal form: The PMM server ID field is automatically populated with the ID identified for your PMM instance. Enter the name of your PMM instance and click Get token to go to Percona Platform Portal and generate your access token. Log into Percona Platform using your Percona Account (if you don\u2019t have an active current session). On the Profile Settings page , copy the code from the Percona Platform Access Token field. Back into PMM, paste the Access Token into the Percona Platform Access Token field, and click Connect . To confirm that you have successfully connected the server and check the list of all servers currently connected to an organization, go to Percona Platform > Dashboard tab and click View Instances next to the Connect your PMM step.","title":"Token validity"},{"location":"how-to/integrate-platform.html#check-percona-portal-entitlements","text":"After connecting to the Percona Platform, PMM has access to additional alert templates, Advisors checks, and account information. See (../how-to/account-info.md)","title":"Check Percona Portal entitlements"},{"location":"how-to/integrate-platform.html#disconnect-a-pmm-instance","text":"Disconnect a PMM instance when you want to unlink it from your Percona Platform organization or stop monitoring it there. To disconnect a PMM server, go to > Configuration > Settings > Percona Platform and click Disconnect .","title":"Disconnect a PMM instance"},{"location":"how-to/integrate-platform.html#disconnecting-instances-as-an-admin","text":"In situations where you are not able to disconnect servers yourself, ask your PMM Admin to disconnect the server for you. For example, you may not be able to disconnect servers when PMM is moved to a network segment without outbound connections to public networks. Availability This feature is available starting with PMM 2.29.0. If you cannot disconnect servers yourself, ask your PMM Admin to disconnect the server for you. For example, you may not be able to disconnect servers when PMM is moved to a network segment without outbound connections to public networks. If you are a PMM Admin, you can terminate any connections to Percona Platform, even if you are not logged into PMM with a Percona Account. However, we recommend logging in with a Percona Account before disconnecting servers, as this will automatically remove the disconnected servers from Percona Platform as well. If you do disconnect servers without being connected with a Percona Account, you\u2019ll have to manually remove the unavailable servers from Percona Platform. This ensures that your list of connected PMM instances stays up-to-date in Percona Platform. To do this, go to PMM instances , and remove any servers that you have already disconnected from PMM.","title":"Disconnecting instances as an Admin"},{"location":"how-to/integrate-platform.html#sign-into-pmm-with-your-percona-account","text":"Once you\u2019ve successfully connected your PMM instance to the Percona Platform, you can also sign into PMM using your Percona Account: Log out of your existing PMM session. On the PMM login screen, click Sign in with Percona Account . If you have an active Percona Account session on the same browser, PMM will log you in automatically. Otherwise, enter your Percona Account credentials to start a new session.","title":"Sign into PMM with your Percona Account"},{"location":"how-to/manage-users.html","text":"Manage users \u00b6 This topic explains user management in PMM. You can manage users from the main menu by navigating to Server Admin \u2192 Users page. Add users \u00b6 You can add a user in PMM from User \u2192 New user tab. To add a new user in PMM: On the Users tab, click New user . On the Add new user dialog box, enter the following: Name email address or username (if this is an existing grafana user) Username Password Click create user . Edit users \u00b6 You can edit users by changing the information or settings for an individual user account. Important After changing the default admin password for the PMM server, register the pmm-agent using the same credentials and add the services again. Otherwise, PMM will cease to monitor the service/nodes. Grant or Revoke admin privileges \u00b6 You can grant or revoke admin access to a user as follows: On the Users tab, click the user account you want to edit. To grant or revoke the privileges, click the user. User information dialog box opens. In the Permissions section, click Change and then select Yes/No , depending on whether you want to provide admin access or not. Click Change . Important After connecting your PMM instance to the Percona Platform, when you log in using your Percona account, you will be granted the Viewer access. For Admin access, log in to PMM as an admin, and change the permissions for this user. Change organization role \u00b6 You can change the organization role assigned to your user account. To change the role: On the Users tab, click the user for whom you want to change the role. In the Organisations section, click Change role . Select the role from the dropdown and click save . The following are the privileges for the various roles: Admin - Managing data sources, teams, and users within an organization. Editor - Creating and editing dashboards. Viewer - Viewing dashboards. For detailed information on the privileges for these roles and the different tasks that they can perform, refer to: Grafana organization roles . Delete Users \u00b6 You can delete a user in PMM as follows: On the User tab, click the user you want to delete. Click Delete user .","title":"Manage users"},{"location":"how-to/manage-users.html#manage-users","text":"This topic explains user management in PMM. You can manage users from the main menu by navigating to Server Admin \u2192 Users page.","title":"Manage users"},{"location":"how-to/manage-users.html#add-users","text":"You can add a user in PMM from User \u2192 New user tab. To add a new user in PMM: On the Users tab, click New user . On the Add new user dialog box, enter the following: Name email address or username (if this is an existing grafana user) Username Password Click create user .","title":"Add users"},{"location":"how-to/manage-users.html#edit-users","text":"You can edit users by changing the information or settings for an individual user account. Important After changing the default admin password for the PMM server, register the pmm-agent using the same credentials and add the services again. Otherwise, PMM will cease to monitor the service/nodes.","title":"Edit users"},{"location":"how-to/manage-users.html#grant-or-revoke-admin-privileges","text":"You can grant or revoke admin access to a user as follows: On the Users tab, click the user account you want to edit. To grant or revoke the privileges, click the user. User information dialog box opens. In the Permissions section, click Change and then select Yes/No , depending on whether you want to provide admin access or not. Click Change . Important After connecting your PMM instance to the Percona Platform, when you log in using your Percona account, you will be granted the Viewer access. For Admin access, log in to PMM as an admin, and change the permissions for this user.","title":"Grant or Revoke admin privileges"},{"location":"how-to/manage-users.html#change-organization-role","text":"You can change the organization role assigned to your user account. To change the role: On the Users tab, click the user for whom you want to change the role. In the Organisations section, click Change role . Select the role from the dropdown and click save . The following are the privileges for the various roles: Admin - Managing data sources, teams, and users within an organization. Editor - Creating and editing dashboards. Viewer - Viewing dashboards. For detailed information on the privileges for these roles and the different tasks that they can perform, refer to: Grafana organization roles .","title":"Change organization role"},{"location":"how-to/manage-users.html#delete-users","text":"You can delete a user in PMM as follows: On the User tab, click the user you want to delete. Click Delete user .","title":"Delete Users"},{"location":"how-to/optimize.html","text":"Optimize \u00b6 Improving PMM Performance with Table Statistics Options \u00b6 If a MySQL instance has a lot of schemas or tables, there are two options to help improve the performance of PMM when adding instances with pmm-admin add : --disable-tablestats , or, --disable-tablestats-limit . Important These settings are only for adding an instance. To change them, you must remove and re-add the instances. Only one of these options can be used when adding an instance. Disable per-table statistics for an instance \u00b6 When adding an instance with pmm-admin add , the --disable-tablestats option disables table statistics collection when there are more than the default number (1000) of tables in the instance. USAGE \u00b6 pmm-admin add mysql --disable-tablestats Change the number of tables beyond which per-table statistics is disabled \u00b6 When adding an instance with pmm-admin add , the --disable-tablestats-limit option changes the number of tables (from the default of 1000) beyond which per-table statistics collection is disabled. USAGE \u00b6 pmm-admin add mysql --disable-tablestats-limit = <LIMIT> EXAMPLE \u00b6 Add a MySQL instance, disabling per-table statistics collection when the number of tables in the instance reaches 2000. pmm-admin add mysql --disable-tablestats-limit = 2000","title":"Optimize"},{"location":"how-to/optimize.html#optimize","text":"","title":"Optimize"},{"location":"how-to/optimize.html#improving-pmm-performance-with-table-statistics-options","text":"If a MySQL instance has a lot of schemas or tables, there are two options to help improve the performance of PMM when adding instances with pmm-admin add : --disable-tablestats , or, --disable-tablestats-limit . Important These settings are only for adding an instance. To change them, you must remove and re-add the instances. Only one of these options can be used when adding an instance.","title":"Improving PMM Performance with Table Statistics Options"},{"location":"how-to/optimize.html#disable-per-table-statistics-for-an-instance","text":"When adding an instance with pmm-admin add , the --disable-tablestats option disables table statistics collection when there are more than the default number (1000) of tables in the instance.","title":"Disable per-table statistics for an instance"},{"location":"how-to/optimize.html#usage","text":"pmm-admin add mysql --disable-tablestats","title":"USAGE"},{"location":"how-to/optimize.html#change-the-number-of-tables-beyond-which-per-table-statistics-is-disabled","text":"When adding an instance with pmm-admin add , the --disable-tablestats-limit option changes the number of tables (from the default of 1000) beyond which per-table statistics collection is disabled.","title":"Change the number of tables beyond which per-table statistics is disabled"},{"location":"how-to/optimize.html#usage_1","text":"pmm-admin add mysql --disable-tablestats-limit = <LIMIT>","title":"USAGE"},{"location":"how-to/optimize.html#example","text":"Add a MySQL instance, disabling per-table statistics collection when the number of tables in the instance reaches 2000. pmm-admin add mysql --disable-tablestats-limit = 2000","title":"EXAMPLE"},{"location":"how-to/secure.html","text":"Secure \u00b6 You can improve the security of your PMM installation with: SSL encryption to secure traffic between client and server; Grafana HTTPS secure cookies To see which security features are enabled: pmm-admin status Tip You can gain an extra level of security by keeping PMM Server isolated from the internet, if possible. SSL encryption \u00b6 You need valid SSL certificates to encrypt traffic between client and server. With our Docker, OVF and AMI images, self-signed certificates are in /srv/nginx . To use your own, you can either: mount the local certificate directory to the same location, or, copy your certificates to a running PMM Server container. Mounting certificates \u00b6 For example, if your own certificates are in /etc/pmm-certs : docker run -d -p 443 :443 --volumes-from pmm-data \\ --name pmm-server -v /etc/pmm-certs:/srv/nginx \\ --restart always percona/pmm-server:2 The certificates must be owned by root. You can do this with: chown 0:0 /etc/pmm-certs/* The mounted certificate directory ( /etc/pmm-certs in this example) must contain the files certificate.crt , certificate.key , ca-certs.pem and dhparam.pem . For SSL encryption, the container must publish on port 443 instead of 80. Copying certificates \u00b6 If PMM Server is running as a Docker image, use docker cp to copy certificates. This example copies certificate files from the current working directory to a running PMM Server docker container. docker cp certificate.crt pmm-server:/srv/nginx/certificate.crt docker cp certificate.key pmm-server:/srv/nginx/certificate.key docker cp ca-certs.pem pmm-server:/srv/nginx/ca-certs.pem docker cp dhparam.pem pmm-server:/srv/nginx/dhparam.pem Enabling SSL when connecting PMM Client to PMM Server \u00b6 pmm-admin config --server-url = https://<user>:<password>@<server IP> Grafana HTTPS secure cookies \u00b6 To enable: Start a shell within the Docker container. docker exec -it pmm-server bash Edit /etc/grafana/grafana.ini . Enable cookie_secure and set the value to true . Restart Grafana. supervisorctl restart grafana","title":"Secure"},{"location":"how-to/secure.html#secure","text":"You can improve the security of your PMM installation with: SSL encryption to secure traffic between client and server; Grafana HTTPS secure cookies To see which security features are enabled: pmm-admin status Tip You can gain an extra level of security by keeping PMM Server isolated from the internet, if possible.","title":"Secure"},{"location":"how-to/secure.html#ssl-encryption","text":"You need valid SSL certificates to encrypt traffic between client and server. With our Docker, OVF and AMI images, self-signed certificates are in /srv/nginx . To use your own, you can either: mount the local certificate directory to the same location, or, copy your certificates to a running PMM Server container.","title":"SSL encryption"},{"location":"how-to/secure.html#mounting-certificates","text":"For example, if your own certificates are in /etc/pmm-certs : docker run -d -p 443 :443 --volumes-from pmm-data \\ --name pmm-server -v /etc/pmm-certs:/srv/nginx \\ --restart always percona/pmm-server:2 The certificates must be owned by root. You can do this with: chown 0:0 /etc/pmm-certs/* The mounted certificate directory ( /etc/pmm-certs in this example) must contain the files certificate.crt , certificate.key , ca-certs.pem and dhparam.pem . For SSL encryption, the container must publish on port 443 instead of 80.","title":"Mounting certificates"},{"location":"how-to/secure.html#copying-certificates","text":"If PMM Server is running as a Docker image, use docker cp to copy certificates. This example copies certificate files from the current working directory to a running PMM Server docker container. docker cp certificate.crt pmm-server:/srv/nginx/certificate.crt docker cp certificate.key pmm-server:/srv/nginx/certificate.key docker cp ca-certs.pem pmm-server:/srv/nginx/ca-certs.pem docker cp dhparam.pem pmm-server:/srv/nginx/dhparam.pem","title":"Copying certificates"},{"location":"how-to/secure.html#enabling-ssl-when-connecting-pmm-client-to-pmm-server","text":"pmm-admin config --server-url = https://<user>:<password>@<server IP>","title":"Enabling SSL when connecting PMM Client to PMM Server"},{"location":"how-to/secure.html#grafana-https-secure-cookies","text":"To enable: Start a shell within the Docker container. docker exec -it pmm-server bash Edit /etc/grafana/grafana.ini . Enable cookie_secure and set the value to true . Restart Grafana. supervisorctl restart grafana","title":"Grafana HTTPS secure cookies"},{"location":"how-to/share-dashboard%202.html","text":"Share dashboards and panels \u00b6 When you need to share a dashboard with your team members, you can either send them a direct link to the dashboard, or render and send the dashboard as a .PNG image. Share as direct link \u00b6 Go to the dashboard that you want to share. Click at the top of the dashboard to display the panel menu. Select Share to reveal the Share Panel and either: copy and send the full URL for the dashboard, OR toggle the Short URL option to generate a simple link with a unique identifier Tip If your current domain is different than the one specified in the Grafana .INI configuration file, PMM will ask you to correct this mismatch before you can generate a short URL: To fix this Share as a PNG file \u00b6 Rendering images requires the Image Renderer plug-in. If your PMM Admin has not installed this for your PMM instance, you will see the following error message under Share Panel > Link . To install the dependencies: Connect to your PMM Server Docker container. docker exec -it pmm-server bash Install Grafana plug-ins. grafana-cli plugins install grafana-image-renderer Restart Grafana. supervisorctl restart grafana Install libraries. yum install -y libXcomposite libXdamage libXtst cups libXScrnSaver pango \\ atk adwaita-cursor-theme adwaita-icon-theme at at-spi2-atk at-spi2-core \\ cairo-gobject colord-libs dconf desktop-file-utils ed emacs-filesystem \\ gdk-pixbuf2 glib-networking gnutls gsettings-desktop-schemas \\ gtk-update-icon-cache gtk3 hicolor-icon-theme jasper-libs json-glib \\ libappindicator-gtk3 libdbusmenu libdbusmenu-gtk3 libepoxy \\ liberation-fonts liberation-narrow-fonts liberation-sans-fonts \\ liberation-serif-fonts libgusb libindicator-gtk3 libmodman libproxy \\ libsoup libwayland-cursor libwayland-egl libxkbcommon m4 mailx nettle \\ patch psmisc redhat-lsb-core redhat-lsb-submod-security rest spax time \\ trousers xdg-utils xkeyboard-config alsa-lib To render the image: Go to the dashboard that you want to share. Click at the top of the dashboard to display the panel menu. Select Share to reveal the Share Panel . Click Direct link rendered image . This opens a new browser tab. Wait for the image to be rendered, then use your browser\u2019s Image Save function to download the image.","title":"Share dashboards and panels"},{"location":"how-to/share-dashboard%202.html#share-dashboards-and-panels","text":"When you need to share a dashboard with your team members, you can either send them a direct link to the dashboard, or render and send the dashboard as a .PNG image.","title":"Share dashboards and panels"},{"location":"how-to/share-dashboard%202.html#share-as-direct-link","text":"Go to the dashboard that you want to share. Click at the top of the dashboard to display the panel menu. Select Share to reveal the Share Panel and either: copy and send the full URL for the dashboard, OR toggle the Short URL option to generate a simple link with a unique identifier Tip If your current domain is different than the one specified in the Grafana .INI configuration file, PMM will ask you to correct this mismatch before you can generate a short URL: To fix this","title":"Share as direct link"},{"location":"how-to/share-dashboard%202.html#share-as-a-png-file","text":"Rendering images requires the Image Renderer plug-in. If your PMM Admin has not installed this for your PMM instance, you will see the following error message under Share Panel > Link . To install the dependencies: Connect to your PMM Server Docker container. docker exec -it pmm-server bash Install Grafana plug-ins. grafana-cli plugins install grafana-image-renderer Restart Grafana. supervisorctl restart grafana Install libraries. yum install -y libXcomposite libXdamage libXtst cups libXScrnSaver pango \\ atk adwaita-cursor-theme adwaita-icon-theme at at-spi2-atk at-spi2-core \\ cairo-gobject colord-libs dconf desktop-file-utils ed emacs-filesystem \\ gdk-pixbuf2 glib-networking gnutls gsettings-desktop-schemas \\ gtk-update-icon-cache gtk3 hicolor-icon-theme jasper-libs json-glib \\ libappindicator-gtk3 libdbusmenu libdbusmenu-gtk3 libepoxy \\ liberation-fonts liberation-narrow-fonts liberation-sans-fonts \\ liberation-serif-fonts libgusb libindicator-gtk3 libmodman libproxy \\ libsoup libwayland-cursor libwayland-egl libxkbcommon m4 mailx nettle \\ patch psmisc redhat-lsb-core redhat-lsb-submod-security rest spax time \\ trousers xdg-utils xkeyboard-config alsa-lib To render the image: Go to the dashboard that you want to share. Click at the top of the dashboard to display the panel menu. Select Share to reveal the Share Panel . Click Direct link rendered image . This opens a new browser tab. Wait for the image to be rendered, then use your browser\u2019s Image Save function to download the image.","title":"Share as a PNG file"},{"location":"how-to/share-dashboard.html","text":"Share dashboards and panels \u00b6 When you need to share a dashboard with your team members, you can either send them a direct link to the dashboard, or render and send the dashboard as a .PNG image. Share as direct link \u00b6 Go to the dashboard that you want to share. Click at the top of the dashboard to display the panel menu. Select Share to reveal the Share Panel and either: copy and send the full URL for the dashboard, OR toggle the Short URL option to generate a simple link with a unique identifier Tip If your current domain is different than the one specified in the Grafana .INI configuration file, PMM will ask you to correct this mismatch before you can generate a short URL: To fix this Share as a PNG file \u00b6 Rendering images requires the Image Renderer plug-in. If your PMM Admin has not installed this for your PMM instance, you will see the following error message under Share Panel > Link . To install the dependencies: Connect to your PMM Server Docker container. docker exec -it pmm-server bash Install Grafana plug-ins. grafana-cli plugins install grafana-image-renderer Restart Grafana. supervisorctl restart grafana Install libraries. yum install -y libXcomposite libXdamage libXtst cups libXScrnSaver pango \\ atk adwaita-cursor-theme adwaita-icon-theme at at-spi2-atk at-spi2-core \\ cairo-gobject colord-libs dconf desktop-file-utils ed emacs-filesystem \\ gdk-pixbuf2 glib-networking gnutls gsettings-desktop-schemas \\ gtk-update-icon-cache gtk3 hicolor-icon-theme jasper-libs json-glib \\ libappindicator-gtk3 libdbusmenu libdbusmenu-gtk3 libepoxy \\ liberation-fonts liberation-narrow-fonts liberation-sans-fonts \\ liberation-serif-fonts libgusb libindicator-gtk3 libmodman libproxy \\ libsoup libwayland-cursor libwayland-egl libxkbcommon m4 mailx nettle \\ patch psmisc redhat-lsb-core redhat-lsb-submod-security rest spax time \\ trousers xdg-utils xkeyboard-config alsa-lib To render the image: Go to the dashboard that you want to share. Click at the top of the dashboard to display the panel menu. Select Share to reveal the Share Panel . Click Direct link rendered image . This opens a new browser tab. Wait for the image to be rendered, then use your browser\u2019s Image Save function to download the image.","title":"Share dashboards and panels"},{"location":"how-to/share-dashboard.html#share-dashboards-and-panels","text":"When you need to share a dashboard with your team members, you can either send them a direct link to the dashboard, or render and send the dashboard as a .PNG image.","title":"Share dashboards and panels"},{"location":"how-to/share-dashboard.html#share-as-direct-link","text":"Go to the dashboard that you want to share. Click at the top of the dashboard to display the panel menu. Select Share to reveal the Share Panel and either: copy and send the full URL for the dashboard, OR toggle the Short URL option to generate a simple link with a unique identifier Tip If your current domain is different than the one specified in the Grafana .INI configuration file, PMM will ask you to correct this mismatch before you can generate a short URL: To fix this","title":"Share as direct link"},{"location":"how-to/share-dashboard.html#share-as-a-png-file","text":"Rendering images requires the Image Renderer plug-in. If your PMM Admin has not installed this for your PMM instance, you will see the following error message under Share Panel > Link . To install the dependencies: Connect to your PMM Server Docker container. docker exec -it pmm-server bash Install Grafana plug-ins. grafana-cli plugins install grafana-image-renderer Restart Grafana. supervisorctl restart grafana Install libraries. yum install -y libXcomposite libXdamage libXtst cups libXScrnSaver pango \\ atk adwaita-cursor-theme adwaita-icon-theme at at-spi2-atk at-spi2-core \\ cairo-gobject colord-libs dconf desktop-file-utils ed emacs-filesystem \\ gdk-pixbuf2 glib-networking gnutls gsettings-desktop-schemas \\ gtk-update-icon-cache gtk3 hicolor-icon-theme jasper-libs json-glib \\ libappindicator-gtk3 libdbusmenu libdbusmenu-gtk3 libepoxy \\ liberation-fonts liberation-narrow-fonts liberation-sans-fonts \\ liberation-serif-fonts libgusb libindicator-gtk3 libmodman libproxy \\ libsoup libwayland-cursor libwayland-egl libxkbcommon m4 mailx nettle \\ patch psmisc redhat-lsb-core redhat-lsb-submod-security rest spax time \\ trousers xdg-utils xkeyboard-config alsa-lib To render the image: Go to the dashboard that you want to share. Click at the top of the dashboard to display the panel menu. Select Share to reveal the Share Panel . Click Direct link rendered image . This opens a new browser tab. Wait for the image to be rendered, then use your browser\u2019s Image Save function to download the image.","title":"Share as a PNG file"},{"location":"how-to/troubleshoot.html","text":"Resolve issues \u00b6 This section describes solutions to common problems and scenarios you might encounter while using PMM. Troubleshooting checklist \u00b6 The following questions might help you identify the origin of the problem while using Percona Monitoring and Management: Are you using the latest PMM version? Did you check the known issues section in the Release Notes for that particular PMM release? Are you receiving any error messages? Do the logs contain any messages about the problem? See Message logs and Trace logs for more information. Does the problem occur while configuring PMM, such as: Does the problem occur while you configure a specific function? Does the problem occur when you perform a particular task? Are you using the recommended authentication method? Does your system\u2019s firewall allow TCP traffic on the ports used by PMM? Have you allocated enough disk space for installing PMM? If not, check the disk allocation space. Are you using a Technical Preview feature? Technical Preview features are not production-ready and should only be used in testing environments. For more information, see the relevant Release Notes. For installing the PMM client, are you using a package other than a binary package without root permissions? Is your PMM Server installed and running with a known IP address accessible from the client node? Is the PMM Client installed, and is the node registered with PMM Server ? Is PMM-client configured correctly and has access to the config file? For monitoring MongoDB, do you have adminUserAnyDatabase or superuser role privilege to any database servers you want to monitor? For monitoring Amazon RDS using PMM, is there too much latency between PMM Server and the Amazon RDS instance? Have you upgraded the PMM Server before you upgraded the PMM Client? If yes, there might be configuration issues, thus leading to failure in the client-server communication, as PMM Server might not be able to identify all the parameters in the configuration. Is the PMM Server version higher than or equal to the PMM Client version? Otherwise, there might be configuration issues, thus leading to failure in the client-server communication, as PMM Server might not be able to identify all the parameters in the configuration. Troubleshooting areas \u00b6 Upgrade issues \u00b6 PMM server not updating correctly \u00b6 If the PMM server wasn\u2019t updated correctly, or if you have concerns about the release, you can force the update process in 2 ways: From the UI - Home panel: click the Alt key on the reload icon in the Update panel to make the Update Button visible even if you are on the same version as available for update. Pressing this button will force the system to rerun the update so that any broken or not installed components can be installed. In this case, you\u2019ll go through the usual update process with update logs and successful messages at the end. By API call (if UI not available): You can call the Update API directly with: curl --user admin:admin --request POST 'http://PMM_SERVER/v1/Updates/Start' Replace admin:admin with your username/password, and replace PMM_SERVER with your server address. You will not see the logs using this method. Refresh The Home page in 2-5 minutes, and you should see that PMM was updated. Upgrade PMM server using Docker . Configuration issues \u00b6 This section focuses on configuration issues, such as PMM-agent connection, adding and removing services for monitoring, and so on. Client-server connections \u00b6 There are many causes of broken network connectivity. The container is constrained by the host-level routing and firewall rules when using using Docker . For example, your hosting provider might have default iptables rules on their hosts that block communication between PMM Server and PMM Client, resulting in DOWN targets in VictoriaMetrics. If this happens, check the firewall and routing settings on the Docker host. PMM can also generate diagnostics data that can be examined and/or shared with our support team to help solve an issue. You can get collected logs from PMM Client using the pmm-admin summary command. Logs obtained in this way include PMM Client logs and logs received from the PMM Server, and stored separately in the client and server folders. The server folder also contains its client subfolder with the self-monitoring client information collected on the PMM Server. Beginning with PMM 2.4.0 , there is a flag that enables the fetching of pprof debug profiles and adds them to the diagnostics data. To enable, run pmm-admin summary --pprof . You can get PMM Server logs with either of these methods: Direct download In a browser, visit https://<address-of-your-pmm-server>/logs.zip . From Help menu Select Help \u2192 PMM Logs . Click PMM Logs to retrieve PMM diagnostics data which can be examined and shared with our support team should you need help. Connection difficulties \u00b6 Passwords When adding a service, the host might not be detected if the password contains special symbols (e.g., @ , % , etc.). In such cases, you should convert any password, replacing special characters with their escape sequence equivalents. One way to do this is to use the encodeURIComponent JavaScript function in your browser\u2019s web console (commonly found under a Development Tools menu). Run the function with your password as the parameter. For example: > encodeURIComponent ( \"s3cR#tpa$$worD\" ) will give: \"s3cR%23tpa%24%24worD\" Password change When adding clients to the PMM server, you use the admin user. However, if you change the password for the admin user from the PMM UI, then the clients will not be able to access PMM due to authentication issues. Also, Grafana will lock out the admin user due to multiple unsuccessful login attempts. In such a scenario, use API key for authentication. You can use API keys as a replacement for basic authentication. Percona Alerting \u00b6 No Alert rule templates tab on the Alerting page \u00b6 Percona Alerting option isn\u2019t active. Go to Configuration \u2192 Settings \u2192 Advanced Settings . Enable Alerting . Custom alert rule templates not migrated to Percona Alerting \u00b6 If you have used Integrated Alerting in previous PMM versions, and had custom templates under /srv/ia/templates , make sure to transfer them to /srv/alerting/templates . PMM is no longer sourcing templates from the ia folder, since we have deprecated Integrated Alerting with the 2.31 release. Unreachable external IP addresses \u00b6 If you get an email or page from your system that the IP is not reachable from outside my organization, do the following: To configure your PMM Server\u2019s Public Address, select Configuration \u2192 Settings \u2192 Advanced Settings , and supply an address to use in your alert notifications. Alert Rule Templates are disabled \u00b6 Built-In alerts are not editable, but you can copy them and edit the copies. (In PMM 2.14.0 and above). If you create a custom alert rule template, you will have access to edit. QAN issues \u00b6 This section focuses on problems with QAN, such as queries not being retrieved so on. Missing data \u00b6 Why don\u2019t I see any query-related information? There might be multiple places where the problem might come from: Connection problem between pmm-agent and pmm-managed PMM-agent cannot connect to the database. Data source is not properly configured. Why don\u2019t I see the whole query? Long query examples and fingerprints can be truncated to 1024 symbols to reduce space usage. In this case, the query explains section will not work.","title":"Resolve issues"},{"location":"how-to/troubleshoot.html#resolve-issues","text":"This section describes solutions to common problems and scenarios you might encounter while using PMM.","title":"Resolve issues"},{"location":"how-to/troubleshoot.html#troubleshooting-checklist","text":"The following questions might help you identify the origin of the problem while using Percona Monitoring and Management: Are you using the latest PMM version? Did you check the known issues section in the Release Notes for that particular PMM release? Are you receiving any error messages? Do the logs contain any messages about the problem? See Message logs and Trace logs for more information. Does the problem occur while configuring PMM, such as: Does the problem occur while you configure a specific function? Does the problem occur when you perform a particular task? Are you using the recommended authentication method? Does your system\u2019s firewall allow TCP traffic on the ports used by PMM? Have you allocated enough disk space for installing PMM? If not, check the disk allocation space. Are you using a Technical Preview feature? Technical Preview features are not production-ready and should only be used in testing environments. For more information, see the relevant Release Notes. For installing the PMM client, are you using a package other than a binary package without root permissions? Is your PMM Server installed and running with a known IP address accessible from the client node? Is the PMM Client installed, and is the node registered with PMM Server ? Is PMM-client configured correctly and has access to the config file? For monitoring MongoDB, do you have adminUserAnyDatabase or superuser role privilege to any database servers you want to monitor? For monitoring Amazon RDS using PMM, is there too much latency between PMM Server and the Amazon RDS instance? Have you upgraded the PMM Server before you upgraded the PMM Client? If yes, there might be configuration issues, thus leading to failure in the client-server communication, as PMM Server might not be able to identify all the parameters in the configuration. Is the PMM Server version higher than or equal to the PMM Client version? Otherwise, there might be configuration issues, thus leading to failure in the client-server communication, as PMM Server might not be able to identify all the parameters in the configuration.","title":"Troubleshooting checklist"},{"location":"how-to/troubleshoot.html#troubleshooting-areas","text":"","title":"Troubleshooting areas"},{"location":"how-to/troubleshoot.html#upgrade-issues","text":"","title":"Upgrade issues"},{"location":"how-to/troubleshoot.html#pmm-server-not-updating-correctly","text":"If the PMM server wasn\u2019t updated correctly, or if you have concerns about the release, you can force the update process in 2 ways: From the UI - Home panel: click the Alt key on the reload icon in the Update panel to make the Update Button visible even if you are on the same version as available for update. Pressing this button will force the system to rerun the update so that any broken or not installed components can be installed. In this case, you\u2019ll go through the usual update process with update logs and successful messages at the end. By API call (if UI not available): You can call the Update API directly with: curl --user admin:admin --request POST 'http://PMM_SERVER/v1/Updates/Start' Replace admin:admin with your username/password, and replace PMM_SERVER with your server address. You will not see the logs using this method. Refresh The Home page in 2-5 minutes, and you should see that PMM was updated. Upgrade PMM server using Docker .","title":"PMM server not updating correctly"},{"location":"how-to/troubleshoot.html#configuration-issues","text":"This section focuses on configuration issues, such as PMM-agent connection, adding and removing services for monitoring, and so on.","title":"Configuration issues"},{"location":"how-to/troubleshoot.html#client-server-connections","text":"There are many causes of broken network connectivity. The container is constrained by the host-level routing and firewall rules when using using Docker . For example, your hosting provider might have default iptables rules on their hosts that block communication between PMM Server and PMM Client, resulting in DOWN targets in VictoriaMetrics. If this happens, check the firewall and routing settings on the Docker host. PMM can also generate diagnostics data that can be examined and/or shared with our support team to help solve an issue. You can get collected logs from PMM Client using the pmm-admin summary command. Logs obtained in this way include PMM Client logs and logs received from the PMM Server, and stored separately in the client and server folders. The server folder also contains its client subfolder with the self-monitoring client information collected on the PMM Server. Beginning with PMM 2.4.0 , there is a flag that enables the fetching of pprof debug profiles and adds them to the diagnostics data. To enable, run pmm-admin summary --pprof . You can get PMM Server logs with either of these methods: Direct download In a browser, visit https://<address-of-your-pmm-server>/logs.zip . From Help menu Select Help \u2192 PMM Logs . Click PMM Logs to retrieve PMM diagnostics data which can be examined and shared with our support team should you need help.","title":"Client-server connections"},{"location":"how-to/troubleshoot.html#connection-difficulties","text":"Passwords When adding a service, the host might not be detected if the password contains special symbols (e.g., @ , % , etc.). In such cases, you should convert any password, replacing special characters with their escape sequence equivalents. One way to do this is to use the encodeURIComponent JavaScript function in your browser\u2019s web console (commonly found under a Development Tools menu). Run the function with your password as the parameter. For example: > encodeURIComponent ( \"s3cR#tpa$$worD\" ) will give: \"s3cR%23tpa%24%24worD\" Password change When adding clients to the PMM server, you use the admin user. However, if you change the password for the admin user from the PMM UI, then the clients will not be able to access PMM due to authentication issues. Also, Grafana will lock out the admin user due to multiple unsuccessful login attempts. In such a scenario, use API key for authentication. You can use API keys as a replacement for basic authentication.","title":"Connection difficulties"},{"location":"how-to/troubleshoot.html#percona-alerting","text":"","title":"Percona Alerting"},{"location":"how-to/troubleshoot.html#no-alert-rule-templates-tab-on-the-alerting-page","text":"Percona Alerting option isn\u2019t active. Go to Configuration \u2192 Settings \u2192 Advanced Settings . Enable Alerting .","title":"No Alert rule templates tab on the Alerting page"},{"location":"how-to/troubleshoot.html#custom-alert-rule-templates-not-migrated-to-percona-alerting","text":"If you have used Integrated Alerting in previous PMM versions, and had custom templates under /srv/ia/templates , make sure to transfer them to /srv/alerting/templates . PMM is no longer sourcing templates from the ia folder, since we have deprecated Integrated Alerting with the 2.31 release.","title":"Custom alert rule templates not migrated to Percona Alerting"},{"location":"how-to/troubleshoot.html#unreachable-external-ip-addresses","text":"If you get an email or page from your system that the IP is not reachable from outside my organization, do the following: To configure your PMM Server\u2019s Public Address, select Configuration \u2192 Settings \u2192 Advanced Settings , and supply an address to use in your alert notifications.","title":"Unreachable external IP addresses"},{"location":"how-to/troubleshoot.html#alert-rule-templates-are-disabled","text":"Built-In alerts are not editable, but you can copy them and edit the copies. (In PMM 2.14.0 and above). If you create a custom alert rule template, you will have access to edit.","title":"Alert Rule Templates are disabled"},{"location":"how-to/troubleshoot.html#qan-issues","text":"This section focuses on problems with QAN, such as queries not being retrieved so on.","title":"QAN issues"},{"location":"how-to/troubleshoot.html#missing-data","text":"Why don\u2019t I see any query-related information? There might be multiple places where the problem might come from: Connection problem between pmm-agent and pmm-managed PMM-agent cannot connect to the database. Data source is not properly configured. Why don\u2019t I see the whole query? Long query examples and fingerprints can be truncated to 1024 symbols to reduce space usage. In this case, the query explains section will not work.","title":"Missing data"},{"location":"how-to/upgrade.html","text":"Upgrade \u00b6 Important Upgrade the PMM Server before you upgrade the PMM Client. Ensure that the PMM Server version is higher than or equal to the PMM Client version. Otherwise, there might be configuration issues, thus leading to failure in the client-server communication as PMM Server might not be able to identify all the parameters in the configuration. For example, for a PMM Server version 2.25.0, the PMM Client version should be 2.25.0 or 2.24.0. If the PMM Client version is 2.26.0, PMM might not work as expected. Updating a Server \u00b6 Client and server components are installed and updated separately. PMM Server can run natively, as a Docker image, a virtual appliance, or an AWS cloud instance. Each has its own installation and update steps. The preferred and simplest way to update PMM Server is with the PMM Upgrade panel on the Home page. The panel shows: the current server version and release date; whether the server is up to date; the last time a check was made for updates. Click the refresh button to manually check for updates. If one is available, click the update button to update to the version indicated. See also PMM Server Docker upgrade Updating a PMM-Agent \u00b6 PMM-Agent can be updated from tarball: Download tar.gz with pmm2-client. Extract it. Run ./install_tarball script with the \u201c-u\u201d flag. Hint! The configuration file will be overwritten if you do not provide the \u201c-u\u201d flag while the pmm-agent is updated. Upgrade from PMM 1 \u00b6 Because of the significant architectural changes between PMM1 and PMM2, there is no direct upgrade path. The approach to making the switch from PMM version 1 to 2 is a gradual transition, outlined in this blog post . In short, it involves first standing up a new PMM2 server on a new host and connecting clients to it. As new data is reported to the PMM2 server, old metrics will age with the retention period (30 days, by default), at which point you\u2019ll be able to shut down your existing PMM1 server. Any alerts configured through the Grafana UI will have to be recreated due to the target dashboard id\u2019s not matching between PMM1 and PMM2. In this instance we recommend moving to Alertmanager recipes in PMM2 for alerting which, for the time being, requires a separate Alertmanager instance. We are working on integrating this natively into PMM2 Server and expect to support your existing Alertmanager rules.","title":"Upgrade"},{"location":"how-to/upgrade.html#upgrade","text":"Important Upgrade the PMM Server before you upgrade the PMM Client. Ensure that the PMM Server version is higher than or equal to the PMM Client version. Otherwise, there might be configuration issues, thus leading to failure in the client-server communication as PMM Server might not be able to identify all the parameters in the configuration. For example, for a PMM Server version 2.25.0, the PMM Client version should be 2.25.0 or 2.24.0. If the PMM Client version is 2.26.0, PMM might not work as expected.","title":"Upgrade"},{"location":"how-to/upgrade.html#updating-a-server","text":"Client and server components are installed and updated separately. PMM Server can run natively, as a Docker image, a virtual appliance, or an AWS cloud instance. Each has its own installation and update steps. The preferred and simplest way to update PMM Server is with the PMM Upgrade panel on the Home page. The panel shows: the current server version and release date; whether the server is up to date; the last time a check was made for updates. Click the refresh button to manually check for updates. If one is available, click the update button to update to the version indicated. See also PMM Server Docker upgrade","title":"Updating a Server"},{"location":"how-to/upgrade.html#updating-a-pmm-agent","text":"PMM-Agent can be updated from tarball: Download tar.gz with pmm2-client. Extract it. Run ./install_tarball script with the \u201c-u\u201d flag. Hint! The configuration file will be overwritten if you do not provide the \u201c-u\u201d flag while the pmm-agent is updated.","title":"Updating a PMM-Agent"},{"location":"how-to/upgrade.html#upgrade-from-pmm-1","text":"Because of the significant architectural changes between PMM1 and PMM2, there is no direct upgrade path. The approach to making the switch from PMM version 1 to 2 is a gradual transition, outlined in this blog post . In short, it involves first standing up a new PMM2 server on a new host and connecting clients to it. As new data is reported to the PMM2 server, old metrics will age with the retention period (30 days, by default), at which point you\u2019ll be able to shut down your existing PMM1 server. Any alerts configured through the Grafana UI will have to be recreated due to the target dashboard id\u2019s not matching between PMM1 and PMM2. In this instance we recommend moving to Alertmanager recipes in PMM2 for alerting which, for the time being, requires a separate Alertmanager instance. We are working on integrating this natively into PMM2 Server and expect to support your existing Alertmanager rules.","title":"Upgrade from PMM 1"},{"location":"release-notes/index.html","text":"Release Notes \u00b6 Percona Monitoring and Management 2.31.0 Percona Monitoring and Management 2.30.0 Percona Monitoring and Management 2.29.1 Percona Monitoring and Management 2.29.0 Percona Monitoring and Management 2.28.0 Percona Monitoring and Management 2.27.0 Percona Monitoring and Management 2.26.0 Percona Monitoring and Management 2.25.0 Percona Monitoring and Management 2.24.0 Percona Monitoring and Management 2.23.0 Percona Monitoring and Management 2.22.0 Percona Monitoring and Management 2.21.0 Percona Monitoring and Management 2.20.0 Percona Monitoring and Management 2.19.0 Percona Monitoring and Management 2.18.0 Percona Monitoring and Management 2.17.0 Percona Monitoring and Management 2.16.0 Percona Monitoring and Management 2.15.1 Percona Monitoring and Management 2.15.0 Percona Monitoring and Management 2.14.0 Percona Monitoring and Management 2.13.0 Percona Monitoring and Management 2.12.0 Percona Monitoring and Management 2.11.1 Percona Monitoring and Management 2.11.0 Percona Monitoring and Management 2.10.1 Percona Monitoring and Management 2.10.0 Percona Monitoring and Management 2.9.1 Percona Monitoring and Management 2.9.0 Percona Monitoring and Management 2.8.0 Percona Monitoring and Management 2.7.0 Percona Monitoring and Management 2.6.1 Percona Monitoring and Management 2.6.0 Percona Monitoring and Management 2.5.0 Percona Monitoring and Management 2.4.0 Percona Monitoring and Management 2.3.0 Percona Monitoring and Management 2.2.2 Percona Monitoring and Management 2.2.1 Percona Monitoring and Management 2.2.0 Percona Monitoring and Management 2.1.0 Percona Monitoring and Management 2.0.1 Percona Monitoring and Management 2.0.0","title":"Release Notes"},{"location":"release-notes/index.html#release-notes","text":"Percona Monitoring and Management 2.31.0 Percona Monitoring and Management 2.30.0 Percona Monitoring and Management 2.29.1 Percona Monitoring and Management 2.29.0 Percona Monitoring and Management 2.28.0 Percona Monitoring and Management 2.27.0 Percona Monitoring and Management 2.26.0 Percona Monitoring and Management 2.25.0 Percona Monitoring and Management 2.24.0 Percona Monitoring and Management 2.23.0 Percona Monitoring and Management 2.22.0 Percona Monitoring and Management 2.21.0 Percona Monitoring and Management 2.20.0 Percona Monitoring and Management 2.19.0 Percona Monitoring and Management 2.18.0 Percona Monitoring and Management 2.17.0 Percona Monitoring and Management 2.16.0 Percona Monitoring and Management 2.15.1 Percona Monitoring and Management 2.15.0 Percona Monitoring and Management 2.14.0 Percona Monitoring and Management 2.13.0 Percona Monitoring and Management 2.12.0 Percona Monitoring and Management 2.11.1 Percona Monitoring and Management 2.11.0 Percona Monitoring and Management 2.10.1 Percona Monitoring and Management 2.10.0 Percona Monitoring and Management 2.9.1 Percona Monitoring and Management 2.9.0 Percona Monitoring and Management 2.8.0 Percona Monitoring and Management 2.7.0 Percona Monitoring and Management 2.6.1 Percona Monitoring and Management 2.6.0 Percona Monitoring and Management 2.5.0 Percona Monitoring and Management 2.4.0 Percona Monitoring and Management 2.3.0 Percona Monitoring and Management 2.2.2 Percona Monitoring and Management 2.2.1 Percona Monitoring and Management 2.2.0 Percona Monitoring and Management 2.1.0 Percona Monitoring and Management 2.0.1 Percona Monitoring and Management 2.0.0","title":"Release Notes"},{"location":"release-notes/2.0.0.html","text":"Percona Monitoring and Management 2.0.0 \u00b6 Date: September 19, 2019 Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. You can run PMM in your own environment for the best security and reliability. It provides thorough time-based analysis for MySQL, MongoDB, and PostgreSQL servers to ensure that your data works as efficiently as possible. For install instructions, see Installing Percona Monitoring and Management . Caution PMM 2 is designed to be used as a new installation \u2014 please don\u2019t try to upgrade your existing PMM 1 environment. The new PMM2 introduces a number of enhancements and additional feature improvements, including: Detailed query analytics and filtering technologies which enable you to identify issues faster than ever before. A better user experience: Service-level dashboards give you immediate access to the data you need. The new addition of PostgreSQL query tuning. Enhanced security protocols to ensure your data is safe. Our new API allows you to extend and interact with third-party tools. More details about new and improved features available within the release can be found in the corresponding blog post . Help us improve our software quality by reporting any Percona Monitoring and Management bugs you encounter using our bug tracking system .","title":"PMM 2.0.0"},{"location":"release-notes/2.0.0.html#percona-monitoring-and-management-200","text":"Date: September 19, 2019 Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. You can run PMM in your own environment for the best security and reliability. It provides thorough time-based analysis for MySQL, MongoDB, and PostgreSQL servers to ensure that your data works as efficiently as possible. For install instructions, see Installing Percona Monitoring and Management . Caution PMM 2 is designed to be used as a new installation \u2014 please don\u2019t try to upgrade your existing PMM 1 environment. The new PMM2 introduces a number of enhancements and additional feature improvements, including: Detailed query analytics and filtering technologies which enable you to identify issues faster than ever before. A better user experience: Service-level dashboards give you immediate access to the data you need. The new addition of PostgreSQL query tuning. Enhanced security protocols to ensure your data is safe. Our new API allows you to extend and interact with third-party tools. More details about new and improved features available within the release can be found in the corresponding blog post . Help us improve our software quality by reporting any Percona Monitoring and Management bugs you encounter using our bug tracking system .","title":"Percona Monitoring and Management 2.0.0"},{"location":"release-notes/2.0.1.html","text":"Percona Monitoring and Management 2.0.1 \u00b6 Date: October 9, 2019 Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. You can run PMM in your own environment for maximum security and reliability. It provides thorough time-based analysis for MySQL, MongoDB, and PostgreSQL servers to ensure that your data works as efficiently as possible. For install instructions, see Installing Percona Monitoring and Management . Caution PMM 2 is designed to be used as a new installation \u2014 please don\u2019t try to upgrade your existing PMM 1 environment. Improvements \u00b6 PMM-4779 : Securely share dashboards with Percona PMM-4735 : Keep one old slowlog file after rotation PMM-4724 : Alt+click on check updates button enables force-update PMM-4444 : Return \u201cwhat\u2019s new\u201d URL with the information extracted from the pmm-update package change log Fixed bugs \u00b6 PMM-4758 : Remove Inventory rows from dashboards PMM-4757 : qan_mysql_perfschema_agent failed querying events_statements_summary_by_digest due to data types conversion PMM-4755 : Fixed a typo in the InnoDB AHI Miss Ratio formula PMM-4749 : Navigation from Dashboards to QAN when some Node or Service was selected now applies filtering by them in QAN PMM-4742 : General information links were updated to go to PMM 2 related pages PMM-4739 : Remove request instances list PMM-4734 : A fix was made for the collecting node_name formula at MySQL Replication Summary dashboard PMM-4729 : Fixes were made for formulas on MySQL Instances Overview PMM-4726 : Links to services in MongoDB singlestats didn\u2019t show Node name PMM-4720 : machine_id could contain trailing \\\\n PMM-4640 : It was not possible to add MongoDB remotely if password contained a # symbol Help us improve our software quality by reporting any Percona Monitoring and Management bugs you encounter using our bug tracking system .","title":"PMM 2.0.1"},{"location":"release-notes/2.0.1.html#percona-monitoring-and-management-201","text":"Date: October 9, 2019 Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. You can run PMM in your own environment for maximum security and reliability. It provides thorough time-based analysis for MySQL, MongoDB, and PostgreSQL servers to ensure that your data works as efficiently as possible. For install instructions, see Installing Percona Monitoring and Management . Caution PMM 2 is designed to be used as a new installation \u2014 please don\u2019t try to upgrade your existing PMM 1 environment.","title":"Percona Monitoring and Management 2.0.1"},{"location":"release-notes/2.0.1.html#improvements","text":"PMM-4779 : Securely share dashboards with Percona PMM-4735 : Keep one old slowlog file after rotation PMM-4724 : Alt+click on check updates button enables force-update PMM-4444 : Return \u201cwhat\u2019s new\u201d URL with the information extracted from the pmm-update package change log","title":"Improvements"},{"location":"release-notes/2.0.1.html#fixed-bugs","text":"PMM-4758 : Remove Inventory rows from dashboards PMM-4757 : qan_mysql_perfschema_agent failed querying events_statements_summary_by_digest due to data types conversion PMM-4755 : Fixed a typo in the InnoDB AHI Miss Ratio formula PMM-4749 : Navigation from Dashboards to QAN when some Node or Service was selected now applies filtering by them in QAN PMM-4742 : General information links were updated to go to PMM 2 related pages PMM-4739 : Remove request instances list PMM-4734 : A fix was made for the collecting node_name formula at MySQL Replication Summary dashboard PMM-4729 : Fixes were made for formulas on MySQL Instances Overview PMM-4726 : Links to services in MongoDB singlestats didn\u2019t show Node name PMM-4720 : machine_id could contain trailing \\\\n PMM-4640 : It was not possible to add MongoDB remotely if password contained a # symbol Help us improve our software quality by reporting any Percona Monitoring and Management bugs you encounter using our bug tracking system .","title":"Fixed bugs"},{"location":"release-notes/2.1.0.html","text":"Percona Monitoring and Management 2.1.0 \u00b6 Date: November 11, 2019 Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. You can run PMM in your own environment for maximum security and reliability. It provides thorough time-based analysis for MySQL, MongoDB, and PostgreSQL servers to ensure that your data works as efficiently as possible. For install instructions, see Installing Percona Monitoring and Management . Caution PMM 2 is designed to be used as a new installation \u2014 please don\u2019t try to upgrade your existing PMM 1 environment. Improvements and new features \u00b6 PMM-4063 : Update QAN filter panel to show only labels available for selection under currently applied filters PMM-815 : Latency Detail graph added to the MongoDB Instance Summary dashboard PMM-4768 : Disable heavy-load collectors automatically when there are too many tables PMM-4821 : Use color gradient in filled graphs on all dashboards PMM-4733 : Add more log and configuration files to the downloadable logs.zip archive PMM-4672 : Use integer percentage values in QAN filter panel PMM-4857 : Update tooltips for all MongoDB dashboards PMM-4616 : Rename column in the Query Details section in QAN from Total to Sum PMM-4770 : Use Go 1.12.10 PMM-4780 : Update Grafana to version 6.4.1 PMM-4918 : Update Grafana plugins to newer versions, including the clickhouse-datasource plugin Fixed bugs \u00b6 PMM-4935 : Wrong instance name displayed on the MySQL Instance Summary dashboard due to the incorrect string crop PMM-4916 : Wrong values are shown when changing the time range for the Node Summary Dashboard in case of remote instances PMM-4895 and PMM-4814 : The update process reports completion before it is actually done and therefore some dashboards, etc. may not be updated PMM-4876 : PMM Server access credentials are shown by the pmm-admin status command instead of hiding them for security reasons PMM-4875 : PostgreSQL error log gets flooded with warnings when pg_stat_statements extension is not installed in the database used by PMM Server or when PostgreSQL user is unable to connect to it PMM-4852 : Node name has an incorrect value if the Home dashboard opened after QAN PMM-4847 : Drill-downs from the Environment Overview dashboard doesn\u2019t show data for the preselected host PMM-4841 and PMM-4845 : pg_stat_statement QAN Agent leaks database connections PMM-4831 : Clean-up representation of selectors names on MySQL-related dashboards for a better consistency PMM-4824 : Incorrectly calculated singlestat values on MySQL Instances Overview dashboard PMM-4819 : In case of the only one monitored host, its uptime is shown as a smaller value than the all hosts uptime due to the inaccurate rounding PMM-4816 : Set equal thresholds to avoid confusing singlestat color differences on a Home dashboard PMM-4718 : Labels are not fully displayed in the filter panel of the Query Details section in QAN PMM-4545 : Long queries are not fully visible in the Query Examples section in QAN Help us improve our software quality by reporting any Percona Monitoring and Management bugs you encounter using our bug tracking system .","title":"PMM 2.1.0"},{"location":"release-notes/2.1.0.html#percona-monitoring-and-management-210","text":"Date: November 11, 2019 Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. You can run PMM in your own environment for maximum security and reliability. It provides thorough time-based analysis for MySQL, MongoDB, and PostgreSQL servers to ensure that your data works as efficiently as possible. For install instructions, see Installing Percona Monitoring and Management . Caution PMM 2 is designed to be used as a new installation \u2014 please don\u2019t try to upgrade your existing PMM 1 environment.","title":"Percona Monitoring and Management 2.1.0"},{"location":"release-notes/2.1.0.html#improvements-and-new-features","text":"PMM-4063 : Update QAN filter panel to show only labels available for selection under currently applied filters PMM-815 : Latency Detail graph added to the MongoDB Instance Summary dashboard PMM-4768 : Disable heavy-load collectors automatically when there are too many tables PMM-4821 : Use color gradient in filled graphs on all dashboards PMM-4733 : Add more log and configuration files to the downloadable logs.zip archive PMM-4672 : Use integer percentage values in QAN filter panel PMM-4857 : Update tooltips for all MongoDB dashboards PMM-4616 : Rename column in the Query Details section in QAN from Total to Sum PMM-4770 : Use Go 1.12.10 PMM-4780 : Update Grafana to version 6.4.1 PMM-4918 : Update Grafana plugins to newer versions, including the clickhouse-datasource plugin","title":"Improvements and new features"},{"location":"release-notes/2.1.0.html#fixed-bugs","text":"PMM-4935 : Wrong instance name displayed on the MySQL Instance Summary dashboard due to the incorrect string crop PMM-4916 : Wrong values are shown when changing the time range for the Node Summary Dashboard in case of remote instances PMM-4895 and PMM-4814 : The update process reports completion before it is actually done and therefore some dashboards, etc. may not be updated PMM-4876 : PMM Server access credentials are shown by the pmm-admin status command instead of hiding them for security reasons PMM-4875 : PostgreSQL error log gets flooded with warnings when pg_stat_statements extension is not installed in the database used by PMM Server or when PostgreSQL user is unable to connect to it PMM-4852 : Node name has an incorrect value if the Home dashboard opened after QAN PMM-4847 : Drill-downs from the Environment Overview dashboard doesn\u2019t show data for the preselected host PMM-4841 and PMM-4845 : pg_stat_statement QAN Agent leaks database connections PMM-4831 : Clean-up representation of selectors names on MySQL-related dashboards for a better consistency PMM-4824 : Incorrectly calculated singlestat values on MySQL Instances Overview dashboard PMM-4819 : In case of the only one monitored host, its uptime is shown as a smaller value than the all hosts uptime due to the inaccurate rounding PMM-4816 : Set equal thresholds to avoid confusing singlestat color differences on a Home dashboard PMM-4718 : Labels are not fully displayed in the filter panel of the Query Details section in QAN PMM-4545 : Long queries are not fully visible in the Query Examples section in QAN Help us improve our software quality by reporting any Percona Monitoring and Management bugs you encounter using our bug tracking system .","title":"Fixed bugs"},{"location":"release-notes/2.10.0.html","text":"Percona Monitoring and Management 2.10.0 \u00b6 Date: September 15, 2020 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. New Features \u00b6 PMM-2045 : New dashboard: MySQL Group Replication Summary PMM-5738 : Enhanced exporter: replaced original mongodb-exporter with a completely rewritten one with improved functionality PMM-5126 : Query Analytics Dashboard: Search by query substring or dimension (Thanks to user debug for reporting this issue) PMM-6360 : Grafana Upgrade to 7.1.3 PMM-6355 : Upgrade Prometheus to 2.19.3 PMM-6597 : Documentation: Updated Image rendering instructions for PMM PMM-6568 : Reusable user interface component: Pop-up dialog. Allows for more consistent interfaces across PMM PMM-6375 , PMM-6373 , PMM-6372 : Sign in, Sign up and Sign out UI for Percona Account inside PMM Server PMM-6328 : Query Analytics Dashboard: Mouse-over crosshair shows value on sparklines PMM-3831 : Node Summary Dashboard: Add pt-summary output to dashboard to provide details on system status and configuration Improvements \u00b6 PMM-6647 : MongoDB dashboards: RocksDB Details removed, MMAPv1 & Cluster Summary changed PMM-6536 : Query Analytics Dashboard: Improved filter/time search message when no results PMM-6467 : PMM Settings: User-friendly error message PMM-5947 : Bind services to internal address for containers Bugs Fixed \u00b6 PMM-6336 : Suppress sensitive data: honor pmm-admin flag --disable-queryexamples when used in conjunction with --query-source=perfschema PMM-6244 : MySQL InnoDB Details Dashboard: Inverted color scheme on \u201cBP Write Buffering\u201d panel PMM-6294 : Query Analytics Dashboard doesn\u2019t resize well for some screen resolutions (Thanks to user debug for reporting this issue) PMM-5701 : Home Dashboard: Incorrect metric for DB uptime (Thanks to user hubi_oediv for reporting this issue) PMM-6427 : Query Analytics dashboard: Examples broken when switching from MongoDB to MySQL query PMM-5684 : Use actual data from INFORMATION_SCHEMA vs relying on cached data (which can be 24 hrs old by default) PMM-6500 : PMM Database Checks: Unwanted high-contrast styling PMM-6440 : MongoDB ReplSet Summary Dashboard: Primary shows more lag than replicas PMM-6436 : Query Analytics Dashboard: Styles updated to conform with upgrade to Grafana 7.x PMM-6415 : Node Summary Dashboard: Redirection to database\u2019s Instance Summary dashboard omits Service Name PMM-6324 : Query Analytics Dashboard: Showing stale data while fetching updated data for query details section PMM-6316 : Query Analytics Dashboard: Inconsistent scrollbar styles PMM-6276 : PMM Inventory: Long lists unclear; poor contrast & column headings scroll out of view PMM-6529 : Query Analytics filter input margin disappears after scrolling Known Issues \u00b6 PMM-6643 : High CPU usage for new MongoDB exporter (fixed in Percona Monitoring and Management 2.10.1 )","title":"PMM 2.10.0"},{"location":"release-notes/2.10.0.html#percona-monitoring-and-management-2100","text":"Date: September 15, 2020 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance.","title":"Percona Monitoring and Management 2.10.0"},{"location":"release-notes/2.10.0.html#new-features","text":"PMM-2045 : New dashboard: MySQL Group Replication Summary PMM-5738 : Enhanced exporter: replaced original mongodb-exporter with a completely rewritten one with improved functionality PMM-5126 : Query Analytics Dashboard: Search by query substring or dimension (Thanks to user debug for reporting this issue) PMM-6360 : Grafana Upgrade to 7.1.3 PMM-6355 : Upgrade Prometheus to 2.19.3 PMM-6597 : Documentation: Updated Image rendering instructions for PMM PMM-6568 : Reusable user interface component: Pop-up dialog. Allows for more consistent interfaces across PMM PMM-6375 , PMM-6373 , PMM-6372 : Sign in, Sign up and Sign out UI for Percona Account inside PMM Server PMM-6328 : Query Analytics Dashboard: Mouse-over crosshair shows value on sparklines PMM-3831 : Node Summary Dashboard: Add pt-summary output to dashboard to provide details on system status and configuration","title":"New Features"},{"location":"release-notes/2.10.0.html#improvements","text":"PMM-6647 : MongoDB dashboards: RocksDB Details removed, MMAPv1 & Cluster Summary changed PMM-6536 : Query Analytics Dashboard: Improved filter/time search message when no results PMM-6467 : PMM Settings: User-friendly error message PMM-5947 : Bind services to internal address for containers","title":"Improvements"},{"location":"release-notes/2.10.0.html#bugs-fixed","text":"PMM-6336 : Suppress sensitive data: honor pmm-admin flag --disable-queryexamples when used in conjunction with --query-source=perfschema PMM-6244 : MySQL InnoDB Details Dashboard: Inverted color scheme on \u201cBP Write Buffering\u201d panel PMM-6294 : Query Analytics Dashboard doesn\u2019t resize well for some screen resolutions (Thanks to user debug for reporting this issue) PMM-5701 : Home Dashboard: Incorrect metric for DB uptime (Thanks to user hubi_oediv for reporting this issue) PMM-6427 : Query Analytics dashboard: Examples broken when switching from MongoDB to MySQL query PMM-5684 : Use actual data from INFORMATION_SCHEMA vs relying on cached data (which can be 24 hrs old by default) PMM-6500 : PMM Database Checks: Unwanted high-contrast styling PMM-6440 : MongoDB ReplSet Summary Dashboard: Primary shows more lag than replicas PMM-6436 : Query Analytics Dashboard: Styles updated to conform with upgrade to Grafana 7.x PMM-6415 : Node Summary Dashboard: Redirection to database\u2019s Instance Summary dashboard omits Service Name PMM-6324 : Query Analytics Dashboard: Showing stale data while fetching updated data for query details section PMM-6316 : Query Analytics Dashboard: Inconsistent scrollbar styles PMM-6276 : PMM Inventory: Long lists unclear; poor contrast & column headings scroll out of view PMM-6529 : Query Analytics filter input margin disappears after scrolling","title":"Bugs Fixed"},{"location":"release-notes/2.10.0.html#known-issues","text":"PMM-6643 : High CPU usage for new MongoDB exporter (fixed in Percona Monitoring and Management 2.10.1 )","title":"Known Issues"},{"location":"release-notes/2.10.1.html","text":"Percona Monitoring and Management 2.10.1 \u00b6 Date: September 22, 2020 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. Bugs Fixed \u00b6 PMM-6643 : New MongoDB exporter has higher CPU usage compared with old","title":"PMM 2.10.1"},{"location":"release-notes/2.10.1.html#percona-monitoring-and-management-2101","text":"Date: September 22, 2020 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance.","title":"Percona Monitoring and Management 2.10.1"},{"location":"release-notes/2.10.1.html#bugs-fixed","text":"PMM-6643 : New MongoDB exporter has higher CPU usage compared with old","title":"Bugs Fixed"},{"location":"release-notes/2.11.0.html","text":"Percona Monitoring and Management 2.11.0 \u00b6 Date: October 14, 2020 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. New Features \u00b6 PMM-6567 : Technical preview of new PostgreSQL extension pg_stat_monitor PMM-6515 : Link added directly to Node/Service page from Query Analytics filters, opens in new window Improvements \u00b6 PMM-6727 : Grafana plugin updates: grafana-polystat-panel=1.2.2 , grafana-piechart-panel=1.6.1 PMM-6625 : Default sort to \u201cAverage - descending\u201d on all dashboards PMM-6609 : MySQL Instances Compare & Summary dashboards: Changed metric in \u2018MySQL Internal Memory Overview\u2019 PMM-6598 : Dashboard image sharing (Share Panel): Improved wording with link to configuration instructions PMM-6557 : Update Prometheus to 2.21.0 PMM-6554 : MySQL InnoDB Details dashboard: Add \u201csync flushing\u201d to \u201cInnoDB Flushing by Type\u201d Bugs Fixed \u00b6 PMM-4547 : MongoDB dashboard replication lag count incorrect (Thanks to user vvol for reporting this issue) PMM-6639 : Integrated update does not detect all container types PMM-6765 : Tables information tab reports \u2018table not found\u2019 with new PostgreSQL extension pg_stat_monitor PMM-6764 : Query Analytics: cannot filter items that are hidden - must use \u201cShow all\u201d PMM-6742 : Upgrade via PMM UI stalls (on yum update pmm-update ) PMM-6689 : No PostgreSQL queries or metrics in Query Analytics with PostgreSQL 13 ( postgresql_pgstatements_agent in Waiting status) PMM-6738 : PostgreSQL examples shown despite --disable-queryexamples option PMM-6535 : Unable to open \u2018Explore\u2019 in new window from Grafana menu PMM-6532 : Click-through URLs lose time ranges when redirecting to other dashboards PMM-6531 : Counter-intuitive coloring of element \u201cUpdate Stats when Metadata Queried\u201d PMM-6645 : Clean up unnecessary errors in logs ( vertamedia-clickhouse-datasource plugin) PMM-6547 : Hexagonal graph tooltip text overflows bounding box","title":"PMM 2.11.0"},{"location":"release-notes/2.11.0.html#percona-monitoring-and-management-2110","text":"Date: October 14, 2020 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance.","title":"Percona Monitoring and Management 2.11.0"},{"location":"release-notes/2.11.0.html#new-features","text":"PMM-6567 : Technical preview of new PostgreSQL extension pg_stat_monitor PMM-6515 : Link added directly to Node/Service page from Query Analytics filters, opens in new window","title":"New Features"},{"location":"release-notes/2.11.0.html#improvements","text":"PMM-6727 : Grafana plugin updates: grafana-polystat-panel=1.2.2 , grafana-piechart-panel=1.6.1 PMM-6625 : Default sort to \u201cAverage - descending\u201d on all dashboards PMM-6609 : MySQL Instances Compare & Summary dashboards: Changed metric in \u2018MySQL Internal Memory Overview\u2019 PMM-6598 : Dashboard image sharing (Share Panel): Improved wording with link to configuration instructions PMM-6557 : Update Prometheus to 2.21.0 PMM-6554 : MySQL InnoDB Details dashboard: Add \u201csync flushing\u201d to \u201cInnoDB Flushing by Type\u201d","title":"Improvements"},{"location":"release-notes/2.11.0.html#bugs-fixed","text":"PMM-4547 : MongoDB dashboard replication lag count incorrect (Thanks to user vvol for reporting this issue) PMM-6639 : Integrated update does not detect all container types PMM-6765 : Tables information tab reports \u2018table not found\u2019 with new PostgreSQL extension pg_stat_monitor PMM-6764 : Query Analytics: cannot filter items that are hidden - must use \u201cShow all\u201d PMM-6742 : Upgrade via PMM UI stalls (on yum update pmm-update ) PMM-6689 : No PostgreSQL queries or metrics in Query Analytics with PostgreSQL 13 ( postgresql_pgstatements_agent in Waiting status) PMM-6738 : PostgreSQL examples shown despite --disable-queryexamples option PMM-6535 : Unable to open \u2018Explore\u2019 in new window from Grafana menu PMM-6532 : Click-through URLs lose time ranges when redirecting to other dashboards PMM-6531 : Counter-intuitive coloring of element \u201cUpdate Stats when Metadata Queried\u201d PMM-6645 : Clean up unnecessary errors in logs ( vertamedia-clickhouse-datasource plugin) PMM-6547 : Hexagonal graph tooltip text overflows bounding box","title":"Bugs Fixed"},{"location":"release-notes/2.11.1.html","text":"Percona Monitoring and Management 2.11.1 \u00b6 Date: October 19, 2020 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. Bugs Fixed \u00b6 PMM-6782 : High CPU usage after update to 2.11.0","title":"PMM 2.11.1"},{"location":"release-notes/2.11.1.html#percona-monitoring-and-management-2111","text":"Date: October 19, 2020 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance.","title":"Percona Monitoring and Management 2.11.1"},{"location":"release-notes/2.11.1.html#bugs-fixed","text":"PMM-6782 : High CPU usage after update to 2.11.0","title":"Bugs Fixed"},{"location":"release-notes/2.12.0.html","text":"Percona Monitoring and Management 2.12.0 \u00b6 Date: December 1, 2020 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. Release Highlights \u00b6 VictoriaMetrics replaces Prometheus and is now the default data source. VictoriaMetrics supports both PUSH (client to server) and PULL metrics collection modes. ( Read more. ) PMM Client can be run as a Docker image. The \u2018Add Instance\u2019 page and forms have been redesigned and look much better. New Features \u00b6 PMM-5799 : PMM Client now available as docker image in addition to RPM, DEB and .tgz PMM-6968 : Integrated Alerting: Basic notification channels actions API Create, Read, Update, Delete PMM-6842 : VictoriaMetrics: Grafana dashboards to monitor VictoriaMetricsDB as replacement for dashboards that used to monitor Prometheus DB PMM-6395 : Replace Prometheus with VictoriaMetrics in PMM for better performance and additional functionality Improvements \u00b6 PMM-6744 : Prevent timeout of low resolution metrics in MySQL instances with many tables (~1000\u2019s) PMM-6504 : MySQL Replication Summary: MySQL Replication Delay graph not factoring in value of intentionally set SQL_Delay thus inflating time displayed PMM-6820 : pmm-admin status --wait option added to allow for configurable delay in checking status of pmm-agent PMM-6710 : pmm-admin : Allow user-specified custom \u2018group\u2019 name when adding external services PMM-6825 : Allow user to specify \u2018listen address\u2019 to pmm-agent otherwise default to 127.0.0.1 PMM-6793 : Improve user experience of \u2018add remote instance\u2019 workflow PMM-6759 : Enable Kubernetes startup probes to get status of pmm-agent using \u2018GET HTTP\u2019 verb PMM-6736 : MongoDB Instance Summary dashboard: Ensure colors for ReplSet status matches those in MongoDB ReplSet Summary dashboard for better consistency PMM-6730 : Node Overview/Summary Cleanup: Remove duplicate service type \u2018DB Service Connections\u2019 PMM-6542 : PMM Add Instance: Redesign page for more intuitive experience when adding various instance types to monitoring PMM-6518 : Update default data source name from \u2018Prometheus\u2019 to \u2018Metrics\u2019 to ensure graphs are populated correctly after upgrade to VictoriaMetrics PMM-6428 : Query Analytics dashboard - Ensure user-selected filter selections are always visible even if they don\u2019t appear in top 5 results PMM-5020 : PMM Add Remote Instance: User can specify \u2018Table Statistics Limit\u2019 for MySQL and AWS RDS MySQL to disable table stat metrics which can have an adverse impact on performance with too many tables Bugs Fixed \u00b6 PMM-6811 : MongoDB Cluster Summary: when secondary optime is newer than primary optime, lag incorrectly shows 136 years PMM-6650 : Custom queries for MySQL 8 fail on 5.x (on update to pmm-agent 2.10) (Thanks to user debug for reporting this issue) PMM-6751 : PXC/Galera dashboards: Empty service name with MySQL version < 5.6.40 PMM-5823 : PMM Server: Timeout when simultaneously generating and accessing logs via download or API PMM-4547 : MongoDB dashboard replication lag count incorrect (Thanks to user vvol for reporting this issue) PMM-7057 : MySQL Instances Overview: Many monitored instances (~250+) gives \u2018too long query\u2019 error PMM-6883 : Query Analytics: \u2018Reset All\u2019 and \u2018Show Selected\u2019 filters behaving incorrectly PMM-6686 : Query Analytics: Filters panel blank on Microsoft Edge 44.18362.449.0 PMM-6007 : PMM Server virtual appliance\u2019s IP address not shown in OVF console PMM-6754 : Query Analytics: Bad alignment of percentage values in Filters panel PMM-6752 : Query Analytics: Time interval not preserved when using filter panel dashboard shortcuts PMM-6664 : Query Analytics: No horizontal scroll bar on Explain tab PMM-6632 : Node Summary - Virtual Memory Utilization chart: incorrect formulas PMM-6537 : MySQL InnoDB Details - Logging - Group Commit Batch Size: giving incorrect description PMM-6055 : PMM Inventory - Services: \u2018Service Type\u2019 column empty when it should be \u2018External\u2019 for external services Known Issues \u00b6 PMM-7092 : Update docker pmm-server 2.11.1 to 2.12.0 results in an unhealthy container. Workaround: A folder is not created on container upgrade and will need to be created manually for one of the components. Before starting the new pmm-server 2.12.0, execute: docker exec -ti pmm-server mkdir -p /srv/victoriametrics/data docker exec -ti pmm-server chown -R pmm:pmm /srv/victoriametrics/ docker restart pmm-server","title":"PMM 2.12.0"},{"location":"release-notes/2.12.0.html#percona-monitoring-and-management-2120","text":"Date: December 1, 2020 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance.","title":"Percona Monitoring and Management 2.12.0"},{"location":"release-notes/2.12.0.html#release-highlights","text":"VictoriaMetrics replaces Prometheus and is now the default data source. VictoriaMetrics supports both PUSH (client to server) and PULL metrics collection modes. ( Read more. ) PMM Client can be run as a Docker image. The \u2018Add Instance\u2019 page and forms have been redesigned and look much better.","title":"Release Highlights"},{"location":"release-notes/2.12.0.html#new-features","text":"PMM-5799 : PMM Client now available as docker image in addition to RPM, DEB and .tgz PMM-6968 : Integrated Alerting: Basic notification channels actions API Create, Read, Update, Delete PMM-6842 : VictoriaMetrics: Grafana dashboards to monitor VictoriaMetricsDB as replacement for dashboards that used to monitor Prometheus DB PMM-6395 : Replace Prometheus with VictoriaMetrics in PMM for better performance and additional functionality","title":"New Features"},{"location":"release-notes/2.12.0.html#improvements","text":"PMM-6744 : Prevent timeout of low resolution metrics in MySQL instances with many tables (~1000\u2019s) PMM-6504 : MySQL Replication Summary: MySQL Replication Delay graph not factoring in value of intentionally set SQL_Delay thus inflating time displayed PMM-6820 : pmm-admin status --wait option added to allow for configurable delay in checking status of pmm-agent PMM-6710 : pmm-admin : Allow user-specified custom \u2018group\u2019 name when adding external services PMM-6825 : Allow user to specify \u2018listen address\u2019 to pmm-agent otherwise default to 127.0.0.1 PMM-6793 : Improve user experience of \u2018add remote instance\u2019 workflow PMM-6759 : Enable Kubernetes startup probes to get status of pmm-agent using \u2018GET HTTP\u2019 verb PMM-6736 : MongoDB Instance Summary dashboard: Ensure colors for ReplSet status matches those in MongoDB ReplSet Summary dashboard for better consistency PMM-6730 : Node Overview/Summary Cleanup: Remove duplicate service type \u2018DB Service Connections\u2019 PMM-6542 : PMM Add Instance: Redesign page for more intuitive experience when adding various instance types to monitoring PMM-6518 : Update default data source name from \u2018Prometheus\u2019 to \u2018Metrics\u2019 to ensure graphs are populated correctly after upgrade to VictoriaMetrics PMM-6428 : Query Analytics dashboard - Ensure user-selected filter selections are always visible even if they don\u2019t appear in top 5 results PMM-5020 : PMM Add Remote Instance: User can specify \u2018Table Statistics Limit\u2019 for MySQL and AWS RDS MySQL to disable table stat metrics which can have an adverse impact on performance with too many tables","title":"Improvements"},{"location":"release-notes/2.12.0.html#bugs-fixed","text":"PMM-6811 : MongoDB Cluster Summary: when secondary optime is newer than primary optime, lag incorrectly shows 136 years PMM-6650 : Custom queries for MySQL 8 fail on 5.x (on update to pmm-agent 2.10) (Thanks to user debug for reporting this issue) PMM-6751 : PXC/Galera dashboards: Empty service name with MySQL version < 5.6.40 PMM-5823 : PMM Server: Timeout when simultaneously generating and accessing logs via download or API PMM-4547 : MongoDB dashboard replication lag count incorrect (Thanks to user vvol for reporting this issue) PMM-7057 : MySQL Instances Overview: Many monitored instances (~250+) gives \u2018too long query\u2019 error PMM-6883 : Query Analytics: \u2018Reset All\u2019 and \u2018Show Selected\u2019 filters behaving incorrectly PMM-6686 : Query Analytics: Filters panel blank on Microsoft Edge 44.18362.449.0 PMM-6007 : PMM Server virtual appliance\u2019s IP address not shown in OVF console PMM-6754 : Query Analytics: Bad alignment of percentage values in Filters panel PMM-6752 : Query Analytics: Time interval not preserved when using filter panel dashboard shortcuts PMM-6664 : Query Analytics: No horizontal scroll bar on Explain tab PMM-6632 : Node Summary - Virtual Memory Utilization chart: incorrect formulas PMM-6537 : MySQL InnoDB Details - Logging - Group Commit Batch Size: giving incorrect description PMM-6055 : PMM Inventory - Services: \u2018Service Type\u2019 column empty when it should be \u2018External\u2019 for external services","title":"Bugs Fixed"},{"location":"release-notes/2.12.0.html#known-issues","text":"PMM-7092 : Update docker pmm-server 2.11.1 to 2.12.0 results in an unhealthy container. Workaround: A folder is not created on container upgrade and will need to be created manually for one of the components. Before starting the new pmm-server 2.12.0, execute: docker exec -ti pmm-server mkdir -p /srv/victoriametrics/data docker exec -ti pmm-server chown -R pmm:pmm /srv/victoriametrics/ docker restart pmm-server","title":"Known Issues"},{"location":"release-notes/2.13.0.html","text":"Percona Monitoring and Management 2.13.0 \u00b6 Date: December 29, 2020 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. Release Highlights \u00b6 Ability to monitor SSL-enabled MongoDB Allows PMM administrators to set up configured SSL certificate \u201ckeys\u201d to authenticate the connection to PMM, specifically for setting up MongoDB. This is a critical security requirement especially in large enterprise infrastructure environments. Technical Previews Caution We do not recommend the use of technical preview features in enterprise or production environments until the functionality is released as general availability (GA). While in Technical Preview status, these features are not supported by Percona Support SLA, except by Product/Engineering on a best-efforts basis. Integrated Alerting MVP A new feature in PMM to set up parameters and revive alerts about the Services and Nodes monitored by PMM. Read more on our blog and in our documentation . Node Summary/Nodes Overview dashboards: Show External services on dashboards Improves the user experience for adding and viewing external services on the Node Summary dashboard of PMM. External services means any data that can be monitored by a Prometheus exporter, for example, non-Percona supported databases like Redis, ElasticSearch, Cassandra, etc. or an organization\u2019s external application. DBaaS Preview phase 1.0 We are also releasing the first preview of DBaaS functionality; when combined with a compatible Kubernetes environment and Percona Operators, you can create Percona XtraDB or MongoDB clusters with just a few clicks. (Read more about configuration and usage .) Improvements \u00b6 PMM-5364 : Ability to monitor SSL-enabled MongoDB by passing certificate parameters in pmm-admin add command (Thanks to Hubertus Krogmann for reporting this issue) PMM-7086 : Re-mapped /prometheus/<end-point> to /victoriametrics/<end-point> but created aliases for users that still rely on the /prometheus/<end-point> in bookmarks and scripts (Thanks to Daniel Guzman Burgos for reporting this issue) PMM-6713 : Node Summary/Nodes Overview dashboards: External exporters can now be added to dashboard and shown as part of grouping of a broader service PMM-7173 : VictoriaMetrics updated to 1.50.2: Includes HTML pages vs JSON output and new functions available for alerting rules ( see all tags ) Bugs Fixed \u00b6 PMM-7054 : ProxySQL Instance Summary dashboard: no Node Metrics PMM-7092 : PMM Server Docker update from 2.11.1 to 2.12.0 leaves container in unhealthy state (Thanks to Hubertus Krogmann for reporting this issue) PMM-7208 : Confusing \u201cAccess denied\u201d message for \u2018Viewer\u2019 users on many dashboards PMM-6987 : No IP address shown in log file of OVF appliance running in headless mode PMM-7146 : MongoDB Instance Summary dashboard: ReplSet element showing metric name instead of replication set PMM-6992 : Administrators can\u2019t see user\u2019s actual IP address in Grafana profile-Preferences-Sessions PMM-6865 : Rendered dashboard images partly obscured by error message","title":"PMM 2.13.0"},{"location":"release-notes/2.13.0.html#percona-monitoring-and-management-2130","text":"Date: December 29, 2020 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance.","title":"Percona Monitoring and Management 2.13.0"},{"location":"release-notes/2.13.0.html#release-highlights","text":"Ability to monitor SSL-enabled MongoDB Allows PMM administrators to set up configured SSL certificate \u201ckeys\u201d to authenticate the connection to PMM, specifically for setting up MongoDB. This is a critical security requirement especially in large enterprise infrastructure environments. Technical Previews Caution We do not recommend the use of technical preview features in enterprise or production environments until the functionality is released as general availability (GA). While in Technical Preview status, these features are not supported by Percona Support SLA, except by Product/Engineering on a best-efforts basis. Integrated Alerting MVP A new feature in PMM to set up parameters and revive alerts about the Services and Nodes monitored by PMM. Read more on our blog and in our documentation . Node Summary/Nodes Overview dashboards: Show External services on dashboards Improves the user experience for adding and viewing external services on the Node Summary dashboard of PMM. External services means any data that can be monitored by a Prometheus exporter, for example, non-Percona supported databases like Redis, ElasticSearch, Cassandra, etc. or an organization\u2019s external application. DBaaS Preview phase 1.0 We are also releasing the first preview of DBaaS functionality; when combined with a compatible Kubernetes environment and Percona Operators, you can create Percona XtraDB or MongoDB clusters with just a few clicks. (Read more about configuration and usage .)","title":"Release Highlights"},{"location":"release-notes/2.13.0.html#improvements","text":"PMM-5364 : Ability to monitor SSL-enabled MongoDB by passing certificate parameters in pmm-admin add command (Thanks to Hubertus Krogmann for reporting this issue) PMM-7086 : Re-mapped /prometheus/<end-point> to /victoriametrics/<end-point> but created aliases for users that still rely on the /prometheus/<end-point> in bookmarks and scripts (Thanks to Daniel Guzman Burgos for reporting this issue) PMM-6713 : Node Summary/Nodes Overview dashboards: External exporters can now be added to dashboard and shown as part of grouping of a broader service PMM-7173 : VictoriaMetrics updated to 1.50.2: Includes HTML pages vs JSON output and new functions available for alerting rules ( see all tags )","title":"Improvements"},{"location":"release-notes/2.13.0.html#bugs-fixed","text":"PMM-7054 : ProxySQL Instance Summary dashboard: no Node Metrics PMM-7092 : PMM Server Docker update from 2.11.1 to 2.12.0 leaves container in unhealthy state (Thanks to Hubertus Krogmann for reporting this issue) PMM-7208 : Confusing \u201cAccess denied\u201d message for \u2018Viewer\u2019 users on many dashboards PMM-6987 : No IP address shown in log file of OVF appliance running in headless mode PMM-7146 : MongoDB Instance Summary dashboard: ReplSet element showing metric name instead of replication set PMM-6992 : Administrators can\u2019t see user\u2019s actual IP address in Grafana profile-Preferences-Sessions PMM-6865 : Rendered dashboard images partly obscured by error message","title":"Bugs Fixed"},{"location":"release-notes/2.14.0.html","text":"Percona Monitoring and Management 2.14.0 \u00b6 Date: January 28, 2021 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. Release Highlights \u00b6 Switch to push metrics by default In PMM 2.12.0, Percona replaced its metrics collection engine (formerly Prometheus) with VictoriaMetrics. Historically, PMM used a pull method with Prometheus while VictoriaMetrics can operate in either a pull or push method. When PMM 2.12.0 was released, Percona kept the default method as pull . Now with PMM 2.14.0, Percona is shifting the default to push for all newly-added instances. This blog post describes the two methods and why push benefits users. Also, here is a post by Peter Zaitzev of FAQs relating to the move to VictoriaMetrics and the push model. Documentation on the push method is here . Note : Installing the 2.14.0 or newer PMM server will change the default behavior on 2.12.0 and 2.13.0 clients from \u201cpull\u201d method to \u201cpush\u201d for any newly added services. Existing services will remain in whatever mode they were prior to upgrade. DBaaS Preview phase 1.0 (Technical Preview) In 2.13.0 we introduced Percona\u2019s Database as a Service (DBaaS) which enables non-DBAs (software architects, developers, site reliability engineers, etc.) to perform typical DBA tasks to manage an organization\u2019s database environment via user interfaces and automation orchestration. This release contains several enhancements and fixes, many directly from user feedback. Note : This capability is feature-flagged and turned off by default. Users require a variable to be passed to PMM to expose this functionality. External services presentation on node summary dashboard Improvements to the user experience for adding and viewing external services (any data that can be monitored by a Prometheus exporter such as: non-Percona supported databases like Redis, ElasticSearch, Cassandra, etc. or an organization\u2019s external application) on the Node Summary dashboard of PMM. New Features \u00b6 PMM-5765 : Ability to monitor External Services for situations where PMM Client can\u2019t be installed \u2013 Uses a new command pmm-admin add external-serverless . (See pmm-admin .) (This is a Technical Preview feature) PMM-7015 : DBaaS Preview: Create DB cluster with randomly-generated password PMM-7007 : Integrated Alerting: Ability to copy (duplicate) alert rules PMM-7006 : Integrated Alerting: Ability to delete alert rules PMM-6941 : Integrated Alerting: Ability to delete alert rule templates Improvements \u00b6 PMM-6985 : DBaaS: Ability to force unregister Kubernetes cluster PMM-7145 : \u2018Push\u2019 metrics mode is default when adding services and nodes (All agents collecting data from Services and Nodes will now use PUSH model if not specified explicitly. You will still be able to use --metrics-mode flag to use Pull metrics if needed. All previously set up agents will keep their existing mode. To change these you need to remove and re-add them.) PMM-7282 : Integrated Alerting: Ability to create rule without channels and filters PMM-7226 : Integrated Alerting: Validate parameters during rule creation/update PMM-7082 : Integrated Alerting: Severity levels are color-coded PMM-7065 : Integrated Alerting: Show rule details for items in Alert Rules list PMM-7048 : DBaaS: Simplify Cluster creation by moving Create Cluster button to earlier steps PMM-6993 : Protect against possible problems with EXPLAIN of stored functions in MySQL \u2013 We are fixing possible problems caused by an attempt to analyze queries covered in https://bugs.mysql.com/bug.php?id=67632 . Bugs Fixed \u00b6 PMM-7312 : Error when accessing Metrics data on Dashboards for large installations PMM-7310 : VictoriaMetrics consuming 100\u2019s Gb\u2019s of disk in /tmp/searchResults in PMM 2.13.0 PMM-5137 : Swagger page redirect isn\u2019t working PMM-7144 : DBaaS: Creating DB cluster with same name (Thanks to Beata Handzelova for reporting this issue) PMM-7323 : DBaaS: \u2018Remove DB Cluster from Kubernetes Cluster\u2019 removes wrong one PMM-7251 : Integrated Alerting: Error Rule with ID \"mysql_version\" not found if both Security Threat Tool and Integrated Alerting enabled PMM-7247 : DBaaS: Disk size is always 0 for Percona XtraDB cluster PMM-7178 : pg_stat_monitor integration is broken with version 0.6.0 of the plugin PMM-7169 : Old data (from Prometheus) not deleted when Retention period expires PMM-7105 : Query Analytics: no \u2018Example\u2019 or \u2018Explain\u2019 data for MariaDB PMM-7239 : Integrated Alerting: Validate Slack channel names in Notification Channels PMM-7213 : MySQL InnoDB Details dashboard: remove color-coding on \u2018Data Buffer Pool Fit\u2019 element PMM-7167 : Some panels not visible when using long time intervals (e.g. 30 days) PMM-7133 : Incorrect descriptions for data links in dashboards PMM-7103 : VictoriaMetrics build logs not deleted from PMM Server Docker image PMM-6904 : pmm-admin annotate command crashes for non-generic node types PMM-6902 : No query Examples on PostgreSQL 12 with pg_stat_monitor PMM-6838 : ProxySQL Instance Summary dashboard: Incorrect \u201cHostgroup Size\u201d formula PMM-6490 : rds_exporter crashes when more than 100 AWS RDS instances added (Thanks to https://github.com/vlinevych for fixing this) PMM-6096 : pmm-agent connection checker does not check authentication for MongoDB PMM-7303 : Disk Details, Nodes Compare dashboards: \u2018Disk Utilization\u2019 description is confusing","title":"PMM 2.14.0"},{"location":"release-notes/2.14.0.html#percona-monitoring-and-management-2140","text":"Date: January 28, 2021 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance.","title":"Percona Monitoring and Management 2.14.0"},{"location":"release-notes/2.14.0.html#release-highlights","text":"Switch to push metrics by default In PMM 2.12.0, Percona replaced its metrics collection engine (formerly Prometheus) with VictoriaMetrics. Historically, PMM used a pull method with Prometheus while VictoriaMetrics can operate in either a pull or push method. When PMM 2.12.0 was released, Percona kept the default method as pull . Now with PMM 2.14.0, Percona is shifting the default to push for all newly-added instances. This blog post describes the two methods and why push benefits users. Also, here is a post by Peter Zaitzev of FAQs relating to the move to VictoriaMetrics and the push model. Documentation on the push method is here . Note : Installing the 2.14.0 or newer PMM server will change the default behavior on 2.12.0 and 2.13.0 clients from \u201cpull\u201d method to \u201cpush\u201d for any newly added services. Existing services will remain in whatever mode they were prior to upgrade. DBaaS Preview phase 1.0 (Technical Preview) In 2.13.0 we introduced Percona\u2019s Database as a Service (DBaaS) which enables non-DBAs (software architects, developers, site reliability engineers, etc.) to perform typical DBA tasks to manage an organization\u2019s database environment via user interfaces and automation orchestration. This release contains several enhancements and fixes, many directly from user feedback. Note : This capability is feature-flagged and turned off by default. Users require a variable to be passed to PMM to expose this functionality. External services presentation on node summary dashboard Improvements to the user experience for adding and viewing external services (any data that can be monitored by a Prometheus exporter such as: non-Percona supported databases like Redis, ElasticSearch, Cassandra, etc. or an organization\u2019s external application) on the Node Summary dashboard of PMM.","title":"Release Highlights"},{"location":"release-notes/2.14.0.html#new-features","text":"PMM-5765 : Ability to monitor External Services for situations where PMM Client can\u2019t be installed \u2013 Uses a new command pmm-admin add external-serverless . (See pmm-admin .) (This is a Technical Preview feature) PMM-7015 : DBaaS Preview: Create DB cluster with randomly-generated password PMM-7007 : Integrated Alerting: Ability to copy (duplicate) alert rules PMM-7006 : Integrated Alerting: Ability to delete alert rules PMM-6941 : Integrated Alerting: Ability to delete alert rule templates","title":"New Features"},{"location":"release-notes/2.14.0.html#improvements","text":"PMM-6985 : DBaaS: Ability to force unregister Kubernetes cluster PMM-7145 : \u2018Push\u2019 metrics mode is default when adding services and nodes (All agents collecting data from Services and Nodes will now use PUSH model if not specified explicitly. You will still be able to use --metrics-mode flag to use Pull metrics if needed. All previously set up agents will keep their existing mode. To change these you need to remove and re-add them.) PMM-7282 : Integrated Alerting: Ability to create rule without channels and filters PMM-7226 : Integrated Alerting: Validate parameters during rule creation/update PMM-7082 : Integrated Alerting: Severity levels are color-coded PMM-7065 : Integrated Alerting: Show rule details for items in Alert Rules list PMM-7048 : DBaaS: Simplify Cluster creation by moving Create Cluster button to earlier steps PMM-6993 : Protect against possible problems with EXPLAIN of stored functions in MySQL \u2013 We are fixing possible problems caused by an attempt to analyze queries covered in https://bugs.mysql.com/bug.php?id=67632 .","title":"Improvements"},{"location":"release-notes/2.14.0.html#bugs-fixed","text":"PMM-7312 : Error when accessing Metrics data on Dashboards for large installations PMM-7310 : VictoriaMetrics consuming 100\u2019s Gb\u2019s of disk in /tmp/searchResults in PMM 2.13.0 PMM-5137 : Swagger page redirect isn\u2019t working PMM-7144 : DBaaS: Creating DB cluster with same name (Thanks to Beata Handzelova for reporting this issue) PMM-7323 : DBaaS: \u2018Remove DB Cluster from Kubernetes Cluster\u2019 removes wrong one PMM-7251 : Integrated Alerting: Error Rule with ID \"mysql_version\" not found if both Security Threat Tool and Integrated Alerting enabled PMM-7247 : DBaaS: Disk size is always 0 for Percona XtraDB cluster PMM-7178 : pg_stat_monitor integration is broken with version 0.6.0 of the plugin PMM-7169 : Old data (from Prometheus) not deleted when Retention period expires PMM-7105 : Query Analytics: no \u2018Example\u2019 or \u2018Explain\u2019 data for MariaDB PMM-7239 : Integrated Alerting: Validate Slack channel names in Notification Channels PMM-7213 : MySQL InnoDB Details dashboard: remove color-coding on \u2018Data Buffer Pool Fit\u2019 element PMM-7167 : Some panels not visible when using long time intervals (e.g. 30 days) PMM-7133 : Incorrect descriptions for data links in dashboards PMM-7103 : VictoriaMetrics build logs not deleted from PMM Server Docker image PMM-6904 : pmm-admin annotate command crashes for non-generic node types PMM-6902 : No query Examples on PostgreSQL 12 with pg_stat_monitor PMM-6838 : ProxySQL Instance Summary dashboard: Incorrect \u201cHostgroup Size\u201d formula PMM-6490 : rds_exporter crashes when more than 100 AWS RDS instances added (Thanks to https://github.com/vlinevych for fixing this) PMM-6096 : pmm-agent connection checker does not check authentication for MongoDB PMM-7303 : Disk Details, Nodes Compare dashboards: \u2018Disk Utilization\u2019 description is confusing","title":"Bugs Fixed"},{"location":"release-notes/2.15.0.html","text":"Percona Monitoring and Management 2.15.0 \u00b6 Date: March 01, 2021 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. Release Highlights \u00b6 PMM 1 vs. 2 Parity Disable collectors during adding node/service to monitoring With this feature users can disable any collector used by PMM to get metrics. When metrics cannot be collected or are no longer needed, disabling the collector(s) prevents PMM from flooding logs and saves infrastructure resources. Our vision for PMM collectors is to provide \u201cstop from collecting\u201d functionality to prevent possible harm to the user environment. This \u201cdisable\u201d feature is an initial step towards the ideal functionality. The full and flexible management for \u201cWhat metrics to collect and in what resolution\u201d is slated for future releases. External services monitoring Since PMM 1.4.0, users had the ability to monitor external services Percona didn\u2019t currently support (e.g., Redis). This blog article from 2018 nicely described external services monitoring at that time. (At that time Percona was not natively supporting a PostgreSQL monitoring service and so this was listed as an external service. Today, PostgreSQL is natively supported by PMM.) Until now, PMM 2.x didn\u2019t support external services monitoring. With this release, any non-natively supported by PMM service will now become supported with external services monitoring. You can see the list of possible exporters to be used in https://prometheus.io/docs/instrumenting/exporters/ . Natively-supported services will continue to deliver an expanded set of metrics and insights. Provide summary information for systems ( pt-*-summary actions ) With the addition of pt-*-summary in PMM 2, users can now view summary information about services and nodes on PMM\u2019s dashboard. This summary information is in the industry common format of pt-*-summary tools output to simplify portability of this data. This format will also be preserved in the snapshot of the dashboard shared with Percona Support to simplify investigations of issues. Note: pt-*-summary includes formats for: pt-mysql-summary pt-mongodb-summary pt-pg-summary pt-summary HAProxy support by PMM Users are able to add HAProxy Services for monitoring in PMM2. The support level of them in PMM will be the same we have for ProxySQL, so they will be presented in Inventory and on Dashboard. This will allow users who use HAProxy in their HA configuration to also have this component monitored by PMM. In future releases PMM will start use HAProxy by default for the DBaaS feature and will also use this functionality to monitor HAProxy. DBaaS Preview improvements (Technical Preview) From now you will be able to see the progress of internal steps the system makes when executing some operations with DBaaS. The Progress Bar will not be time-related and will present only steps. The Progress Bar component will also reflect the K8s/Operator-related errors to the user, so in the case of errors, you will have the error text on the UI, and no need to use K8s tools to see the error. With the same UI, you will be able to see the latest logs from K8s so they will have even more information about why the error happened. Known Limitations: The progress bar will not provide valuable information for the Delete operation (will be in a later version when we\u2019ll change the API with Operators Team), Operation of DB Cluster Modification will have \u201cstrange\u201d behavior and will start changes from non-zero values of steps. (This will be modified after API changes.) New Features \u00b6 PMM-4172 , PMM-4306 , PMM-5784 , PMM-7177 : Services and Nodes Summary presentation. Present information about DB\u2019s and Node status using pt-mysql-summary , pt-mongodb-summary , pt-pg-summary outputs (in API and on Dashboards). PMM-7123 : Ability to add External Services via the UI in PMM server. PMM-6711 : Add external-group flag for pmm-admin inventory commands for simpler work with External services. PMM-7405 : Check connection response format when adding External Service to monitoring. PMM-6797 : HAProxy monitoring: Ability to add HAProxy services with pmm-admin [inventory] add [service] haproxy command. PMM-7487 : HAProxy monitoring: Check connection to HAProxy services when adding them for monitoring. PMM-7496 : HAProxy monitoring: New HAProxy PXC dashboards. PMM-6943 : HAProxy monitoring: Show HAProxy type services in PMM Inventory. PMM-6924 : Integrated Alerting: Show \u2018breadcrumbs\u2019 navigation aid on non-dashboard pages as well as Grafana dashboard pages. PMM-7294 : Integrated Alerting: Pagination for viewing large numbers of Alert Rules. PMM-7417 : Security Threat Tool: Show list of all available security checks. PMM-7418 : Security Threat Tool: Ability to disable specific security checks. PMM-7419 : DBaaS: Ability to see DB Cluster creation/modification logs. PMM-7266 : DBaaS: Cluster creation progress bar \u2013 You can now see the progress of DBaaS DB cluster creation. (The progress bar is based on the number of back-end technical steps, not the time required to perform the tasks.) Improvements \u00b6 PMM-4679 : Docker: :latest tag for pmm-server and pmm-client images has been moved from v1 latest release to v2 latest release. Note : use of the latest tag is not recommended in production environments, instead use :2 tag. PMM-7472 : Remove Prometheus data source \u2013 If you were using custom dashboards with a specified data source (not using empty to use default one) you may need to edit your dashboards to use the proper data source. PMM is no longer using Prometheus but uses compatible storage for metrics from VictoriaMetrics. We renamed the data source to be more technology-agnostic. PMM-6695 : Software update: Grafana 7.1.3 to 7.3.7 (See What\u2019s new in Grafana 7.2 and What\u2019s new in Grafana 7.3 .) PMM-7471 : Software update: VictoriaMetrics 1.52.0 to 1.53.1 (See VictoriaMetrics 1.53.0 and VictoriaMetrics 1.53.1 .) PMM-6693 : API keys usage \u2013 PMM users can now use API keys (generated in Grafana UI) for interaction with PMM server instead of username/password pairs. The API key should have the same level of access (Admin or Viewer) as is required for username/password pairs. PMM-7240 : DBaaS: Change from Dashboard to Grafana Page \u2013 We changed the DBaaS page from a Grafana Dashboard to a Grafana Page to be better aligned with the DBaaS enable/disable status and avoid confusion when DBaaS is disabled. PMM-7328 : Security Threat Tool: Download and run checks when activated, immediately, repeating every 24 hours thereafter (Previously, downloading and running new checks happened every 24 hours but the cycle didn\u2019t begin when STT was activated.) PMM-7329 : Security Threat Tool: Hide check results tab if STT is disabled. PMM-7331 : Security Threat Tool: Failed checks have \u2018Read more\u2019 links with helpful content. PMM-7422 : Security Threat Tool: View all active and silenced alerts. PMM-7257 , PMM-7433 : Integrated Alerting: Easier-to-read rule details in Alert Rules list (API and UI presentation). PMM-7259 : Integrated Alerting: Better UI error reporting for disabled Integrated Alerting. (Hint to users how to enable it.) PMM-5533 : Better indentation of columns in pmm-admin list output. PMM-5888 : Improve pmm-admin --help descriptions for external services. Bugs Fixed \u00b6 PMM-5837 : pmm-agent reports \u201cMalformed DSN\u201d error when adding PostgreSQL instance with a PMM user password containing = (equals sign) (Thanks to Alexandre Barth for reporting this issue). PMM-5969 : Removing Services or Nodes with pmm-admin ... --force mode does not stop running agents, VictoriaMetrics continues collecting data from exporters. PMM-6685 : In low screen resolutions Services submenu wraps, becomes obscured, and can\u2019t be accessed. PMM-6681 : Not all PMM admin users can download diagnostic logs, only those with Grafana admin rights. PMM-7227 : Table stats metrics not being collected in instances with millions of tables. PMM-7426 : vmagent continually restarts, blocking comms between pmm-agent & pmm-managed \u2013 Users running multiple services on the same PMM agent in \u2018push\u2019 mode could face this issue when restarting the agent after bulk-adding services. PMM-6636 : Dashboards: MySQL Replication Summary: \u2018Binlog Size\u2019, \u2018Binlog Data Written Hourly\u2019, \u2018Node\u2019 not being charted when the instance is RDS. PMM-7325 : Dashboards: MySQL User Details: user labels unreadable with high number (>20) of users (Thanks to Andrei Fedorov for reporting this issue). PMM-7416 : Dashboards: PostgreSQL Instance Summary: Some panels (e.g. Tuple) not using selected database. PMM-7235 : Integrated Alerting: Filtered out alerts are shown in the UI as firing. PMM-7324 : Integrated Alerting: Add Pager Duty Notification Channel: after user pastes copied key Add button is not enabled. PMM-7346 : Integrated Alerting: It is possible to create Alert Rule with negative duration time. PMM-7366 : Integrated Alerting: Entities (e.g. templates, channels, rules) are in inconsistent states. PMM-7467 : Integrated Alerting: < (less-than symbol) wrongly interpreted by Alert templates (as &lt; ). PMM-7591 : Integrated Alerting: User can not receive notifications on email after password update. PMM-7343 : Security Threat Tool: Check results show previously failed checks after STT re-enabled. PMM-7250 : DBaaS: Confusing error \u201cCannot get PSMDB/PXC cluster\u201d appears after removing DB cluster. PMM-7193 : DBaaS: Number of Nodes can be set as float. PMM-7349 : DBaaS: Host and Password occasionally disappearing from Connection column.","title":"PMM 2.15.0"},{"location":"release-notes/2.15.0.html#percona-monitoring-and-management-2150","text":"Date: March 01, 2021 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance.","title":"Percona Monitoring and Management 2.15.0"},{"location":"release-notes/2.15.0.html#release-highlights","text":"PMM 1 vs. 2 Parity Disable collectors during adding node/service to monitoring With this feature users can disable any collector used by PMM to get metrics. When metrics cannot be collected or are no longer needed, disabling the collector(s) prevents PMM from flooding logs and saves infrastructure resources. Our vision for PMM collectors is to provide \u201cstop from collecting\u201d functionality to prevent possible harm to the user environment. This \u201cdisable\u201d feature is an initial step towards the ideal functionality. The full and flexible management for \u201cWhat metrics to collect and in what resolution\u201d is slated for future releases. External services monitoring Since PMM 1.4.0, users had the ability to monitor external services Percona didn\u2019t currently support (e.g., Redis). This blog article from 2018 nicely described external services monitoring at that time. (At that time Percona was not natively supporting a PostgreSQL monitoring service and so this was listed as an external service. Today, PostgreSQL is natively supported by PMM.) Until now, PMM 2.x didn\u2019t support external services monitoring. With this release, any non-natively supported by PMM service will now become supported with external services monitoring. You can see the list of possible exporters to be used in https://prometheus.io/docs/instrumenting/exporters/ . Natively-supported services will continue to deliver an expanded set of metrics and insights. Provide summary information for systems ( pt-*-summary actions ) With the addition of pt-*-summary in PMM 2, users can now view summary information about services and nodes on PMM\u2019s dashboard. This summary information is in the industry common format of pt-*-summary tools output to simplify portability of this data. This format will also be preserved in the snapshot of the dashboard shared with Percona Support to simplify investigations of issues. Note: pt-*-summary includes formats for: pt-mysql-summary pt-mongodb-summary pt-pg-summary pt-summary HAProxy support by PMM Users are able to add HAProxy Services for monitoring in PMM2. The support level of them in PMM will be the same we have for ProxySQL, so they will be presented in Inventory and on Dashboard. This will allow users who use HAProxy in their HA configuration to also have this component monitored by PMM. In future releases PMM will start use HAProxy by default for the DBaaS feature and will also use this functionality to monitor HAProxy. DBaaS Preview improvements (Technical Preview) From now you will be able to see the progress of internal steps the system makes when executing some operations with DBaaS. The Progress Bar will not be time-related and will present only steps. The Progress Bar component will also reflect the K8s/Operator-related errors to the user, so in the case of errors, you will have the error text on the UI, and no need to use K8s tools to see the error. With the same UI, you will be able to see the latest logs from K8s so they will have even more information about why the error happened. Known Limitations: The progress bar will not provide valuable information for the Delete operation (will be in a later version when we\u2019ll change the API with Operators Team), Operation of DB Cluster Modification will have \u201cstrange\u201d behavior and will start changes from non-zero values of steps. (This will be modified after API changes.)","title":"Release Highlights"},{"location":"release-notes/2.15.0.html#new-features","text":"PMM-4172 , PMM-4306 , PMM-5784 , PMM-7177 : Services and Nodes Summary presentation. Present information about DB\u2019s and Node status using pt-mysql-summary , pt-mongodb-summary , pt-pg-summary outputs (in API and on Dashboards). PMM-7123 : Ability to add External Services via the UI in PMM server. PMM-6711 : Add external-group flag for pmm-admin inventory commands for simpler work with External services. PMM-7405 : Check connection response format when adding External Service to monitoring. PMM-6797 : HAProxy monitoring: Ability to add HAProxy services with pmm-admin [inventory] add [service] haproxy command. PMM-7487 : HAProxy monitoring: Check connection to HAProxy services when adding them for monitoring. PMM-7496 : HAProxy monitoring: New HAProxy PXC dashboards. PMM-6943 : HAProxy monitoring: Show HAProxy type services in PMM Inventory. PMM-6924 : Integrated Alerting: Show \u2018breadcrumbs\u2019 navigation aid on non-dashboard pages as well as Grafana dashboard pages. PMM-7294 : Integrated Alerting: Pagination for viewing large numbers of Alert Rules. PMM-7417 : Security Threat Tool: Show list of all available security checks. PMM-7418 : Security Threat Tool: Ability to disable specific security checks. PMM-7419 : DBaaS: Ability to see DB Cluster creation/modification logs. PMM-7266 : DBaaS: Cluster creation progress bar \u2013 You can now see the progress of DBaaS DB cluster creation. (The progress bar is based on the number of back-end technical steps, not the time required to perform the tasks.)","title":"New Features"},{"location":"release-notes/2.15.0.html#improvements","text":"PMM-4679 : Docker: :latest tag for pmm-server and pmm-client images has been moved from v1 latest release to v2 latest release. Note : use of the latest tag is not recommended in production environments, instead use :2 tag. PMM-7472 : Remove Prometheus data source \u2013 If you were using custom dashboards with a specified data source (not using empty to use default one) you may need to edit your dashboards to use the proper data source. PMM is no longer using Prometheus but uses compatible storage for metrics from VictoriaMetrics. We renamed the data source to be more technology-agnostic. PMM-6695 : Software update: Grafana 7.1.3 to 7.3.7 (See What\u2019s new in Grafana 7.2 and What\u2019s new in Grafana 7.3 .) PMM-7471 : Software update: VictoriaMetrics 1.52.0 to 1.53.1 (See VictoriaMetrics 1.53.0 and VictoriaMetrics 1.53.1 .) PMM-6693 : API keys usage \u2013 PMM users can now use API keys (generated in Grafana UI) for interaction with PMM server instead of username/password pairs. The API key should have the same level of access (Admin or Viewer) as is required for username/password pairs. PMM-7240 : DBaaS: Change from Dashboard to Grafana Page \u2013 We changed the DBaaS page from a Grafana Dashboard to a Grafana Page to be better aligned with the DBaaS enable/disable status and avoid confusion when DBaaS is disabled. PMM-7328 : Security Threat Tool: Download and run checks when activated, immediately, repeating every 24 hours thereafter (Previously, downloading and running new checks happened every 24 hours but the cycle didn\u2019t begin when STT was activated.) PMM-7329 : Security Threat Tool: Hide check results tab if STT is disabled. PMM-7331 : Security Threat Tool: Failed checks have \u2018Read more\u2019 links with helpful content. PMM-7422 : Security Threat Tool: View all active and silenced alerts. PMM-7257 , PMM-7433 : Integrated Alerting: Easier-to-read rule details in Alert Rules list (API and UI presentation). PMM-7259 : Integrated Alerting: Better UI error reporting for disabled Integrated Alerting. (Hint to users how to enable it.) PMM-5533 : Better indentation of columns in pmm-admin list output. PMM-5888 : Improve pmm-admin --help descriptions for external services.","title":"Improvements"},{"location":"release-notes/2.15.0.html#bugs-fixed","text":"PMM-5837 : pmm-agent reports \u201cMalformed DSN\u201d error when adding PostgreSQL instance with a PMM user password containing = (equals sign) (Thanks to Alexandre Barth for reporting this issue). PMM-5969 : Removing Services or Nodes with pmm-admin ... --force mode does not stop running agents, VictoriaMetrics continues collecting data from exporters. PMM-6685 : In low screen resolutions Services submenu wraps, becomes obscured, and can\u2019t be accessed. PMM-6681 : Not all PMM admin users can download diagnostic logs, only those with Grafana admin rights. PMM-7227 : Table stats metrics not being collected in instances with millions of tables. PMM-7426 : vmagent continually restarts, blocking comms between pmm-agent & pmm-managed \u2013 Users running multiple services on the same PMM agent in \u2018push\u2019 mode could face this issue when restarting the agent after bulk-adding services. PMM-6636 : Dashboards: MySQL Replication Summary: \u2018Binlog Size\u2019, \u2018Binlog Data Written Hourly\u2019, \u2018Node\u2019 not being charted when the instance is RDS. PMM-7325 : Dashboards: MySQL User Details: user labels unreadable with high number (>20) of users (Thanks to Andrei Fedorov for reporting this issue). PMM-7416 : Dashboards: PostgreSQL Instance Summary: Some panels (e.g. Tuple) not using selected database. PMM-7235 : Integrated Alerting: Filtered out alerts are shown in the UI as firing. PMM-7324 : Integrated Alerting: Add Pager Duty Notification Channel: after user pastes copied key Add button is not enabled. PMM-7346 : Integrated Alerting: It is possible to create Alert Rule with negative duration time. PMM-7366 : Integrated Alerting: Entities (e.g. templates, channels, rules) are in inconsistent states. PMM-7467 : Integrated Alerting: < (less-than symbol) wrongly interpreted by Alert templates (as &lt; ). PMM-7591 : Integrated Alerting: User can not receive notifications on email after password update. PMM-7343 : Security Threat Tool: Check results show previously failed checks after STT re-enabled. PMM-7250 : DBaaS: Confusing error \u201cCannot get PSMDB/PXC cluster\u201d appears after removing DB cluster. PMM-7193 : DBaaS: Number of Nodes can be set as float. PMM-7349 : DBaaS: Host and Password occasionally disappearing from Connection column.","title":"Bugs Fixed"},{"location":"release-notes/2.15.1.html","text":"Percona Monitoring and Management 2.15.1 \u00b6 Date: March 18, 2021 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. Release Highlights \u00b6 This patch release fixes performance issues discovered in systems, together with other small fixes. Bugs Fixed \u00b6 PMM-7635 : Fix high CPU consumption by Grafana server after upgrade by docker replacement to 2.15.0 with large numbers of services in \u2018push\u2019 mode. PMM-7713 : Fix high CPU and Memory consumption by Victoria Metrics after upgrade by docker replacement to 2.15.0 with large numbers of services in \u2018pull\u2019 mode. PMM-7470 : MongoDB exporter IndexStatsCollections is assigned values from wrong flag (intended for 2.15.0, omitted due to missing merge cutoff) (Thanks to Tim for reporting this issue). PMM-1531 : Metrics not being collected due to rename of MySQL 8 information schema tables.","title":"PMM 2.15.1"},{"location":"release-notes/2.15.1.html#percona-monitoring-and-management-2151","text":"Date: March 18, 2021 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance.","title":"Percona Monitoring and Management 2.15.1"},{"location":"release-notes/2.15.1.html#release-highlights","text":"This patch release fixes performance issues discovered in systems, together with other small fixes.","title":"Release Highlights"},{"location":"release-notes/2.15.1.html#bugs-fixed","text":"PMM-7635 : Fix high CPU consumption by Grafana server after upgrade by docker replacement to 2.15.0 with large numbers of services in \u2018push\u2019 mode. PMM-7713 : Fix high CPU and Memory consumption by Victoria Metrics after upgrade by docker replacement to 2.15.0 with large numbers of services in \u2018pull\u2019 mode. PMM-7470 : MongoDB exporter IndexStatsCollections is assigned values from wrong flag (intended for 2.15.0, omitted due to missing merge cutoff) (Thanks to Tim for reporting this issue). PMM-1531 : Metrics not being collected due to rename of MySQL 8 information schema tables.","title":"Bugs Fixed"},{"location":"release-notes/2.16.0.html","text":"Percona Monitoring and Management 2.16.0 \u00b6 Date: April 15, 2021 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. Important note for users of PMM 2.16.0 \u00b6 If you started using PMM from version 2.16 and have already upgraded to 2.17, 2.18, or 2.19, you might have some problems with PMM Server monitoring, Remove Monitoring, or RDS/Azure monitoring. If you experience a problem, we recommend you upgrade and replace the Docker container by following the official instructions for an upgrade here: https://www.percona.com/doc/percona-monitoring-and-management/2.x/setting-up/server/docker.html#upgrade . If you can\u2019t do this, then you need to perform additional steps after upgrading to 2.20. Enter the container: docker exec -it pmm-server bash Roll back pmm2-client package to stable version: yum downgrade -y pmm2-client Release Highlights \u00b6 Amazon RDS PostgreSQL monitoring AWS monitoring in PMM now covers PostgreSQL RDS and PostgreSQL Aurora types. PMM will include them in a Discovery UI where they can be added which will result in node related metrics as well as PostgreSQL database performance metrics. Before this release, this was available only to MySQL-related instances from Amazon RDS. Azure Discovery and Node metrics extraction Technical Preview : PMM will have the same level of support for Microsoft Azure Database as a Service (DBaaS) as we have for AWS\u2019s DBaaS (RDS/Aurora on MySQL or PostgreSQL). You will be able to easily discover and add Azure databases for monitoring by PMM complete with node-level monitoring. This feature is available only if you explicitly activate it on the PMM Settings page. Deactivating it will not remove added services from monitoring, but will just hide the ability to discover and add new Microsoft Azure Services. (This is a feature technical preview because we want to release it as soon as possible to get feedback from users. We are expecting to do more work on this feature to make it more API and resource efficient.) Security Threat Tool Scheduling - Manage execution and execution intervals Security Threat Tool users are now able to control the Security Check execution time intervals for groups of checks, move checks between groups, and disable individual checks if necessary. Support for pg_stat_monitor 0.8 Added compatibility with pg_stat_monitor plugin v 0.8.0 . This is not exposing the new features for the plugin in PMM yet, but ensures Query Analytics metrics are collected to the same degree it was with version 0.6.0 of the plugin. Consistent support of Technical Preview Features Reworked the PMM Settings page to make it clear what features are in Technical Preview vs General Availability (GA) and to simplify activation/deactivation of technical preview features. We also provide a better definition of what a Technical Preview is. Migration of Settings and other service pages in PMM from Grafana dashboards The PMM Settings page and several others (including Add Instance and Inventory ) are being converted to Grafana pages and will no longer be presented as dashboards. Additionally, we\u2019re moving the menu to the sidebar navigation for consistency and more flexibility compared to the older menu structure. Integrated Alerting improvements We released the next stage of improvements in Integrated Alerting functionality of PMM to simplify the usage of the feature. Together with improvements, we continue fixing known bugs in this feature. [DBaaS] Resource planning and prediction (Resource calculator) Technical preview : While creating a DB cluster a user can see a prediction of the resources this cluster will consume with all components as well as the current total and available resources in the K8s. Users will be warned that if they attempt to create a DB cluster it may be unsuccessful because of available resources in the K8s. [DBaaS] PSMDB 1.7.0 operator support DBaaS in PMM will be using the recently-released Percona Kubernetes Operator for Percona Server for MongoDB 1.7.0 to create MongoDB clusters. New Features \u00b6 PMM-7313 , PMM-7610 : Ability to discover and monitor Amazon RDS PostgreSQL instances with collecting PostgreSQL and RDS node metrics (Thanks to Daniel Guzman Burgos for reporting this issue). PMM-7345 : Expose metrics for all available databases on a PMM monitored PostgreSQL server. PMM-7344 : Update postgres_exporter version from 0.4.6 to 0.8.0. (See the full list of improvements in the changelog .) PMM-7767 , PMM-7696 : Implement feature flag to enable Microsoft Azure monitoring. Users can use the UI or set an environment variable ( ENABLE_AZUREDISCOVER=1 ) during container creation. PMM-7684 , PMM-7498 : Ability to discover running and supported Microsoft Azure Databases instances in a provided account. PMM-7678 , PMM-7679 , PMM-7676 , PMM-7499 , PMM-7691 : Prepare, modify and use azure_exporter to collect Node related metrics. PMM-7681 : Use Microsoft Azure metrics on Node/OS-related dashboards to show the metrics on panels. PMM-7339 : Security Threat Tool: Ability to execute security checks individually and on-demand. PMM-7451 , PMM-7337 : Security Threat Tool: Ability to change intervals for security checks on the PMM Settings page. PMM-7772 , PMM-7338 : Security Threat Tool: Ability to change default execution interval per check. PMM-7336 : Security Threat Tool: Execute checks based on execution interval they belong to. PMM-7335 : Security Threat Tool: Ship security check files with predefined execution interval. PMM-7748 : Add an additional experimental menu for Dashboards on the left side panel. PMM-7688 : Unify UX and layout of all PMM specific pages like Settings, Add Instance etc. PMM-7687 : Modify links in menus to ensure both menus are working as expected after dashboard URL change. PMM-7705 : Simplify display of features in technical preview to easily identify them and their current state. PMM-7522 , PMM-7511 : Integrated Alerting: Improve Notification Channels UX by Pagination for the Notification list. PMM-7521 , PMM-7510 : Integrated Alerting: Improve Alert Rule Templates UX by Pagination on Rule Templates list. PMM-7652 , PMM-7674 , PMM-7503 , PMM-7486 : DBaaS: While creating the DB cluster see all and available resources in the K8s cluster, such as Disk, CPU & Memory. PMM-7508 , PMM-7488 : DBaaS: See predicted resource usage for selected DB Cluster configuration. PMM-7364 : DBaaS: Show warning before starting creating the cluster if there are not enough resources in the K8s cluster to create DB Cluster with requested configuration. PMM-7580 , PMM-7359 : DBaaS: Users can select the database version to use during DB Cluster creation. Improvements \u00b6 PMM-7506 : Security Threat Tool: Reduce False Positives due to Roles automatically created in PXC with no password but cannot be used to login. PMM-7569 : Make PMM compatible with pg_stat_monitor 0.8 release. PMM-7571 : Modified Percona Platform Account registration flow from PMM server UI. PMM-7513 : Integrated Alerting: Ability to see default values and Threshold values during the Alert Rule creation. PMM-7461 : Integrated Alerting: Improve UX of tables presentation and loading on UI. PMM-7375 : Integrated Alerting: Inform users about the template that they are editing and warn them about the limitations. PMM-7260 : Integrated Alerting: Make it clearer what rule is being edited. Bugs Fixed \u00b6 PMM-7131 , PMM-7555 : QAN for PostgreSQL attempts to connect to a database with the same name as the username. (Thanks to Daniel Guzman Burgos for reporting this issue) PMM-7481 : Query Analytics is not showing \u201cQuery with Errors\u201d in the Profile section. PMM-7464 : NGINX misconfiguration leads to log storm in push mode PMM-7434 : Integrated Alerting: Unknown parameters [threshold] error during Add/Update Alert Rule. PMM-7231 : Integrated Alerting: Disabling channels does nothing. PMM-7379 : Integrated Alerting: Can not edit Alert Rule Name through API. PMM-7232 : Integrated Alerting: Disabling IA does not disable rules evaluation and notifications sending. PMM-7119 : Integrated Alerting: Update error notification for adding/update Alert rule Template \u2013 There was inconsistent behavior if you tried to add a new Rule Template with an already-used name. PMM-7543 : Integrated Alerting: selected section disappears from a breadcrumb after clicking the tab for a second time. PMM-7766 : DBaaS: PMM Upgrade breaks DBaaS get credentials method. PMM-7351 : DBaaS: Safari does not accept float numbers as a custom option in the \u201cCreate Cluster\u201d dialogue. PMM-7701 : DBaaS: PSMDB clusters stuck in initializing due to special characters in secrets.","title":"PMM 2.16.0"},{"location":"release-notes/2.16.0.html#percona-monitoring-and-management-2160","text":"Date: April 15, 2021 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance.","title":"Percona Monitoring and Management 2.16.0"},{"location":"release-notes/2.16.0.html#important-note-for-users-of-pmm-2160","text":"If you started using PMM from version 2.16 and have already upgraded to 2.17, 2.18, or 2.19, you might have some problems with PMM Server monitoring, Remove Monitoring, or RDS/Azure monitoring. If you experience a problem, we recommend you upgrade and replace the Docker container by following the official instructions for an upgrade here: https://www.percona.com/doc/percona-monitoring-and-management/2.x/setting-up/server/docker.html#upgrade . If you can\u2019t do this, then you need to perform additional steps after upgrading to 2.20. Enter the container: docker exec -it pmm-server bash Roll back pmm2-client package to stable version: yum downgrade -y pmm2-client","title":"Important note for users of PMM 2.16.0"},{"location":"release-notes/2.16.0.html#release-highlights","text":"Amazon RDS PostgreSQL monitoring AWS monitoring in PMM now covers PostgreSQL RDS and PostgreSQL Aurora types. PMM will include them in a Discovery UI where they can be added which will result in node related metrics as well as PostgreSQL database performance metrics. Before this release, this was available only to MySQL-related instances from Amazon RDS. Azure Discovery and Node metrics extraction Technical Preview : PMM will have the same level of support for Microsoft Azure Database as a Service (DBaaS) as we have for AWS\u2019s DBaaS (RDS/Aurora on MySQL or PostgreSQL). You will be able to easily discover and add Azure databases for monitoring by PMM complete with node-level monitoring. This feature is available only if you explicitly activate it on the PMM Settings page. Deactivating it will not remove added services from monitoring, but will just hide the ability to discover and add new Microsoft Azure Services. (This is a feature technical preview because we want to release it as soon as possible to get feedback from users. We are expecting to do more work on this feature to make it more API and resource efficient.) Security Threat Tool Scheduling - Manage execution and execution intervals Security Threat Tool users are now able to control the Security Check execution time intervals for groups of checks, move checks between groups, and disable individual checks if necessary. Support for pg_stat_monitor 0.8 Added compatibility with pg_stat_monitor plugin v 0.8.0 . This is not exposing the new features for the plugin in PMM yet, but ensures Query Analytics metrics are collected to the same degree it was with version 0.6.0 of the plugin. Consistent support of Technical Preview Features Reworked the PMM Settings page to make it clear what features are in Technical Preview vs General Availability (GA) and to simplify activation/deactivation of technical preview features. We also provide a better definition of what a Technical Preview is. Migration of Settings and other service pages in PMM from Grafana dashboards The PMM Settings page and several others (including Add Instance and Inventory ) are being converted to Grafana pages and will no longer be presented as dashboards. Additionally, we\u2019re moving the menu to the sidebar navigation for consistency and more flexibility compared to the older menu structure. Integrated Alerting improvements We released the next stage of improvements in Integrated Alerting functionality of PMM to simplify the usage of the feature. Together with improvements, we continue fixing known bugs in this feature. [DBaaS] Resource planning and prediction (Resource calculator) Technical preview : While creating a DB cluster a user can see a prediction of the resources this cluster will consume with all components as well as the current total and available resources in the K8s. Users will be warned that if they attempt to create a DB cluster it may be unsuccessful because of available resources in the K8s. [DBaaS] PSMDB 1.7.0 operator support DBaaS in PMM will be using the recently-released Percona Kubernetes Operator for Percona Server for MongoDB 1.7.0 to create MongoDB clusters.","title":"Release Highlights"},{"location":"release-notes/2.16.0.html#new-features","text":"PMM-7313 , PMM-7610 : Ability to discover and monitor Amazon RDS PostgreSQL instances with collecting PostgreSQL and RDS node metrics (Thanks to Daniel Guzman Burgos for reporting this issue). PMM-7345 : Expose metrics for all available databases on a PMM monitored PostgreSQL server. PMM-7344 : Update postgres_exporter version from 0.4.6 to 0.8.0. (See the full list of improvements in the changelog .) PMM-7767 , PMM-7696 : Implement feature flag to enable Microsoft Azure monitoring. Users can use the UI or set an environment variable ( ENABLE_AZUREDISCOVER=1 ) during container creation. PMM-7684 , PMM-7498 : Ability to discover running and supported Microsoft Azure Databases instances in a provided account. PMM-7678 , PMM-7679 , PMM-7676 , PMM-7499 , PMM-7691 : Prepare, modify and use azure_exporter to collect Node related metrics. PMM-7681 : Use Microsoft Azure metrics on Node/OS-related dashboards to show the metrics on panels. PMM-7339 : Security Threat Tool: Ability to execute security checks individually and on-demand. PMM-7451 , PMM-7337 : Security Threat Tool: Ability to change intervals for security checks on the PMM Settings page. PMM-7772 , PMM-7338 : Security Threat Tool: Ability to change default execution interval per check. PMM-7336 : Security Threat Tool: Execute checks based on execution interval they belong to. PMM-7335 : Security Threat Tool: Ship security check files with predefined execution interval. PMM-7748 : Add an additional experimental menu for Dashboards on the left side panel. PMM-7688 : Unify UX and layout of all PMM specific pages like Settings, Add Instance etc. PMM-7687 : Modify links in menus to ensure both menus are working as expected after dashboard URL change. PMM-7705 : Simplify display of features in technical preview to easily identify them and their current state. PMM-7522 , PMM-7511 : Integrated Alerting: Improve Notification Channels UX by Pagination for the Notification list. PMM-7521 , PMM-7510 : Integrated Alerting: Improve Alert Rule Templates UX by Pagination on Rule Templates list. PMM-7652 , PMM-7674 , PMM-7503 , PMM-7486 : DBaaS: While creating the DB cluster see all and available resources in the K8s cluster, such as Disk, CPU & Memory. PMM-7508 , PMM-7488 : DBaaS: See predicted resource usage for selected DB Cluster configuration. PMM-7364 : DBaaS: Show warning before starting creating the cluster if there are not enough resources in the K8s cluster to create DB Cluster with requested configuration. PMM-7580 , PMM-7359 : DBaaS: Users can select the database version to use during DB Cluster creation.","title":"New Features"},{"location":"release-notes/2.16.0.html#improvements","text":"PMM-7506 : Security Threat Tool: Reduce False Positives due to Roles automatically created in PXC with no password but cannot be used to login. PMM-7569 : Make PMM compatible with pg_stat_monitor 0.8 release. PMM-7571 : Modified Percona Platform Account registration flow from PMM server UI. PMM-7513 : Integrated Alerting: Ability to see default values and Threshold values during the Alert Rule creation. PMM-7461 : Integrated Alerting: Improve UX of tables presentation and loading on UI. PMM-7375 : Integrated Alerting: Inform users about the template that they are editing and warn them about the limitations. PMM-7260 : Integrated Alerting: Make it clearer what rule is being edited.","title":"Improvements"},{"location":"release-notes/2.16.0.html#bugs-fixed","text":"PMM-7131 , PMM-7555 : QAN for PostgreSQL attempts to connect to a database with the same name as the username. (Thanks to Daniel Guzman Burgos for reporting this issue) PMM-7481 : Query Analytics is not showing \u201cQuery with Errors\u201d in the Profile section. PMM-7464 : NGINX misconfiguration leads to log storm in push mode PMM-7434 : Integrated Alerting: Unknown parameters [threshold] error during Add/Update Alert Rule. PMM-7231 : Integrated Alerting: Disabling channels does nothing. PMM-7379 : Integrated Alerting: Can not edit Alert Rule Name through API. PMM-7232 : Integrated Alerting: Disabling IA does not disable rules evaluation and notifications sending. PMM-7119 : Integrated Alerting: Update error notification for adding/update Alert rule Template \u2013 There was inconsistent behavior if you tried to add a new Rule Template with an already-used name. PMM-7543 : Integrated Alerting: selected section disappears from a breadcrumb after clicking the tab for a second time. PMM-7766 : DBaaS: PMM Upgrade breaks DBaaS get credentials method. PMM-7351 : DBaaS: Safari does not accept float numbers as a custom option in the \u201cCreate Cluster\u201d dialogue. PMM-7701 : DBaaS: PSMDB clusters stuck in initializing due to special characters in secrets.","title":"Bugs Fixed"},{"location":"release-notes/2.17.0.html","text":"Percona Monitoring and Management 2.17.0 \u00b6 Date: May 11, 2021 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. Release Highlights \u00b6 Custom certificates help define proper security levels for remotely monitored MySQL instances, including Google Cloud SQL. Usability improvements to the External Monitoring UI. When filling parameters, you can enter the parts of an endpoint (scheme, host, path) or let PMM automatically extract them from a URL. pg_stat_monitor 0.9.0 support. This change will give you compatibility with the latest version. Support for new features will be in an upcoming release. Single-line install of PMM Server on supported Linux distributions (this feature is in Technical Preview). DBaaS Changes: (this feature is in Technical Preview) It is easier to experience DBaaS functionality; you can quickly turn it ON/OFF in Advanced settings on the Settings page. ( Read more ) Database components management will enable PMM administrators to limit users in your organization to specific (admin-approved) database versions in their DBaaS DB Clusters. For PXC clusters created using DBaaS, HAProxy will now be used by default. Please note: Monitoring of the HAProxy in DBaaS will be enabled in an upcoming release. Changes to Sign in to Percona Platform . From this release, Registration of the Percona account will be more secure and require additional confirmation. New Features \u00b6 PMM-7863 : DBaaS: Ability to specify in K8s configuration the version of HAProxy to be used for DB creation PMM-7848 , PMM-7847 , PMM-7421 : Add support for using SSL certificates between pmm-admin and monitored MySQL databases PMM-7883 : Single-line install of PMM Server on supported Linux distributions - [Technical Preview] PMM-7013 , PMM-7819 : DBaaS: Use HAProxy by default instead of ProxySQL for MySQL DB clusters PMM-7356 , PMM-7581 : DBaaS: Management of available versions of DB components PMM-7358 , PMM-7576 : DBaaS: Management of default versions of DB components Improvements \u00b6 PMM-7572 : Add TLS options to mysqld_exporter PMM-7783 : Support of pg_stat_monitor 0.9.0 PMM-7064 : Integrated Alerting: Presenting severity of the Alert Rule using different colors PMM-7946 : Better error message on PMM client if server doesn\u2019t support HAProxy PMM-7932 : Usability improvements on UI for adding External Services PMM-7641 , PMM-7820 : Add DBaaS to Technical Preview section and allow user to Enable/Disable via UI PMM-7966 : Telemetry: Collect enabled/disabled status for Integrated Alerting and Security Threat Tool features Bugs Fixed \u00b6 PMM-7911 : DBaaS: Invalid Number of Nodes results in an annoying error message pop-up PMM-7884 : DBaaS: Fix DB Cluster tab loading PMM-7917 : PostgreSQL exporter has high CPU usage during Restart PMM-8037 : User can create a Percona Platform account without proper confirmation PMM-7702 : DBaaS: Cannot edit already-created PSMDB clusters PMM-7991 : MySQL Summary panel doesn\u2019t exist on MySQL Summary dashboard PMM-7939 : Inconsistent format of version reporting in pmm-admin PMM-7920 : PostgreSQL Exporter has increased memory usage with pmm-client 2.15.1 & pmm-server 2.16.0 PMM-7700 : Integrated Alerting: Rule API crashing with more than two parameters or invalid values PMM-7616 : Integrated Alerting: Incorrect title of the page in a browser PMM-7396 : Integrated Alerting: Alerts tab error if user deletes Alert Rule which has Firing alerts","title":"PMM 2.17.0"},{"location":"release-notes/2.17.0.html#percona-monitoring-and-management-2170","text":"Date: May 11, 2021 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance.","title":"Percona Monitoring and Management 2.17.0"},{"location":"release-notes/2.17.0.html#release-highlights","text":"Custom certificates help define proper security levels for remotely monitored MySQL instances, including Google Cloud SQL. Usability improvements to the External Monitoring UI. When filling parameters, you can enter the parts of an endpoint (scheme, host, path) or let PMM automatically extract them from a URL. pg_stat_monitor 0.9.0 support. This change will give you compatibility with the latest version. Support for new features will be in an upcoming release. Single-line install of PMM Server on supported Linux distributions (this feature is in Technical Preview). DBaaS Changes: (this feature is in Technical Preview) It is easier to experience DBaaS functionality; you can quickly turn it ON/OFF in Advanced settings on the Settings page. ( Read more ) Database components management will enable PMM administrators to limit users in your organization to specific (admin-approved) database versions in their DBaaS DB Clusters. For PXC clusters created using DBaaS, HAProxy will now be used by default. Please note: Monitoring of the HAProxy in DBaaS will be enabled in an upcoming release. Changes to Sign in to Percona Platform . From this release, Registration of the Percona account will be more secure and require additional confirmation.","title":"Release Highlights"},{"location":"release-notes/2.17.0.html#new-features","text":"PMM-7863 : DBaaS: Ability to specify in K8s configuration the version of HAProxy to be used for DB creation PMM-7848 , PMM-7847 , PMM-7421 : Add support for using SSL certificates between pmm-admin and monitored MySQL databases PMM-7883 : Single-line install of PMM Server on supported Linux distributions - [Technical Preview] PMM-7013 , PMM-7819 : DBaaS: Use HAProxy by default instead of ProxySQL for MySQL DB clusters PMM-7356 , PMM-7581 : DBaaS: Management of available versions of DB components PMM-7358 , PMM-7576 : DBaaS: Management of default versions of DB components","title":"New Features"},{"location":"release-notes/2.17.0.html#improvements","text":"PMM-7572 : Add TLS options to mysqld_exporter PMM-7783 : Support of pg_stat_monitor 0.9.0 PMM-7064 : Integrated Alerting: Presenting severity of the Alert Rule using different colors PMM-7946 : Better error message on PMM client if server doesn\u2019t support HAProxy PMM-7932 : Usability improvements on UI for adding External Services PMM-7641 , PMM-7820 : Add DBaaS to Technical Preview section and allow user to Enable/Disable via UI PMM-7966 : Telemetry: Collect enabled/disabled status for Integrated Alerting and Security Threat Tool features","title":"Improvements"},{"location":"release-notes/2.17.0.html#bugs-fixed","text":"PMM-7911 : DBaaS: Invalid Number of Nodes results in an annoying error message pop-up PMM-7884 : DBaaS: Fix DB Cluster tab loading PMM-7917 : PostgreSQL exporter has high CPU usage during Restart PMM-8037 : User can create a Percona Platform account without proper confirmation PMM-7702 : DBaaS: Cannot edit already-created PSMDB clusters PMM-7991 : MySQL Summary panel doesn\u2019t exist on MySQL Summary dashboard PMM-7939 : Inconsistent format of version reporting in pmm-admin PMM-7920 : PostgreSQL Exporter has increased memory usage with pmm-client 2.15.1 & pmm-server 2.16.0 PMM-7700 : Integrated Alerting: Rule API crashing with more than two parameters or invalid values PMM-7616 : Integrated Alerting: Incorrect title of the page in a browser PMM-7396 : Integrated Alerting: Alerts tab error if user deletes Alert Rule which has Firing alerts","title":"Bugs Fixed"},{"location":"release-notes/2.18.0.html","text":"Percona Monitoring and Management 2.18.0 \u00b6 Date: June 1, 2021 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. Release Highlights \u00b6 The goal for this small release was to eliminate a lot of bugs and complete some features. DBaaS Added the ability for PMM to install the latest versions of the K8s operator into the K8s cluster. There is no longer any need to install the operator manually. Just connect PMM to your K8s cluster and let PMM do the rest. Backup Management Backup functionality was released as a Technical Preview feature and will require specific prerequisites from the user side to be installed. Currently, PMM will allow you to: manage storage for backups (S3 only); execute a backup for MySQL and Mongo instances; restore a MySQL backup to the same instance from where it was taken (via the UI). Restore in other cases is not yet implemented on the UI. New Features \u00b6 PMM-7509 : Integrated Alerting: Pagination for Alerts list Improvements \u00b6 PMM-8029 : DBaaS: PSMDB 1.8 operator support PMM-7548 : Integrated Alerting: Disable edit and delete buttons for templates manually staged by user directly on the file system Bugs Fixed \u00b6 PMM-8053 : Better error handling for non-admins who try to access the settings page PMM-7941 : Wrong replication status for MongoDB Replica Set PMM-7302 : Webhook usage with max_alerts attribute for Alertmanager configuration causes errors for PMM PMM-7224 : Instance Overview dashboards behave inconsistently PMM-6864 : MongoDB Oplog Recovery Window dashboard is broken (Thanks to Clyde Shillingford for reporting this issue) PMM-7910 : MongoDB Query metrics stops being collected if the cursor is failed once (Thanks to Yann Rouillard for reporting this issue) PMM-6451 : Passing parameters between Query Analytics and Dashboards is broken PMM-5368 : Unclear message \u201cFailed to get PMM Server parameter\u201d after configuration (Thanks to Martin Wittwer for reporting this issue) PMM-5135 : Query Example is often empty for MySQL 8+ (Thanks to Mikhail Solovyev for reporting this issue) PMM-8083 : Better configuration file checking during configuration PMM-7958 : Databases cannot be deleted while PostgreSQL is being monitored PMM-6553 : Slow log size units are not defined in help PMM-5931 : Graph and values in Query Analytics are identical for TOTAL and case when data is Not Available PMM-5538 : Heavy Load with Distinct Queries on Slowlog enabled could cause no data being reported PMM-8095 : The link to the Community section in the PMM footer is broken PMM-7982 : Query Analytics: Sorting element for the columns is hard to access PMM-6676 : Terms and Privacy pages opened in the current tab complicates registration process PMM-6552 : Do not register with server if configuration file fails to create on client PMM-6505 : Inconsistent style for error messages on Add RDS instance page. PMM-8069 : Integrated Alerting: Alert template now accepts .yaml extension in addition to .yml when manually staging on the file system PMM-7673 : Integrated Alerting: Actions column is transparent PMM-7916 : DBaaS: Wrong required resources when editing a cluster PMM-7753 : DBaaS: Edit DB Cluster shows wrong values by default PMM-7184 : DBaaS: Connection column showing different values after deleting DB cluster PMM-8088 : DBaaS: In case of error, the kubeconfig file is left in the system","title":"PMM 2.18.0"},{"location":"release-notes/2.18.0.html#percona-monitoring-and-management-2180","text":"Date: June 1, 2021 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance.","title":"Percona Monitoring and Management 2.18.0"},{"location":"release-notes/2.18.0.html#release-highlights","text":"The goal for this small release was to eliminate a lot of bugs and complete some features. DBaaS Added the ability for PMM to install the latest versions of the K8s operator into the K8s cluster. There is no longer any need to install the operator manually. Just connect PMM to your K8s cluster and let PMM do the rest. Backup Management Backup functionality was released as a Technical Preview feature and will require specific prerequisites from the user side to be installed. Currently, PMM will allow you to: manage storage for backups (S3 only); execute a backup for MySQL and Mongo instances; restore a MySQL backup to the same instance from where it was taken (via the UI). Restore in other cases is not yet implemented on the UI.","title":"Release Highlights"},{"location":"release-notes/2.18.0.html#new-features","text":"PMM-7509 : Integrated Alerting: Pagination for Alerts list","title":"New Features"},{"location":"release-notes/2.18.0.html#improvements","text":"PMM-8029 : DBaaS: PSMDB 1.8 operator support PMM-7548 : Integrated Alerting: Disable edit and delete buttons for templates manually staged by user directly on the file system","title":"Improvements"},{"location":"release-notes/2.18.0.html#bugs-fixed","text":"PMM-8053 : Better error handling for non-admins who try to access the settings page PMM-7941 : Wrong replication status for MongoDB Replica Set PMM-7302 : Webhook usage with max_alerts attribute for Alertmanager configuration causes errors for PMM PMM-7224 : Instance Overview dashboards behave inconsistently PMM-6864 : MongoDB Oplog Recovery Window dashboard is broken (Thanks to Clyde Shillingford for reporting this issue) PMM-7910 : MongoDB Query metrics stops being collected if the cursor is failed once (Thanks to Yann Rouillard for reporting this issue) PMM-6451 : Passing parameters between Query Analytics and Dashboards is broken PMM-5368 : Unclear message \u201cFailed to get PMM Server parameter\u201d after configuration (Thanks to Martin Wittwer for reporting this issue) PMM-5135 : Query Example is often empty for MySQL 8+ (Thanks to Mikhail Solovyev for reporting this issue) PMM-8083 : Better configuration file checking during configuration PMM-7958 : Databases cannot be deleted while PostgreSQL is being monitored PMM-6553 : Slow log size units are not defined in help PMM-5931 : Graph and values in Query Analytics are identical for TOTAL and case when data is Not Available PMM-5538 : Heavy Load with Distinct Queries on Slowlog enabled could cause no data being reported PMM-8095 : The link to the Community section in the PMM footer is broken PMM-7982 : Query Analytics: Sorting element for the columns is hard to access PMM-6676 : Terms and Privacy pages opened in the current tab complicates registration process PMM-6552 : Do not register with server if configuration file fails to create on client PMM-6505 : Inconsistent style for error messages on Add RDS instance page. PMM-8069 : Integrated Alerting: Alert template now accepts .yaml extension in addition to .yml when manually staging on the file system PMM-7673 : Integrated Alerting: Actions column is transparent PMM-7916 : DBaaS: Wrong required resources when editing a cluster PMM-7753 : DBaaS: Edit DB Cluster shows wrong values by default PMM-7184 : DBaaS: Connection column showing different values after deleting DB cluster PMM-8088 : DBaaS: In case of error, the kubeconfig file is left in the system","title":"Bugs Fixed"},{"location":"release-notes/2.19.0.html","text":"Percona Monitoring and Management 2.19.0 \u00b6 Date: June 30, 2021 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. Visit our forum to comment on this release. Release Highlights \u00b6 Backup Management can now be enabled from the UI. Go to Configuration \u2192 Settings \u2192 Advanced Settings , and you will see it in the Technical Preview section. We also added support for MongoDB services on-demand backup and restore. For now, it only supports ReplicaSet on S3-compatible storage. Dashboards improvements There are several community-driven improvements to ProxySQL data collection, with new dashboards to expose such metrics like: Queries Latency histograms and SHUNNED_REPLICATION_LAG state. Fixes for Amazon Aurora service detection on the dashboard , MongoDB ReplicaSet Summary, and other MongoDB memory-related panels. Improvements to DBaaS secrets by generating strong passwords for operators. This is an improvement to the Automated Operator Installation released in PMM 2.18 , which will greatly enhance security. New Features \u00b6 PMM-7639 : Backup Management: Ability to remove Backup Location even if there are some backup artifacts on it PMM-7567 : Backup Management: Simple backup for MongoDB ReplicaSet PMM-7568 : Backup Management: Simple restore for MongoDB ReplicaSet Improvements \u00b6 PMM-8112 : Dashboards: Collect and present histograms from ProxySQL on Queries Latency dashboard (Thanks to foosinn for helping with this improvement) PMM-8081 : Dashboards: Add collection and presentation for SHUNNED_REPLICATION_LAG state to proxysql_connection_pool_status (Thanks to spaceform for helping with this improvement) PMM-7584 : Components Upgrade: VictoriaMetrics 1.53.1 to 1.60.0 PMM-8001 : Better error handling when pg_stat_monitor is an unsupported version PMM-7659 : DBaaS: Ability to specify the type of connection for DBaaS cluster during DB Cluster creation PMM-7828 : DBaaS: Select Database Type by default if only one operator is installed PMM-8153 : Backup Management: Disable \u2018Restore\u2019 button for backups whose service has been deleted Bugs Fixed \u00b6 PMM-7194 : \u2018Share with Percona\u2019 option doesn\u2019t export data from collapsed panels PMM-8060 : User cannot add remote instances after OVF/AMI upgrade (previously was Known Issue) PMM-7104 : Slowlog rotation by pmm-agent causing additional unexpected rotation of binary logs (Thanks to Arthur Tokarchuk for reporting this issue) PMM-8125 : Error of monitoring GCP based PostgreSQL because of internal database cloudsqladmin PMM-6778 : Can\u2019t specify custom labels during node addition/configuration PMM-8090 : Multi-request protection breaks metrics gathering (Thanks to Francisco Miguel Biete for fixing this bug) PMM-5248 : InnoDB TableSpace data is not collected for Percona Server 8 PMM-4665 : User\u2019s log file contains error messages about TokuDB and heartbeat despite not being used PMM-8014 : Error when adding Amazon RDS MySQL with TLS over API PMM-3352 : Low file descriptors limit (1024) with AMI or OVF images causes errors PMM-7948 : pmm-admin list reports the wrong Node for External Services PMM-6295 : Unclear/Incorrect statuses of the pmm-agent when Agent or PMM server went down (Thanks to Mikhail Solovyev for reporting this issue) PMM-5917 : pmm-agent moves slow logs without checking privileges PMM-8021 : \u201cQuery Analytics\u201d misspelled on left side menu PMM-5283 : Inconsistency with lengths of Example and Fingerprints in Query Analytics PMM-8121 : Error message and help for \u201cRemove Service\u201d command is not helpful PMM-8196 : Additional spaces in email/passwords fields on Sign up/Login pages causes Authentication problems PMM-8009 : Long First/Last names causes errors when used in Register/Login form PMM-8220 : Dashboards: Active Time Series Changes on Victoria Metrics dashboard report no data PMM-8202 : Dashboards: No Amazon Aurora services are available on MySQL Amazon Aurora Details dashboard for selection PMM-8085 : Dashboards: Wrong units are used in MongoDB dashboards on memory-related panels PMM-7154 : Dashboards: No data on some panels from MongoDB ReplSet Summary dashboard PMM-8115 : DBaaS: Delete PSMDB cluster action takes too long PMM-7737 : DBaaS: Replace all default passwords in operator secrets during installation PMM-7970 : DBaaS: Confusing message for Cluster name pattern on DB Cluster creation screen PMM-7755 : DBaaS: Clusters with longer name not initializing PMM-7528 : DBaaS: Error after Kubernetes cluster destroyed or removed externally to DBaaS PMM-8013 : Backup Management: Unable to get Backup Artifact list after Service removal","title":"PMM 2.19.0"},{"location":"release-notes/2.19.0.html#percona-monitoring-and-management-2190","text":"Date: June 30, 2021 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. Visit our forum to comment on this release.","title":"Percona Monitoring and Management 2.19.0"},{"location":"release-notes/2.19.0.html#release-highlights","text":"Backup Management can now be enabled from the UI. Go to Configuration \u2192 Settings \u2192 Advanced Settings , and you will see it in the Technical Preview section. We also added support for MongoDB services on-demand backup and restore. For now, it only supports ReplicaSet on S3-compatible storage. Dashboards improvements There are several community-driven improvements to ProxySQL data collection, with new dashboards to expose such metrics like: Queries Latency histograms and SHUNNED_REPLICATION_LAG state. Fixes for Amazon Aurora service detection on the dashboard , MongoDB ReplicaSet Summary, and other MongoDB memory-related panels. Improvements to DBaaS secrets by generating strong passwords for operators. This is an improvement to the Automated Operator Installation released in PMM 2.18 , which will greatly enhance security.","title":"Release Highlights"},{"location":"release-notes/2.19.0.html#new-features","text":"PMM-7639 : Backup Management: Ability to remove Backup Location even if there are some backup artifacts on it PMM-7567 : Backup Management: Simple backup for MongoDB ReplicaSet PMM-7568 : Backup Management: Simple restore for MongoDB ReplicaSet","title":"New Features"},{"location":"release-notes/2.19.0.html#improvements","text":"PMM-8112 : Dashboards: Collect and present histograms from ProxySQL on Queries Latency dashboard (Thanks to foosinn for helping with this improvement) PMM-8081 : Dashboards: Add collection and presentation for SHUNNED_REPLICATION_LAG state to proxysql_connection_pool_status (Thanks to spaceform for helping with this improvement) PMM-7584 : Components Upgrade: VictoriaMetrics 1.53.1 to 1.60.0 PMM-8001 : Better error handling when pg_stat_monitor is an unsupported version PMM-7659 : DBaaS: Ability to specify the type of connection for DBaaS cluster during DB Cluster creation PMM-7828 : DBaaS: Select Database Type by default if only one operator is installed PMM-8153 : Backup Management: Disable \u2018Restore\u2019 button for backups whose service has been deleted","title":"Improvements"},{"location":"release-notes/2.19.0.html#bugs-fixed","text":"PMM-7194 : \u2018Share with Percona\u2019 option doesn\u2019t export data from collapsed panels PMM-8060 : User cannot add remote instances after OVF/AMI upgrade (previously was Known Issue) PMM-7104 : Slowlog rotation by pmm-agent causing additional unexpected rotation of binary logs (Thanks to Arthur Tokarchuk for reporting this issue) PMM-8125 : Error of monitoring GCP based PostgreSQL because of internal database cloudsqladmin PMM-6778 : Can\u2019t specify custom labels during node addition/configuration PMM-8090 : Multi-request protection breaks metrics gathering (Thanks to Francisco Miguel Biete for fixing this bug) PMM-5248 : InnoDB TableSpace data is not collected for Percona Server 8 PMM-4665 : User\u2019s log file contains error messages about TokuDB and heartbeat despite not being used PMM-8014 : Error when adding Amazon RDS MySQL with TLS over API PMM-3352 : Low file descriptors limit (1024) with AMI or OVF images causes errors PMM-7948 : pmm-admin list reports the wrong Node for External Services PMM-6295 : Unclear/Incorrect statuses of the pmm-agent when Agent or PMM server went down (Thanks to Mikhail Solovyev for reporting this issue) PMM-5917 : pmm-agent moves slow logs without checking privileges PMM-8021 : \u201cQuery Analytics\u201d misspelled on left side menu PMM-5283 : Inconsistency with lengths of Example and Fingerprints in Query Analytics PMM-8121 : Error message and help for \u201cRemove Service\u201d command is not helpful PMM-8196 : Additional spaces in email/passwords fields on Sign up/Login pages causes Authentication problems PMM-8009 : Long First/Last names causes errors when used in Register/Login form PMM-8220 : Dashboards: Active Time Series Changes on Victoria Metrics dashboard report no data PMM-8202 : Dashboards: No Amazon Aurora services are available on MySQL Amazon Aurora Details dashboard for selection PMM-8085 : Dashboards: Wrong units are used in MongoDB dashboards on memory-related panels PMM-7154 : Dashboards: No data on some panels from MongoDB ReplSet Summary dashboard PMM-8115 : DBaaS: Delete PSMDB cluster action takes too long PMM-7737 : DBaaS: Replace all default passwords in operator secrets during installation PMM-7970 : DBaaS: Confusing message for Cluster name pattern on DB Cluster creation screen PMM-7755 : DBaaS: Clusters with longer name not initializing PMM-7528 : DBaaS: Error after Kubernetes cluster destroyed or removed externally to DBaaS PMM-8013 : Backup Management: Unable to get Backup Artifact list after Service removal","title":"Bugs Fixed"},{"location":"release-notes/2.2.0.html","text":"Percona Monitoring and Management 2.2.0 \u00b6 Date: December 24, 2019 Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. You can run PMM in your own environment for maximum security and reliability. It provides thorough time-based analysis for MySQL, MongoDB, and PostgreSQL servers to ensure that your data works as efficiently as possible. Main improvements in this release are: Alternative installation methods available for PMM 1.x are re-implemented for PMM 2: now PMM Server can be installed as a virtual appliance, or run using AWS Marketplace AWS RDS and remote instances monitoring re-added in this release include AWS RDS MySQL / Aurora MySQL instances, and remote PostgreSQL, MySQL, MongoDB, and ProxySQL ones The new Settings dashboard allows configuring PMM Server via the graphical interface For PMM install instructions, see Installing PMM Server and Installing PMM Client . Caution PMM 2 is designed to be used as a new installation \u2014 please don\u2019t try to upgrade your existing PMM 1 environment. Improvements and new features \u00b6 PMM-4575 : The new PMM Settings dashboard allows users to configure various PMM Server options: setting metrics resolution and data retention, enabling or disabling send usage data statistics back to Percona and checking for updates; this dashboard is now the proper place to upload your public key for the SSH login and to download PMM Server logs for diagnostics PMM-4907 and PMM-4767 : The user\u2019s AMI Instance ID is now used to setup running PMM Server using AWS Marketplace as an additional verification on the user, based on the Amazon Marketplace rules PMM-4950 and PMM-3094 : Alternative AWS partitions are now supported when adding an AWS RDS MySQL or Aurora MySQL Instance to PMM PMM-4976 : Home dashboard clean-up: \u201cSystems under monitoring\u201d and \u201cNetwork IO\u201d singlestats were refined to be based on the host variable; also avoiding using color as an indicator of state; \u201cAll\u201d row elements were relinked to the \u201cNodes Overview\u201d dashboard with regards to the selected host. PMM-4800 : The pmm-admin add mysql command has been modified to make help text more descriptive: now when you enable tablestats you will get more detail on if they\u2019re enabled for your environment and where you stand with respect to the auto-disable limit PMM-4969 : Update Grafana to version 6.5.1 PMM-5053 : A tooltip was added to the Head Block graph on the Prometheus dashboard PMM-5068 : Drill-down links were added to the Node Summary dashboard graphs PMM-5050 : Drill-down links were added to the graphs on all Services Compare dashboards PMM-5037 : Drill-down links were added to all graphs on the Services Overview dashboards PMM-4988 : Filtering in Query Analytics have undergone improvements to make group selection more intuitive: Labels unavailable under the current selection are shown as gray/disabled, and the percentage values are dynamically recalculated to reflect Labels available within the currently applied filters PMM-4966 : All passwords are now substituted with asterisk signs in the exporter logs for security reasons when not in debug mode PMM-527 : node_exporter is now providing hardware monitoring information such as CPU temperatures and fan statuses; while this information is being collected by PMM Server, it will not be shown until a dedicated dashboard is added in a future release PMM-3198 : Instead of showing All graphs for all services by default, MySQL Command/Handler Counters Compare dashboard now shows the predefined set of ten most informative ones, to reduce load on PMM Server at its first open Fixed bugs \u00b6 PMM-4978 : The \u201cTop MySQL Questions\u201d singlestat on the MySQL Instances Overview dashboard was changed to show ops instead of percentage PMM-4917 : The \u201cSystems under monitoring\u201d and \u201cMonitored DB Instances\u201d singlestats on the Home dashboard now have a sparkline to make situation more clear with recently shut down nodes/instances PMM-4979 : Set decimal precision 2 for all the elements, including charts and singlestats , on all dashboards PMM-4980 : Fix \u201cLoad Average\u201d singlestat on the Node Summary dashboard to show decimal value instead of percent PMM-4981 : Disable automatic color gradient in filled graphs on all dashboards PMM-4941 : Some charts were incorrectly showing empty fragments with high time resolution turned on PMM-5022 : Fix outdated drill-down links on the Prometheus Exporters Overview and Nodes Overview dashboards PMM-5023 : Make the All instances uptime singlestat on the Home dashboard to show Min values instead of Avg PMM-5029 : Option to upload dashboard snapshot to Percona was disappearing after upgrade to 2.1.x PMM-4946 : Rename singlestats on the Home dashboard for better clarity: \u201cSystems under monitoring\u201d to \u201cNodes under monitoring\u201d and \u201cMonitored DB Instances\u201d to \u201cMonitored DB Services\u201d, and make the last one to count remote DB instances also PMM-5015 : Fix format of Disk Page Buffers singlestat on the Compare dashboard for PostgreSQL to have two digits precision for the consistency with other singlestats PMM-5014 : LVM logical volumes were wrongly sized on a new AWS deployment, resulting in \u201cno space left on device\u201d errors. PMM-4804 : Incorrect parameters validation required both service-name and service-id parameters of the pmm-admin remove command to be presented, while the command itself demanded only one of them to identify the service. PMM-3298 : Panic errors were present in the rds_exporter log after adding an RDS instance from the second AWS account PMM-5089 : The serialize-javascript package was updated to version 2.1.1 because of the possibility of regular expressions cross-site scripting vulnerability in it (CVE-2019-16769). Please note PMM versions were not affected by this vulnerability, as serialize-javascript package is used as a build dependency only. PMM-5149 : Disk Space singlestat was unable to show data for RDS instances because of not taking into account sources with unknown file system type","title":"PMM 2.2.0"},{"location":"release-notes/2.2.0.html#percona-monitoring-and-management-220","text":"Date: December 24, 2019 Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. You can run PMM in your own environment for maximum security and reliability. It provides thorough time-based analysis for MySQL, MongoDB, and PostgreSQL servers to ensure that your data works as efficiently as possible. Main improvements in this release are: Alternative installation methods available for PMM 1.x are re-implemented for PMM 2: now PMM Server can be installed as a virtual appliance, or run using AWS Marketplace AWS RDS and remote instances monitoring re-added in this release include AWS RDS MySQL / Aurora MySQL instances, and remote PostgreSQL, MySQL, MongoDB, and ProxySQL ones The new Settings dashboard allows configuring PMM Server via the graphical interface For PMM install instructions, see Installing PMM Server and Installing PMM Client . Caution PMM 2 is designed to be used as a new installation \u2014 please don\u2019t try to upgrade your existing PMM 1 environment.","title":"Percona Monitoring and Management 2.2.0"},{"location":"release-notes/2.2.0.html#improvements-and-new-features","text":"PMM-4575 : The new PMM Settings dashboard allows users to configure various PMM Server options: setting metrics resolution and data retention, enabling or disabling send usage data statistics back to Percona and checking for updates; this dashboard is now the proper place to upload your public key for the SSH login and to download PMM Server logs for diagnostics PMM-4907 and PMM-4767 : The user\u2019s AMI Instance ID is now used to setup running PMM Server using AWS Marketplace as an additional verification on the user, based on the Amazon Marketplace rules PMM-4950 and PMM-3094 : Alternative AWS partitions are now supported when adding an AWS RDS MySQL or Aurora MySQL Instance to PMM PMM-4976 : Home dashboard clean-up: \u201cSystems under monitoring\u201d and \u201cNetwork IO\u201d singlestats were refined to be based on the host variable; also avoiding using color as an indicator of state; \u201cAll\u201d row elements were relinked to the \u201cNodes Overview\u201d dashboard with regards to the selected host. PMM-4800 : The pmm-admin add mysql command has been modified to make help text more descriptive: now when you enable tablestats you will get more detail on if they\u2019re enabled for your environment and where you stand with respect to the auto-disable limit PMM-4969 : Update Grafana to version 6.5.1 PMM-5053 : A tooltip was added to the Head Block graph on the Prometheus dashboard PMM-5068 : Drill-down links were added to the Node Summary dashboard graphs PMM-5050 : Drill-down links were added to the graphs on all Services Compare dashboards PMM-5037 : Drill-down links were added to all graphs on the Services Overview dashboards PMM-4988 : Filtering in Query Analytics have undergone improvements to make group selection more intuitive: Labels unavailable under the current selection are shown as gray/disabled, and the percentage values are dynamically recalculated to reflect Labels available within the currently applied filters PMM-4966 : All passwords are now substituted with asterisk signs in the exporter logs for security reasons when not in debug mode PMM-527 : node_exporter is now providing hardware monitoring information such as CPU temperatures and fan statuses; while this information is being collected by PMM Server, it will not be shown until a dedicated dashboard is added in a future release PMM-3198 : Instead of showing All graphs for all services by default, MySQL Command/Handler Counters Compare dashboard now shows the predefined set of ten most informative ones, to reduce load on PMM Server at its first open","title":"Improvements and new features"},{"location":"release-notes/2.2.0.html#fixed-bugs","text":"PMM-4978 : The \u201cTop MySQL Questions\u201d singlestat on the MySQL Instances Overview dashboard was changed to show ops instead of percentage PMM-4917 : The \u201cSystems under monitoring\u201d and \u201cMonitored DB Instances\u201d singlestats on the Home dashboard now have a sparkline to make situation more clear with recently shut down nodes/instances PMM-4979 : Set decimal precision 2 for all the elements, including charts and singlestats , on all dashboards PMM-4980 : Fix \u201cLoad Average\u201d singlestat on the Node Summary dashboard to show decimal value instead of percent PMM-4981 : Disable automatic color gradient in filled graphs on all dashboards PMM-4941 : Some charts were incorrectly showing empty fragments with high time resolution turned on PMM-5022 : Fix outdated drill-down links on the Prometheus Exporters Overview and Nodes Overview dashboards PMM-5023 : Make the All instances uptime singlestat on the Home dashboard to show Min values instead of Avg PMM-5029 : Option to upload dashboard snapshot to Percona was disappearing after upgrade to 2.1.x PMM-4946 : Rename singlestats on the Home dashboard for better clarity: \u201cSystems under monitoring\u201d to \u201cNodes under monitoring\u201d and \u201cMonitored DB Instances\u201d to \u201cMonitored DB Services\u201d, and make the last one to count remote DB instances also PMM-5015 : Fix format of Disk Page Buffers singlestat on the Compare dashboard for PostgreSQL to have two digits precision for the consistency with other singlestats PMM-5014 : LVM logical volumes were wrongly sized on a new AWS deployment, resulting in \u201cno space left on device\u201d errors. PMM-4804 : Incorrect parameters validation required both service-name and service-id parameters of the pmm-admin remove command to be presented, while the command itself demanded only one of them to identify the service. PMM-3298 : Panic errors were present in the rds_exporter log after adding an RDS instance from the second AWS account PMM-5089 : The serialize-javascript package was updated to version 2.1.1 because of the possibility of regular expressions cross-site scripting vulnerability in it (CVE-2019-16769). Please note PMM versions were not affected by this vulnerability, as serialize-javascript package is used as a build dependency only. PMM-5149 : Disk Space singlestat was unable to show data for RDS instances because of not taking into account sources with unknown file system type","title":"Fixed bugs"},{"location":"release-notes/2.2.1.html","text":"Percona Monitoring and Management 2.2.1 \u00b6 Date: January 23, 2020 Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. For PMM install instructions, see Installing PMM Server and Installing PMM Client . Caution PMM 2 is designed to be used as a new installation \u2014 please don\u2019t try to upgrade your existing PMM 1 environment. PMM Server version 2.2.0 suffered an unauthenticated denial of service vulnerability (CVE-2020-7920). Any other PMM versions do not carry the same code logic, and are thus unaffected by this issue. Users who have already deployed PMM Server 2.2.0 are advised to upgrade to version 2.2.1 which resolves this issue. Improvements and new features \u00b6 PMM-5229 : The new RDS Exporter section added to the Prometheus Exporter Status dashboard shows singlestats and charts related to the rds_exporter PMM-5228 and PMM-5238 : The Prometheus dashboard and the Exporters Overview dashboard were updated to include the rds_exporter metrics in their charts, allowing better understanding of the impacts of monitoring RDS instances PMM-4830 : The consistency of the applied filters between the Query Analytics and the Overview dashboards was implemented, and now filters selected in QAN will continue to be active after the switch to any of the Overview dashboards available in the Services menu PMM-5235 : The DB uptime singlestats in node rows on the Home dashboard were changed to show minimal values instead of average ones to be consistent with the top row PMM-5127 : The \u201cSearch by\u201d bar on the Query Analytics dashboard was renamed to \u201cFilter by\u201d to make its purpose more clear PMM-5131 : The Filter panel on the Query Analytics dashboard now shows the total number of available Labels within the \u201cSee all\u201d link, which appears if the Filter panel section shows only top 5 of its Labels Fixed bugs \u00b6 PMM-5232 : The pmm-managed component of the PMM Server 2.2.0 is vulnerable to DoS attacks, that could be carried out by anyone who knows the PMM Server IP address (CVE-2020-7920). Versions other than 2.2.0 are not affected. PMM-5226 : The handlebars package was updated to version 4.5.3 because of the Prototype Pollution vulnerability in it (CVE-2019-19919). Please note PMM versions were not affected by this vulnerability, as handlebars package is used as a build dependency only. PMM-5206 : Switching to the Settings dashboard was breaking the visual style of some elements on the Home dashboard PMM-5139 : The breadcrumb panel, which shows all dashboards visited within one session starting from the root, was unable to fully show breadcrumb longer than one line PMM-5212 : The explanatory text was added to the Download PMM Server Logs button in the Diagnostic section of the PMM Settings dashboard, and a link to it was added to the Prometheus dashboard which was the previous place to download logs PMM-5215 : The unneeded mariadb-libs package was removed from the PMM Server 2.2.0 OVF image, resulting in both faster updating with the yum update command and avoiding dependency conflict messages in the update logs PMM-5216 : PMM Server Upgrade to 2.2.0 was showing Grafana Update Error page with the Refresh button which had to be clicked to start using the updated version PMM-5211 : The \u201cWhere do I get the security credentials for my Amazon RDS DB instance\u201d link in the Add AWS RDS MySQL or Aurora MySQL instance dialog was not targeted at the appropriate instruction PMM-5217 : PMM 2.x OVF Image memory size was increased from 1 Gb to 4 Gb with the additional 1 Gb swap space because the previous amount was hardly housing the PMM Server, and it wasn\u2019t enough in some cases like performing an upgrade PMM-5271 : LVM logical volumes were wrongly resized on AWS deployment, resulting in \u201cno space left on device\u201d errors PMM-5295 : InnoDB Transaction Rollback Rate values on the MySQL InnoDB Details dashboard were calculated incorrectly PMM-5270 : PXC/Galera Cluster Summary dashboard was showing empty Cluster drop-down list, making it impossible to choose the cluster name PMM-4769 : The wrongly named \u201cTimeout value used for retransmitting\u201d singlestat on the Network Details dashboard was renamed to \u201cThe algorithm used to determine the timeout value\u201d and updated to show the algorithm name instead of a digital code PMM-5260 : Extensive resource consumption by pmm-agent took place in case of Query Analytics for PostgreSQL; it was fixed by a number of optimizations in the code, resulting in about 4 times smaller memory usage PMM-5261 : CPU usage charts on all dashboards which contain them have undergone colors update to make softIRQ and Steal curves better differentiated PMM-5244 : High memory consumption in the PMM Server with a large number of agents sending data simultaneously was fixed by improving bulk data insertion to the ClickHouse database","title":"PMM 2.2.1"},{"location":"release-notes/2.2.1.html#percona-monitoring-and-management-221","text":"Date: January 23, 2020 Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. For PMM install instructions, see Installing PMM Server and Installing PMM Client . Caution PMM 2 is designed to be used as a new installation \u2014 please don\u2019t try to upgrade your existing PMM 1 environment. PMM Server version 2.2.0 suffered an unauthenticated denial of service vulnerability (CVE-2020-7920). Any other PMM versions do not carry the same code logic, and are thus unaffected by this issue. Users who have already deployed PMM Server 2.2.0 are advised to upgrade to version 2.2.1 which resolves this issue.","title":"Percona Monitoring and Management 2.2.1"},{"location":"release-notes/2.2.1.html#improvements-and-new-features","text":"PMM-5229 : The new RDS Exporter section added to the Prometheus Exporter Status dashboard shows singlestats and charts related to the rds_exporter PMM-5228 and PMM-5238 : The Prometheus dashboard and the Exporters Overview dashboard were updated to include the rds_exporter metrics in their charts, allowing better understanding of the impacts of monitoring RDS instances PMM-4830 : The consistency of the applied filters between the Query Analytics and the Overview dashboards was implemented, and now filters selected in QAN will continue to be active after the switch to any of the Overview dashboards available in the Services menu PMM-5235 : The DB uptime singlestats in node rows on the Home dashboard were changed to show minimal values instead of average ones to be consistent with the top row PMM-5127 : The \u201cSearch by\u201d bar on the Query Analytics dashboard was renamed to \u201cFilter by\u201d to make its purpose more clear PMM-5131 : The Filter panel on the Query Analytics dashboard now shows the total number of available Labels within the \u201cSee all\u201d link, which appears if the Filter panel section shows only top 5 of its Labels","title":"Improvements and new features"},{"location":"release-notes/2.2.1.html#fixed-bugs","text":"PMM-5232 : The pmm-managed component of the PMM Server 2.2.0 is vulnerable to DoS attacks, that could be carried out by anyone who knows the PMM Server IP address (CVE-2020-7920). Versions other than 2.2.0 are not affected. PMM-5226 : The handlebars package was updated to version 4.5.3 because of the Prototype Pollution vulnerability in it (CVE-2019-19919). Please note PMM versions were not affected by this vulnerability, as handlebars package is used as a build dependency only. PMM-5206 : Switching to the Settings dashboard was breaking the visual style of some elements on the Home dashboard PMM-5139 : The breadcrumb panel, which shows all dashboards visited within one session starting from the root, was unable to fully show breadcrumb longer than one line PMM-5212 : The explanatory text was added to the Download PMM Server Logs button in the Diagnostic section of the PMM Settings dashboard, and a link to it was added to the Prometheus dashboard which was the previous place to download logs PMM-5215 : The unneeded mariadb-libs package was removed from the PMM Server 2.2.0 OVF image, resulting in both faster updating with the yum update command and avoiding dependency conflict messages in the update logs PMM-5216 : PMM Server Upgrade to 2.2.0 was showing Grafana Update Error page with the Refresh button which had to be clicked to start using the updated version PMM-5211 : The \u201cWhere do I get the security credentials for my Amazon RDS DB instance\u201d link in the Add AWS RDS MySQL or Aurora MySQL instance dialog was not targeted at the appropriate instruction PMM-5217 : PMM 2.x OVF Image memory size was increased from 1 Gb to 4 Gb with the additional 1 Gb swap space because the previous amount was hardly housing the PMM Server, and it wasn\u2019t enough in some cases like performing an upgrade PMM-5271 : LVM logical volumes were wrongly resized on AWS deployment, resulting in \u201cno space left on device\u201d errors PMM-5295 : InnoDB Transaction Rollback Rate values on the MySQL InnoDB Details dashboard were calculated incorrectly PMM-5270 : PXC/Galera Cluster Summary dashboard was showing empty Cluster drop-down list, making it impossible to choose the cluster name PMM-4769 : The wrongly named \u201cTimeout value used for retransmitting\u201d singlestat on the Network Details dashboard was renamed to \u201cThe algorithm used to determine the timeout value\u201d and updated to show the algorithm name instead of a digital code PMM-5260 : Extensive resource consumption by pmm-agent took place in case of Query Analytics for PostgreSQL; it was fixed by a number of optimizations in the code, resulting in about 4 times smaller memory usage PMM-5261 : CPU usage charts on all dashboards which contain them have undergone colors update to make softIRQ and Steal curves better differentiated PMM-5244 : High memory consumption in the PMM Server with a large number of agents sending data simultaneously was fixed by improving bulk data insertion to the ClickHouse database","title":"Fixed bugs"},{"location":"release-notes/2.2.2.html","text":"Percona Monitoring and Management 2.2.2 \u00b6 Date: February 4, 2020 Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. For PMM install instructions, see Installing PMM Server and Installing PMM Client . Caution PMM 2 is designed to be used as a new installation \u2014 please don\u2019t try to upgrade your existing PMM 1 environment. Improvements and new features \u00b6 PMM-5321 : The optimization of the Query Analytics parser code for PostgreSQL queries allowed us to reduce the memory resources consumption by 1-5%, and the parsing time of an individual query by 30 to 40% PMM-5184 : The pmm-admin summary command have gained a new --skip-server flag which makes it operating in a local-only mode, creating summary file without contacting the PMM Server Fixed bugs \u00b6 PMM-5340 : The Scraping Time Drift graph on the Prometheus dashboard was showing wrong values because the actual metrics resolution wasn\u2019t taken into account PMM-5060 : Query Analytics Dashboard did not show the row with the last query of the first page, if the number of queries to display was 11","title":"PMM 2.2.2"},{"location":"release-notes/2.2.2.html#percona-monitoring-and-management-222","text":"Date: February 4, 2020 Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. For PMM install instructions, see Installing PMM Server and Installing PMM Client . Caution PMM 2 is designed to be used as a new installation \u2014 please don\u2019t try to upgrade your existing PMM 1 environment.","title":"Percona Monitoring and Management 2.2.2"},{"location":"release-notes/2.2.2.html#improvements-and-new-features","text":"PMM-5321 : The optimization of the Query Analytics parser code for PostgreSQL queries allowed us to reduce the memory resources consumption by 1-5%, and the parsing time of an individual query by 30 to 40% PMM-5184 : The pmm-admin summary command have gained a new --skip-server flag which makes it operating in a local-only mode, creating summary file without contacting the PMM Server","title":"Improvements and new features"},{"location":"release-notes/2.2.2.html#fixed-bugs","text":"PMM-5340 : The Scraping Time Drift graph on the Prometheus dashboard was showing wrong values because the actual metrics resolution wasn\u2019t taken into account PMM-5060 : Query Analytics Dashboard did not show the row with the last query of the first page, if the number of queries to display was 11","title":"Fixed bugs"},{"location":"release-notes/2.20.0.html","text":"Percona Monitoring and Management 2.20.0 \u00b6 Date: August 3, 2021 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. Release Highlights \u00b6 The Easy-install script for PMM Server is checksum verified. PMM will use Grafana 7.5 instead of 7.3. We are also preparing for Grafana 8. PostgreSQL monitoring with the pg_stat_monitor plugin enabled exposes new metrics such as Plan Times, WAL Fpi/Bytes/Records. For users who deploy PMM Server through the AWS Marketplace, AWS RDS service discovery will be executed without AWS credentials and tuning IAM roles . For Backup Management (Technical Preview), we added the ability to schedule backups so you can schedule and see already scheduled backups in the UI. Important note for users of PMM who started out using the Docker image of 2.16.0 \u00b6 If you installed PMM version 2.16 as a new Docker image and have since used the home dashboard upgrade widget to upgrade to any of 2.17, 2.18, or 2.19, you might experience problems with monitoring the PMM server itself, Remote Monitoring, or RDS/Azure monitoring. If you experience any of these problems, you can simply run the following commands to get your instance working and it will be automatically resolved in the next release: Enter the container: docker exec -it pmm-server bash Roll back pmm2-client package to stable version: yum downgrade -y pmm2-client Alternatively, you can replace the existing Docker container with a fresh install of the latest release by following the official instructions for an upgrade . (This will guide you through taking a backup of your PMM Server and restoring it after installing a fresh docker image of PMM Server.) New Features \u00b6 PMM-8157 : Backup Management: Show scheduled backups \u2013 A new view that shows the list of scheduled backups with quick actions to enable/disable, edit, copy, and delete. PMM-8155 : Backup Management: Scheduling of backups \u2013 Support for Backup Scheduling has been added so that users can define backup jobs to run automatically in the future with the option of making the schedules recurring. PMM-7010 : Option to unregister current node ( pmm-admin unregister ) Improvements \u00b6 PMM-7552 : PMM Server Docker image: Add labels to PMM Server Docker image (Thanks to Nicolas for reporting this issue) PMM-8266 : PMM Server Docker image: Decommission and remove Prometheus binaries and configuration PMM-8040 : PMM Server Docker image: Remove yum cache to reduce size of image PMM-7809 : Grafana upgrade from 7.3.7 to 7.5.7 \u2013 Read more at grafana.com PMM-8386 : Overview text on the Home page is missing PostgreSQL as a supported technology PMM-7722 : DBaaS: Announce new supported version of operator \u2013 Shows that a new version of the operator is available. PMM-6278 : Modification of MySQL \u2018Delete\u2019 queries to provide \u2018Explain\u2019 information PMM-8468 : Forbid the use of outdated ciphers for HTTPS protocol on exporters PMM-7649 : Security Checks: Show \u201cInsufficient access permissions\u201d in UI for non admin users PMM-8059 : Update Query Analytics UI to clarify estimated results on MySQL \u2018explain\u2019 response where we modified original query PMM-8043 : Return Service Name in GetCheckResults API response PMM-8000 : Expose new numbered metrics available in pg_stat_monitor 0.9 Bugs Fixed \u00b6 PMM-8299 : Backup Management: Remove storage location shows wrong notification \u2013 When removing a storage location, PMM presents misleading information to the user in the form of two notification messages for both error and success. PMM-8283 : Backup Management: Error when removing location with \u2018force delete\u2019 option PMM-8064 : Dashboards: Size of Temp Files Report Metric value has wrong unit on PostgreSQL Instance Summary Dashboard PMM-6981 : Dashboards: Wrong version is shown for MariaDB services PMM-7738 : Integrated Alerting: Alerts for some built-in templates missing service name label PMM-6877 : mongodb_exporter doesn\u2019t recognize being on a mongos host and fills the syslog with replSetGetStatus errors (Thanks to Clyde Shillingford for reporting this issue) PMM-7627 : Consistent PMM Server log management \u2013 Adds consistency to the log management of nginx , postgresql and clickhouse-server , which is now delegated to supervisord . Removes the logrotate daemon from the image. PMM-8492 : PMM Client version is 2.21.0 inside PMM Server after upgrade from 2.16.0 Known Issues (unfixed problems that you should be aware of) \u00b6 PMM-8414 : Backup Scheduler not working if user specifies explicit job start time","title":"PMM 2.20.0"},{"location":"release-notes/2.20.0.html#percona-monitoring-and-management-2200","text":"Date: August 3, 2021 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance.","title":"Percona Monitoring and Management 2.20.0"},{"location":"release-notes/2.20.0.html#release-highlights","text":"The Easy-install script for PMM Server is checksum verified. PMM will use Grafana 7.5 instead of 7.3. We are also preparing for Grafana 8. PostgreSQL monitoring with the pg_stat_monitor plugin enabled exposes new metrics such as Plan Times, WAL Fpi/Bytes/Records. For users who deploy PMM Server through the AWS Marketplace, AWS RDS service discovery will be executed without AWS credentials and tuning IAM roles . For Backup Management (Technical Preview), we added the ability to schedule backups so you can schedule and see already scheduled backups in the UI.","title":"Release Highlights"},{"location":"release-notes/2.20.0.html#important-note-for-users-of-pmm-who-started-out-using-the-docker-image-of-2160","text":"If you installed PMM version 2.16 as a new Docker image and have since used the home dashboard upgrade widget to upgrade to any of 2.17, 2.18, or 2.19, you might experience problems with monitoring the PMM server itself, Remote Monitoring, or RDS/Azure monitoring. If you experience any of these problems, you can simply run the following commands to get your instance working and it will be automatically resolved in the next release: Enter the container: docker exec -it pmm-server bash Roll back pmm2-client package to stable version: yum downgrade -y pmm2-client Alternatively, you can replace the existing Docker container with a fresh install of the latest release by following the official instructions for an upgrade . (This will guide you through taking a backup of your PMM Server and restoring it after installing a fresh docker image of PMM Server.)","title":"Important note for users of PMM who started out using the Docker image of 2.16.0"},{"location":"release-notes/2.20.0.html#new-features","text":"PMM-8157 : Backup Management: Show scheduled backups \u2013 A new view that shows the list of scheduled backups with quick actions to enable/disable, edit, copy, and delete. PMM-8155 : Backup Management: Scheduling of backups \u2013 Support for Backup Scheduling has been added so that users can define backup jobs to run automatically in the future with the option of making the schedules recurring. PMM-7010 : Option to unregister current node ( pmm-admin unregister )","title":"New Features"},{"location":"release-notes/2.20.0.html#improvements","text":"PMM-7552 : PMM Server Docker image: Add labels to PMM Server Docker image (Thanks to Nicolas for reporting this issue) PMM-8266 : PMM Server Docker image: Decommission and remove Prometheus binaries and configuration PMM-8040 : PMM Server Docker image: Remove yum cache to reduce size of image PMM-7809 : Grafana upgrade from 7.3.7 to 7.5.7 \u2013 Read more at grafana.com PMM-8386 : Overview text on the Home page is missing PostgreSQL as a supported technology PMM-7722 : DBaaS: Announce new supported version of operator \u2013 Shows that a new version of the operator is available. PMM-6278 : Modification of MySQL \u2018Delete\u2019 queries to provide \u2018Explain\u2019 information PMM-8468 : Forbid the use of outdated ciphers for HTTPS protocol on exporters PMM-7649 : Security Checks: Show \u201cInsufficient access permissions\u201d in UI for non admin users PMM-8059 : Update Query Analytics UI to clarify estimated results on MySQL \u2018explain\u2019 response where we modified original query PMM-8043 : Return Service Name in GetCheckResults API response PMM-8000 : Expose new numbered metrics available in pg_stat_monitor 0.9","title":"Improvements"},{"location":"release-notes/2.20.0.html#bugs-fixed","text":"PMM-8299 : Backup Management: Remove storage location shows wrong notification \u2013 When removing a storage location, PMM presents misleading information to the user in the form of two notification messages for both error and success. PMM-8283 : Backup Management: Error when removing location with \u2018force delete\u2019 option PMM-8064 : Dashboards: Size of Temp Files Report Metric value has wrong unit on PostgreSQL Instance Summary Dashboard PMM-6981 : Dashboards: Wrong version is shown for MariaDB services PMM-7738 : Integrated Alerting: Alerts for some built-in templates missing service name label PMM-6877 : mongodb_exporter doesn\u2019t recognize being on a mongos host and fills the syslog with replSetGetStatus errors (Thanks to Clyde Shillingford for reporting this issue) PMM-7627 : Consistent PMM Server log management \u2013 Adds consistency to the log management of nginx , postgresql and clickhouse-server , which is now delegated to supervisord . Removes the logrotate daemon from the image. PMM-8492 : PMM Client version is 2.21.0 inside PMM Server after upgrade from 2.16.0","title":"Bugs Fixed"},{"location":"release-notes/2.20.0.html#known-issues-unfixed-problems-that-you-should-be-aware-of","text":"PMM-8414 : Backup Scheduler not working if user specifies explicit job start time","title":"Known Issues (unfixed problems that you should be aware of)"},{"location":"release-notes/2.21.0.html","text":"Percona Monitoring and Management 2.21.0 \u00b6 Date: August 26, 2021 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. Release Highlights: \u00b6 Custom certificates support : We have added support for custom TLS certificates for remote monitoring of PostgreSQL and MongoDB services, configurable on the command line with pmm-admin or through the UI. Backup scheduling with retention (Technical Preview): When scheduling a backup you can now specify how many of the most recent backups to keep. Backups not in this range are automatically deleted to free space for new ones. New supported versions: DBaaS functionality now supports Kubernetes Operator for MongoDB version 1.9.0. PMM Client packages now support Debian 11 (\u201cBullseye\u201d). New Features \u00b6 PMM-8158 : Backup Management: Delete option \u2014 When deleting a backup from Backup Management inventory a new option lets you also delete the data file from storage. PMM-8156 : Backup Management: Retention \u2014 You can now define how many of the most recent scheduled backups to keep. PMM-8214 : Ability to collect Kubernetes cluster metrics \u2014 Collection only, metrics are not currently presented on any PMM dashboard. PMM-7477 : Support custom TLS certificates when monitoring remote MongoDB instances PMM-7888 : Custom TLS certificates now allow SSL connections to PostgreSQL instances (Thanks to Jyoti Prakash for reporting this issue) Improvements \u00b6 PMM-8267 : Backup Management: Active progress indicator PMM-8549 : Backup Management: Show loading status on delete window PMM-8542 : Backup Management: Inform that times should be entered in UTC timezone format PMM-8316 : DBaaS: PSMDB 1.9 operator support \u2014For what\u2019s new see release notes . PMM-7612 : Integrated Alerting: Validate communication settings \u2018From\u2019 email address format PMM-7570 : Specify Custom Basic Auth password for Agents when adding Services PMM-8560 : Add support for Debian 11 (\u201cBullseye\u201d) to pmm-client package PMM-7087 : Rename custom query file to example-queries-postgres.yml and include warning that the file will be overwritten on upgrade; user should create a copy with a new name to prevent losing metrics collection on future upgrades. (Thanks to Daniel Guzman Burgos for reporting this issue) PMM-8568 : Use latest CentOS patches for creating OVA, AMI and Azure images PMM-5291 : Update ClickHouse version from 19.7.5.27 to 21.3-lts PMM-8091 : Collect and present additional ProxySQL metrics taken from runtime_mysql_servers table Bugs Fixed \u00b6 PMM-8616 : Backup Management: No \u2018Delete from storage\u2019 action on backup inventory PMM-8543 : Backups are not visible after PMM Server upgrade PMM-8458 : Backup Management: Inconsistent auto-fill of \u2018Vendor\u2019 field with on-demand backup PMM-8404 : Dashboard image rendering plugin renders image that includes error message PMM-7286 : Query Analytics can\u2019t handle colon character ( : ) in service names (Thanks to Daniel Guzman Burgos for reporting this issue) PMM-7278 : mongo_exporter fails to authorize when MongoDB running with authMechanism=MONGODB-X509 (Thanks to Lena D for reporting this issue) PMM-8307 : Default config limits for allowed connections prevents monitoring large numbers (500+) of DB servers PMM-2168 : rds_exporter not stopped when all RDS instances are removed or disabled PMM-8219 : PMM Server update panel \u201cCurrent version\u201d empty if no internet connectivity PMM-8559 : Unauthorized error appears while logging in Known Issues \u00b6 Failure to upgrade when using a dashboard with custom tags. Important In some cases users may not be able to complete the upgrade to 2.21.0 and we have linked this back to dashboards with custom tags. This is to be fixed in our upcoming 2.22.0 release but there are steps (more in the ticket ) that you can take if you\u2019re already impacted by this: curl -LJOs https://raw.githubusercontent.com/percona/pmm-server/c2e92bc3aec123affda5f1992c96c95ac74f4a2d/import-dashboards.py docker cp import-dashboards.py pmm-server:/usr/share/percona-dashboards/ docker exec -it pmm-server chmod a+x /usr/share/percona-dashboards/import-dashboards.py","title":"PMM 2.21.0"},{"location":"release-notes/2.21.0.html#percona-monitoring-and-management-2210","text":"Date: August 26, 2021 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance.","title":"Percona Monitoring and Management 2.21.0"},{"location":"release-notes/2.21.0.html#release-highlights","text":"Custom certificates support : We have added support for custom TLS certificates for remote monitoring of PostgreSQL and MongoDB services, configurable on the command line with pmm-admin or through the UI. Backup scheduling with retention (Technical Preview): When scheduling a backup you can now specify how many of the most recent backups to keep. Backups not in this range are automatically deleted to free space for new ones. New supported versions: DBaaS functionality now supports Kubernetes Operator for MongoDB version 1.9.0. PMM Client packages now support Debian 11 (\u201cBullseye\u201d).","title":"Release Highlights:"},{"location":"release-notes/2.21.0.html#new-features","text":"PMM-8158 : Backup Management: Delete option \u2014 When deleting a backup from Backup Management inventory a new option lets you also delete the data file from storage. PMM-8156 : Backup Management: Retention \u2014 You can now define how many of the most recent scheduled backups to keep. PMM-8214 : Ability to collect Kubernetes cluster metrics \u2014 Collection only, metrics are not currently presented on any PMM dashboard. PMM-7477 : Support custom TLS certificates when monitoring remote MongoDB instances PMM-7888 : Custom TLS certificates now allow SSL connections to PostgreSQL instances (Thanks to Jyoti Prakash for reporting this issue)","title":"New Features"},{"location":"release-notes/2.21.0.html#improvements","text":"PMM-8267 : Backup Management: Active progress indicator PMM-8549 : Backup Management: Show loading status on delete window PMM-8542 : Backup Management: Inform that times should be entered in UTC timezone format PMM-8316 : DBaaS: PSMDB 1.9 operator support \u2014For what\u2019s new see release notes . PMM-7612 : Integrated Alerting: Validate communication settings \u2018From\u2019 email address format PMM-7570 : Specify Custom Basic Auth password for Agents when adding Services PMM-8560 : Add support for Debian 11 (\u201cBullseye\u201d) to pmm-client package PMM-7087 : Rename custom query file to example-queries-postgres.yml and include warning that the file will be overwritten on upgrade; user should create a copy with a new name to prevent losing metrics collection on future upgrades. (Thanks to Daniel Guzman Burgos for reporting this issue) PMM-8568 : Use latest CentOS patches for creating OVA, AMI and Azure images PMM-5291 : Update ClickHouse version from 19.7.5.27 to 21.3-lts PMM-8091 : Collect and present additional ProxySQL metrics taken from runtime_mysql_servers table","title":"Improvements"},{"location":"release-notes/2.21.0.html#bugs-fixed","text":"PMM-8616 : Backup Management: No \u2018Delete from storage\u2019 action on backup inventory PMM-8543 : Backups are not visible after PMM Server upgrade PMM-8458 : Backup Management: Inconsistent auto-fill of \u2018Vendor\u2019 field with on-demand backup PMM-8404 : Dashboard image rendering plugin renders image that includes error message PMM-7286 : Query Analytics can\u2019t handle colon character ( : ) in service names (Thanks to Daniel Guzman Burgos for reporting this issue) PMM-7278 : mongo_exporter fails to authorize when MongoDB running with authMechanism=MONGODB-X509 (Thanks to Lena D for reporting this issue) PMM-8307 : Default config limits for allowed connections prevents monitoring large numbers (500+) of DB servers PMM-2168 : rds_exporter not stopped when all RDS instances are removed or disabled PMM-8219 : PMM Server update panel \u201cCurrent version\u201d empty if no internet connectivity PMM-8559 : Unauthorized error appears while logging in","title":"Bugs Fixed"},{"location":"release-notes/2.21.0.html#known-issues","text":"Failure to upgrade when using a dashboard with custom tags. Important In some cases users may not be able to complete the upgrade to 2.21.0 and we have linked this back to dashboards with custom tags. This is to be fixed in our upcoming 2.22.0 release but there are steps (more in the ticket ) that you can take if you\u2019re already impacted by this: curl -LJOs https://raw.githubusercontent.com/percona/pmm-server/c2e92bc3aec123affda5f1992c96c95ac74f4a2d/import-dashboards.py docker cp import-dashboards.py pmm-server:/usr/share/percona-dashboards/ docker exec -it pmm-server chmod a+x /usr/share/percona-dashboards/import-dashboards.py","title":"Known Issues"},{"location":"release-notes/2.22.0.html","text":"Percona Monitoring and Management 2.22.0 \u00b6 Date: September 23, 2021 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. Release Highlights \u00b6 DBaaS (Technical preview): DBaaS users can now use the PMM UI to upgrade existing Clusters to the newer version of the operator without interacting directly with Kubernetes. New Features \u00b6 PMM-8396 : Backup Management: Add an ability to specify the number of retries for Scheduled backups before operation fails PMM-8079 : DBaaS: Percona Operators inside Kubernetes cluster managed by PMM can now be upgraded PMM-8077 : DBaaS: Show the current version of Operators used in Kubernetes Cluster PMM-7924 : MySQL Performance Details dashboard: Add \u201cPerformance Schema Status Monitoring\u201d chart Improvements \u00b6 PMM-8688 : Backup Management: When adding a new Backup, the action button now reads \u201cCreate Backup\u201d instead of \u201cEdit\u201d to reflect the action happening PMM-8311 : Integrated Alerting: Disable edit/delete buttons for Percona-sourced Templates PMM-8509 : Management of ability to update PMM Server in the same way as this implemented to other Settings for PMM. Users can use API, UI, or docker Environment Variables to change the setting responsible for the Update process. As with all PMM settings, environment variables have higher priority and can\u2019t be changed with the API or in the UI. PMM-7392 : DBaaS: Change Number of Nodes when editing Topology Bugs Fixed \u00b6 PMM-8613 : Backup Management: Unable to restore MySQL backup PMM-8463 : Backup Management: State stuck on \u201cPending\u201d when creating backup with already existing name PMM-8408 : DBaaS: Development version of PMM Client was used for monitoring DB Clusters created by DBaaS PMM-8584 : Wrong CPU metric labels in dashboards for RDS instances PMM-8421 : Listen-port ignored/removed for external services after server update to PMM 2.19 and higher (Thanks to Rainer Plischke for reporting this issue). Please make sure to upgrade PMM Server to avoid loss of external exporter listen port ( PMM-8829 ) and always upgrade PMM Server before PMM Client ( PMM-8854 ). PMM-8703 : Custom dashboard prevents PMM Server Docker update from 2.20 to 2.21 (Thanks to Hubertus Krogmann for reporting this issue) PMM-7690 : AWS discovery and monitoring based on IAM roles is not working If you have problems upgrading your pmm-2-client packages, try clearing caches with: sudo apt-get clean and remove files manually with: cd /var/cache/apt/archives && sudo rm -rf ./*","title":"PMM 2.22.0"},{"location":"release-notes/2.22.0.html#percona-monitoring-and-management-2220","text":"Date: September 23, 2021 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance.","title":"Percona Monitoring and Management 2.22.0"},{"location":"release-notes/2.22.0.html#release-highlights","text":"DBaaS (Technical preview): DBaaS users can now use the PMM UI to upgrade existing Clusters to the newer version of the operator without interacting directly with Kubernetes.","title":"Release Highlights"},{"location":"release-notes/2.22.0.html#new-features","text":"PMM-8396 : Backup Management: Add an ability to specify the number of retries for Scheduled backups before operation fails PMM-8079 : DBaaS: Percona Operators inside Kubernetes cluster managed by PMM can now be upgraded PMM-8077 : DBaaS: Show the current version of Operators used in Kubernetes Cluster PMM-7924 : MySQL Performance Details dashboard: Add \u201cPerformance Schema Status Monitoring\u201d chart","title":"New Features"},{"location":"release-notes/2.22.0.html#improvements","text":"PMM-8688 : Backup Management: When adding a new Backup, the action button now reads \u201cCreate Backup\u201d instead of \u201cEdit\u201d to reflect the action happening PMM-8311 : Integrated Alerting: Disable edit/delete buttons for Percona-sourced Templates PMM-8509 : Management of ability to update PMM Server in the same way as this implemented to other Settings for PMM. Users can use API, UI, or docker Environment Variables to change the setting responsible for the Update process. As with all PMM settings, environment variables have higher priority and can\u2019t be changed with the API or in the UI. PMM-7392 : DBaaS: Change Number of Nodes when editing Topology","title":"Improvements"},{"location":"release-notes/2.22.0.html#bugs-fixed","text":"PMM-8613 : Backup Management: Unable to restore MySQL backup PMM-8463 : Backup Management: State stuck on \u201cPending\u201d when creating backup with already existing name PMM-8408 : DBaaS: Development version of PMM Client was used for monitoring DB Clusters created by DBaaS PMM-8584 : Wrong CPU metric labels in dashboards for RDS instances PMM-8421 : Listen-port ignored/removed for external services after server update to PMM 2.19 and higher (Thanks to Rainer Plischke for reporting this issue). Please make sure to upgrade PMM Server to avoid loss of external exporter listen port ( PMM-8829 ) and always upgrade PMM Server before PMM Client ( PMM-8854 ). PMM-8703 : Custom dashboard prevents PMM Server Docker update from 2.20 to 2.21 (Thanks to Hubertus Krogmann for reporting this issue) PMM-7690 : AWS discovery and monitoring based on IAM roles is not working If you have problems upgrading your pmm-2-client packages, try clearing caches with: sudo apt-get clean and remove files manually with: cd /var/cache/apt/archives && sudo rm -rf ./*","title":"Bugs Fixed"},{"location":"release-notes/2.23.0.html","text":"Percona Monitoring and Management 2.23.0 \u00b6 Date: October 21, 2021 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. Release Highlights \u00b6 Grafana Upgrade Embedded Grafana version was upgraded from 7.5.7 to 7.5.11 to improve some Grafana instrumentation metrics. Also, the upgrade is delivering a security fix that impacted Grafana. Please upgrade to the latest version of PMM ASAP. Read more about CVE issue here Backup Management (Technical preview): An ability to view logs of the backup process for better visibility over the backup process An ability to schedule Point-In-Time-Recoverable backup from MongoDB clusters with the correct configuration. Note: there is no UI to restore PITR for MongoDB at the moment. It will come with future releases, but it is possible to restore a PITR backup with Percona Backup for MongoDB manually DBaaS (Technical preview): From this release on, PMM users who are using the DBaaS feature will be able to update versions of their DBaaS controlled Databases by the push of a button according to each DB\u2019s compatibility matrix. Please note that we recently found a bug PMM-8723 that was causing significant problems with DBaaS usage. This bug was fixed in this release, and no additional actions will be required. New Features \u00b6 PMM-8269 : Backup Management: Ability to schedule Point-In-Time-Recoverable backups for MongoDB PMM-8159 : Backup Management: Ability to see a logs of backup process for MongoDB PMM-7519 : Backup Management: Version compatibility check prior to attempted MySQL data restoration PMM-8200 : DBaaS: Admin can now initiate a DB version upgrade with just a button click PMM-8273 : Integrated Alerting: Alert templates delivery from Percona.com for anonymous PMM servers Improvements \u00b6 PMM-8973 : Grafana upgrade from 7.5.7 to 7.5.11 Includes better Grafana instrumentation metrics and fix for CVE-2021-39226 (read more on Grafana blog ) PMM-8653 : Added titles to Home Dashboard panels for better readability PMM-8669 : Integrated Alerting: Create a clearer distinction about using PMM Alerting as preferred method vs using an external Alertmanager PMM-8539 : Wrong Cluster Role presentation on MongoDB Cluster Summary PMM-7559 : Integrated Alerting: Improve error message when trying to delete a channel that is used by a rule PMM-6763 : Better color contrast in Time distribution in QAN details PMM-5669 : New flag \u2013paths-base in pmm-agent to avoid problems with hardcoded paths. Please note: this is possible if you run pmm-agent separately from pmm-admin. The ability to specify base paths over pmm-admin is not yet implemented Bugs Fixed \u00b6 PMM-7985 : Users were losing manually installed Grafana plugins after upgrade via Docker Caution The issue is fixed automatically since 2.23.0 version forward. For the upgrades from versions before 2.23.0 please backup plugins first. PMM-8767 : Copied dashboards with tags were endeding up in unexpected folder after upgrade PMM-8635 : MyRocks WAL panel from MySQL MyRocks Details Dashboard presented data in wrong units PMM-8527 : Dashboards: ProxySQL/HAProxy DB Conns, DB QPS, DB uptime metrics were missing on Home dashboard panels PMM-8749 : Adding more than 1 mongos was breaking MongoDB Cluster Summary dashboard PMM-8004 : Fixed broken metrics reporting in case of lost connection to MongoDB. (Thanks to \u00c1lvaro L\u00f3pez L\u00f3pez for reporting this issue) PMM-8489 : Failed to get topology labels when target server is mongos PMM-6877 : Fixed error flooding from when monitoring mongos (Thanks to Clyde Shillingford for reporting this issue) PMM-8851 : Can\u2019t monitor GCP Cloud SQL or other PostgreSQL with custom SSL certificates (Thanks to Jyoti Prakash for reporting this issue) PMM-8646 : PostgreSQL services monitoring was stalled after intermittent connection latency PMM-8723 : PMM wouldn\u2019t restart DBaaS functionality and would break it after upgrade via UI. Affecting versions starting from 2.17.0","title":"PMM 2.23.0"},{"location":"release-notes/2.23.0.html#percona-monitoring-and-management-2230","text":"Date: October 21, 2021 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance.","title":"Percona Monitoring and Management 2.23.0"},{"location":"release-notes/2.23.0.html#release-highlights","text":"Grafana Upgrade Embedded Grafana version was upgraded from 7.5.7 to 7.5.11 to improve some Grafana instrumentation metrics. Also, the upgrade is delivering a security fix that impacted Grafana. Please upgrade to the latest version of PMM ASAP. Read more about CVE issue here Backup Management (Technical preview): An ability to view logs of the backup process for better visibility over the backup process An ability to schedule Point-In-Time-Recoverable backup from MongoDB clusters with the correct configuration. Note: there is no UI to restore PITR for MongoDB at the moment. It will come with future releases, but it is possible to restore a PITR backup with Percona Backup for MongoDB manually DBaaS (Technical preview): From this release on, PMM users who are using the DBaaS feature will be able to update versions of their DBaaS controlled Databases by the push of a button according to each DB\u2019s compatibility matrix. Please note that we recently found a bug PMM-8723 that was causing significant problems with DBaaS usage. This bug was fixed in this release, and no additional actions will be required.","title":"Release Highlights"},{"location":"release-notes/2.23.0.html#new-features","text":"PMM-8269 : Backup Management: Ability to schedule Point-In-Time-Recoverable backups for MongoDB PMM-8159 : Backup Management: Ability to see a logs of backup process for MongoDB PMM-7519 : Backup Management: Version compatibility check prior to attempted MySQL data restoration PMM-8200 : DBaaS: Admin can now initiate a DB version upgrade with just a button click PMM-8273 : Integrated Alerting: Alert templates delivery from Percona.com for anonymous PMM servers","title":"New Features"},{"location":"release-notes/2.23.0.html#improvements","text":"PMM-8973 : Grafana upgrade from 7.5.7 to 7.5.11 Includes better Grafana instrumentation metrics and fix for CVE-2021-39226 (read more on Grafana blog ) PMM-8653 : Added titles to Home Dashboard panels for better readability PMM-8669 : Integrated Alerting: Create a clearer distinction about using PMM Alerting as preferred method vs using an external Alertmanager PMM-8539 : Wrong Cluster Role presentation on MongoDB Cluster Summary PMM-7559 : Integrated Alerting: Improve error message when trying to delete a channel that is used by a rule PMM-6763 : Better color contrast in Time distribution in QAN details PMM-5669 : New flag \u2013paths-base in pmm-agent to avoid problems with hardcoded paths. Please note: this is possible if you run pmm-agent separately from pmm-admin. The ability to specify base paths over pmm-admin is not yet implemented","title":"Improvements"},{"location":"release-notes/2.23.0.html#bugs-fixed","text":"PMM-7985 : Users were losing manually installed Grafana plugins after upgrade via Docker Caution The issue is fixed automatically since 2.23.0 version forward. For the upgrades from versions before 2.23.0 please backup plugins first. PMM-8767 : Copied dashboards with tags were endeding up in unexpected folder after upgrade PMM-8635 : MyRocks WAL panel from MySQL MyRocks Details Dashboard presented data in wrong units PMM-8527 : Dashboards: ProxySQL/HAProxy DB Conns, DB QPS, DB uptime metrics were missing on Home dashboard panels PMM-8749 : Adding more than 1 mongos was breaking MongoDB Cluster Summary dashboard PMM-8004 : Fixed broken metrics reporting in case of lost connection to MongoDB. (Thanks to \u00c1lvaro L\u00f3pez L\u00f3pez for reporting this issue) PMM-8489 : Failed to get topology labels when target server is mongos PMM-6877 : Fixed error flooding from when monitoring mongos (Thanks to Clyde Shillingford for reporting this issue) PMM-8851 : Can\u2019t monitor GCP Cloud SQL or other PostgreSQL with custom SSL certificates (Thanks to Jyoti Prakash for reporting this issue) PMM-8646 : PostgreSQL services monitoring was stalled after intermittent connection latency PMM-8723 : PMM wouldn\u2019t restart DBaaS functionality and would break it after upgrade via UI. Affecting versions starting from 2.17.0","title":"Bugs Fixed"},{"location":"release-notes/2.24.0.html","text":"Percona Monitoring and Management 2.24.0 \u00b6 Date: November 18, 2021 Installation: Installing Percona Monitoring and Management Important note for users of PMM 2.24.0 2.24.0 AMI image has only 8GB available for the data, it is a bug (see PMM-9298 ). To resize a disk to full size you need to login to AMI instance with SSH and use the following command: curl https://raw.githubusercontent.com/percona/pmm-update/main/ansible/playbook/tasks/create-lvm.yml -o lvn-fix.yml && sudo ansible-playbook lvn-fix.yml For instructions about how to access your instances by using an SSH client, see Connecting to Your Linux Instance Using SSH Make sure to replace the user name ec2-user used in this document with admin. What this command does: Downloads Ansible playbook and runs it Copy your data from /srv to the temporary directory Create lvm partition Copy data from system disk to a new LVM partition Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. Release Highlights \u00b6 Enhanced PostgreSQL monitoring Beginning with this release, PMM now supports monitoring of PostgreSQL 14, both - Community edition and Percona Distribution for PostgreSQL . We\u2019ve made significant improvements in exposed data and added additional features if you monitor PostgreSQL together with the pg_stat_monitor extension (Part of Percona distribution for PostgreSQL). These features include: The PostgreSQL queries will have complete Query Execution Plan information. This will help with future optimization of queries and give a clear understanding of query performance Query execution histograms collection and presentation inside Query Analytics for a much more detailed understanding of query executions. Query analytics will both show and let the user drill down to the Top Query of the particular query if it\u2019s a subquery and have this parent query. This feature will allow users to see the dependencies between queries better and understand the impact of subqueries. Query Analytics can filter PostgreSQL queries by query commands like SELECT, UPDATE, etc., and by Application Name if it\u2019s set for PostgreSQL connection from the application. Integrated Alerting (Technical preview): Alerting in PMM now has an additional notification channel type - webhooks. So now, users can integrate Alerting with any tool they use for Incident management. Read more about new notification channels and how to set them up in our documentation New Features \u00b6 PMM-8027 : Integrated Alerting: New notification channel added: Webhooks PMM-8301 : Add data collection support and visualization for newly added dimensions in pg_stat_monitor such as Application Name,Top Query, Plan in Query Analytics PMM-8588 : PostgreSQL Histograms added to QAN when using pg_stat_monitor extension PMM-8632 : New Filter: \u201cCommand Type\u201d allows filtering queries based on type (SELECT, INSERT, UPDATE, DELETE, n/a) when pg_stat_monitor extension enabled Improvements \u00b6 PMM-8803 : Backup Management: Improved error messages to indicate incompatible versions of software PMM-8636 : Integrated Alerting: Additional context to alerts to better convey issue detected PMM-8644 : Integrated Alerting: API should allow textual TLS configs for webhooks PMM-8122 : Integrated Alerting: UI does not indicate a port is needed in configuration for SMTP communication channel PMM-8484 : Added support for PostgreSQL 14 and Percona Distribution for PostgreSQL 14 PMM-7297 : Updated plugin for Clickhouse data source from 2.1.0 to 2.3.1. This fixes some bugs and eliminates noise from warnings in logs as well as adding support of new types (DateTime64) and improved ARRAY JOIN parsing Bugs Fixed \u00b6 PMM-8975 : Backup Management: long presentation of recurrent intervals in Backup scheduling PMM-8541 : Navigating through PMM Settings link at Failed security checks panel takes more than 30 seconds PMM-8387 : MySQL InnoDB Details dashboard is not in the left menu PMM-8858 : Dashboards: No Host uptime on Homepage for RDS instances PMM-8611 : Dashboards: PMM Agents status presented as DOWN while there is no recent data yet on the status PMM-8393 : Integrated Alerting: Alert rules not executed after upgrading PMM Server running as Docker container PMM-8058 : Integrated Alerting: Firing alerts disappear after PMM server restart PMM-8089 : PMM is not exposing data for memory used by MongoDB when it\u2019s mapped with the journal. This was inconsistent behavior compared to older versions of PMM. PMM-9100 : Dashboards: Binary Log related metrics on MySQL Replication dashboard are not prevented and not collected for MySQL8 PMM-8633 : Unfinished queries are included in Query Analytics for PostgreSQL with pg_stat_monitor usage because of incorrect use of state_code. PMM-8859 : Increased memory consumption on Client-side for PostgreSQL monitoring when executing either too many custom queries or some queries against too many tables PMM-9046 : Incorrect link to instructions about installing Image Rendering Plugin PMM-8952 : Query Analytics: No table/indexes information for Views when PostgreSQL server monitored with pg_stat_monitor","title":"PMM 2.24.0"},{"location":"release-notes/2.24.0.html#percona-monitoring-and-management-2240","text":"Date: November 18, 2021 Installation: Installing Percona Monitoring and Management Important note for users of PMM 2.24.0 2.24.0 AMI image has only 8GB available for the data, it is a bug (see PMM-9298 ). To resize a disk to full size you need to login to AMI instance with SSH and use the following command: curl https://raw.githubusercontent.com/percona/pmm-update/main/ansible/playbook/tasks/create-lvm.yml -o lvn-fix.yml && sudo ansible-playbook lvn-fix.yml For instructions about how to access your instances by using an SSH client, see Connecting to Your Linux Instance Using SSH Make sure to replace the user name ec2-user used in this document with admin. What this command does: Downloads Ansible playbook and runs it Copy your data from /srv to the temporary directory Create lvm partition Copy data from system disk to a new LVM partition Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance.","title":"Percona Monitoring and Management 2.24.0"},{"location":"release-notes/2.24.0.html#release-highlights","text":"Enhanced PostgreSQL monitoring Beginning with this release, PMM now supports monitoring of PostgreSQL 14, both - Community edition and Percona Distribution for PostgreSQL . We\u2019ve made significant improvements in exposed data and added additional features if you monitor PostgreSQL together with the pg_stat_monitor extension (Part of Percona distribution for PostgreSQL). These features include: The PostgreSQL queries will have complete Query Execution Plan information. This will help with future optimization of queries and give a clear understanding of query performance Query execution histograms collection and presentation inside Query Analytics for a much more detailed understanding of query executions. Query analytics will both show and let the user drill down to the Top Query of the particular query if it\u2019s a subquery and have this parent query. This feature will allow users to see the dependencies between queries better and understand the impact of subqueries. Query Analytics can filter PostgreSQL queries by query commands like SELECT, UPDATE, etc., and by Application Name if it\u2019s set for PostgreSQL connection from the application. Integrated Alerting (Technical preview): Alerting in PMM now has an additional notification channel type - webhooks. So now, users can integrate Alerting with any tool they use for Incident management. Read more about new notification channels and how to set them up in our documentation","title":"Release Highlights"},{"location":"release-notes/2.24.0.html#new-features","text":"PMM-8027 : Integrated Alerting: New notification channel added: Webhooks PMM-8301 : Add data collection support and visualization for newly added dimensions in pg_stat_monitor such as Application Name,Top Query, Plan in Query Analytics PMM-8588 : PostgreSQL Histograms added to QAN when using pg_stat_monitor extension PMM-8632 : New Filter: \u201cCommand Type\u201d allows filtering queries based on type (SELECT, INSERT, UPDATE, DELETE, n/a) when pg_stat_monitor extension enabled","title":"New Features"},{"location":"release-notes/2.24.0.html#improvements","text":"PMM-8803 : Backup Management: Improved error messages to indicate incompatible versions of software PMM-8636 : Integrated Alerting: Additional context to alerts to better convey issue detected PMM-8644 : Integrated Alerting: API should allow textual TLS configs for webhooks PMM-8122 : Integrated Alerting: UI does not indicate a port is needed in configuration for SMTP communication channel PMM-8484 : Added support for PostgreSQL 14 and Percona Distribution for PostgreSQL 14 PMM-7297 : Updated plugin for Clickhouse data source from 2.1.0 to 2.3.1. This fixes some bugs and eliminates noise from warnings in logs as well as adding support of new types (DateTime64) and improved ARRAY JOIN parsing","title":"Improvements"},{"location":"release-notes/2.24.0.html#bugs-fixed","text":"PMM-8975 : Backup Management: long presentation of recurrent intervals in Backup scheduling PMM-8541 : Navigating through PMM Settings link at Failed security checks panel takes more than 30 seconds PMM-8387 : MySQL InnoDB Details dashboard is not in the left menu PMM-8858 : Dashboards: No Host uptime on Homepage for RDS instances PMM-8611 : Dashboards: PMM Agents status presented as DOWN while there is no recent data yet on the status PMM-8393 : Integrated Alerting: Alert rules not executed after upgrading PMM Server running as Docker container PMM-8058 : Integrated Alerting: Firing alerts disappear after PMM server restart PMM-8089 : PMM is not exposing data for memory used by MongoDB when it\u2019s mapped with the journal. This was inconsistent behavior compared to older versions of PMM. PMM-9100 : Dashboards: Binary Log related metrics on MySQL Replication dashboard are not prevented and not collected for MySQL8 PMM-8633 : Unfinished queries are included in Query Analytics for PostgreSQL with pg_stat_monitor usage because of incorrect use of state_code. PMM-8859 : Increased memory consumption on Client-side for PostgreSQL monitoring when executing either too many custom queries or some queries against too many tables PMM-9046 : Incorrect link to instructions about installing Image Rendering Plugin PMM-8952 : Query Analytics: No table/indexes information for Views when PostgreSQL server monitored with pg_stat_monitor","title":"Bugs Fixed"},{"location":"release-notes/2.25.0.html","text":"Percona Monitoring and Management 2.25.0 \u00b6 Date: December 14, 2021 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. Important note for users considering docker way upgrade to PMM 2.25.0 If you upgrade from a PMM version less than or equal to 2.23.0 using docker, it will fail if your PMM does not have external access (access to \u2018repo. percona.com\u2019). Thus, it is recommended to upgrade to PMM 2.26.0 instead. See PMM-9416 for more details. Release Highlights \u00b6 Percona Platform (Technology Preview): Connect Percona Monitoring and Management (PMM) to Percona Platform to boost the monitoring capabilities of your PMM installations and access all your Percona accounts and services from one single, centralized location. For more information, see the Percona Platform Portal documentation Enhanced PostgreSQL monitoring You can now specify custom database names when adding PostgreSQL Servers for monitoring. Previous PMM versions always used the default postgres name instead. Added support for the new version of pg_stat_monitor extension. Release Candidate v1.0.0-rc.1 brings many new PostgreSQL metrics, Dashboards and Query Analytics! To find out about all the features available in the new pg_stat_monitor version, see the pg_stat_monitor User guide Added compatibility for the latest Percona Distributions for PostgreSQL 14 , 13 , 12 , 11 updated on December 7, 2021, which includes the newest version of the pg_stat_monitor extension. Grafana usability enhancements PMM is now using the native Grafana provisioning mechanism for adding dashboards, plug-ins, and data sources. This ensures faster and more reliable upgrading to newer PMM versions. Your existing plug-ins and dashboard changes are preserved during upgrades, but always make sure to back them up before upgrading and check that everything transferred correctly afterward. Added option to change the time zone on dashboards. This selection is preserved while you navigate over Dashboards. If you need to change this setting permanently for your account, change it in your preferences by the URL: https://YOUR_PMM_SERVER/graph/profile DBaaS (Technical Preview) You can now deploy and update your DBaaS created PXC clusters to the latest version of Percona Distribution for MySQL Operator 1.9.0. This enables you to take advantage of the latest features and fixes. PMM environment enhancements The pmm-client docker container can now be started as a sidecar. For users that use PMM client in Kubernetes or build automation around it, you can now start the client as a sidecar container simply by passing a flag. The client will also gracefully handle any instances where the connection to DB is not available. For more details, see the PMM Client documentation Removed support for Ubuntu 16.04. With the support of new products and new versions of already supported products, we also removed old, unsupported software. As of this release, we are no longer supporting Ubuntu 16.04 in PMM according to recent anouncements New Features \u00b6 PMM-9050 : Connect PMM Server to Percona Platform for additional account info in PMM and value added content Improvements \u00b6 PMM-8545 : DBaaS: Support of Percona Distribution for MySQL Operator 1.9.0 in PMM PMM-7677 : Docker container for pmm-client : Option to change behavior and follow sidecar pattern The flags PMM_AGENT_SIDECAR and PMM_AGENT_SIDECAR_SLEEP does this. Read more in documentation PMM-3516 : Optimize provisioning mechanism of plugins, dashboards, and datasources in PMM PMM-8674 : Integrated Alerting: Add Tooltips to Add Alert Rule fields to make it easier to understand what information is needed PMM-8505 : Integrated Alerting: Clarify description of the \u2018Low memory\u2019 Alert Template PMM-8503 : Integrated Alerting: Field validation in Email and Slack tabs when updating settings PMM-7527 : Integrated Alerting: Improvements to overall user experience for action buttons in Alerting PMM-7079 : Integrated Alerting: New \u2018information\u2019 icon to give additional details about Alerts without cluttering screen PMM-8259 : Better clarification of error messages in pmm-admin when PMM server can\u2019t be unregistered PMM-8972 : Add ability to specify custom base path to exporters and tools using pmm-admin command PMM-8282 : Improved messaging for TLS option when adding Remote instances in PMM over UI Bugs Fixed \u00b6 PMM-9169 : Security Advisor Checks are not working for MongoDB instances PMM-8982 : Backup Management: User is not able to see MongoDB backup logs if backup was taken on older version of pmm-server PMM-9157 : Dashboards: Changing the timezone on dashboards does not persist navigation PMM-7116 : Dashboards: Incorrect STARTUP state on MongoDB ReplSet Summary dashboard PMM-8993 : Integrated Alerting: Sending email using Gmail fails PMM-7802 : PMM can\u2019t monitor MongoDB arbiter nodes (Thanks to Artem Meshcheryakov for reporting this issue) PMM-6937 : Can\u2019t add PostgreSQL instances to PMM without postgres DB in PostgreSQL server (Thanks to Daniel Kowalewski for reporting this issue) PMM-7447 : Can\u2019t add into PMM instances of PostgreSQL with SCRAM-SHA-256 authentication PMM-9085 : PMM Server crashes after upgrading to 2.22 every 4 hours PMM-9156 : pmm-agent paths-base option not working for pmm2-client binary installation in PMM 2.23.0 PMM-8461 : DBaaS: Confusing error when accessing DBaaS pages when it\u2019s disabled PMM-8110 : DBaaS: Registering K8s cluster with operators already installed can cause error PMM-8694 : Query Analytics: URLs in Query Analytics with a selected query and a timestamp range does not select the query PMM-9227 : Pagination Reset on QAN after Time Range change doesn\u2019t work, results in wrong results PMM-9298 : PMM AMI image in 2.24.0 has only 8GB space for data and Volume Size Check fails while upgrading to 2.25.0 Important note for users of PMM 2.24.0 2.24.0 AMI image has only 8GB available for the data, it is a bug (see PMM-9298 ). To resize a disk to full size you need to login to AMI instance with SSH and use the following command: curl https://raw.githubusercontent.com/percona/pmm-update/main/ansible/playbook/tasks/create-lvm.yml -o lvn-fix.yml && sudo ansible-playbook lvn-fix.yml For instructions about how to access your instances by using an SSH client, see Connecting to Your Linux Instance Using SSH Make sure to replace the user name ec2-user used in this document with admin. What this command does: Downloads Ansible playbook and runs it Copy your data from /srv to the temporary directory Create lvm partition Copy data from system disk to a new LVM partition Known issues \u00b6 PMM-9255 : After connecting PMM to Percona Platform, PMM occasionally shows a false permission issue notification, incorrectly suggesting that the connection could not be established due to missing permissions. Reload the page to remove the incorrect notification and confirm the connection. PMM-9312 : It\u2019s not possible to enable collStats, indexStats and -max-collections-limit for MongoDB","title":"PMM 2.25.0"},{"location":"release-notes/2.25.0.html#percona-monitoring-and-management-2250","text":"Date: December 14, 2021 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. Important note for users considering docker way upgrade to PMM 2.25.0 If you upgrade from a PMM version less than or equal to 2.23.0 using docker, it will fail if your PMM does not have external access (access to \u2018repo. percona.com\u2019). Thus, it is recommended to upgrade to PMM 2.26.0 instead. See PMM-9416 for more details.","title":"Percona Monitoring and Management 2.25.0"},{"location":"release-notes/2.25.0.html#release-highlights","text":"Percona Platform (Technology Preview): Connect Percona Monitoring and Management (PMM) to Percona Platform to boost the monitoring capabilities of your PMM installations and access all your Percona accounts and services from one single, centralized location. For more information, see the Percona Platform Portal documentation Enhanced PostgreSQL monitoring You can now specify custom database names when adding PostgreSQL Servers for monitoring. Previous PMM versions always used the default postgres name instead. Added support for the new version of pg_stat_monitor extension. Release Candidate v1.0.0-rc.1 brings many new PostgreSQL metrics, Dashboards and Query Analytics! To find out about all the features available in the new pg_stat_monitor version, see the pg_stat_monitor User guide Added compatibility for the latest Percona Distributions for PostgreSQL 14 , 13 , 12 , 11 updated on December 7, 2021, which includes the newest version of the pg_stat_monitor extension. Grafana usability enhancements PMM is now using the native Grafana provisioning mechanism for adding dashboards, plug-ins, and data sources. This ensures faster and more reliable upgrading to newer PMM versions. Your existing plug-ins and dashboard changes are preserved during upgrades, but always make sure to back them up before upgrading and check that everything transferred correctly afterward. Added option to change the time zone on dashboards. This selection is preserved while you navigate over Dashboards. If you need to change this setting permanently for your account, change it in your preferences by the URL: https://YOUR_PMM_SERVER/graph/profile DBaaS (Technical Preview) You can now deploy and update your DBaaS created PXC clusters to the latest version of Percona Distribution for MySQL Operator 1.9.0. This enables you to take advantage of the latest features and fixes. PMM environment enhancements The pmm-client docker container can now be started as a sidecar. For users that use PMM client in Kubernetes or build automation around it, you can now start the client as a sidecar container simply by passing a flag. The client will also gracefully handle any instances where the connection to DB is not available. For more details, see the PMM Client documentation Removed support for Ubuntu 16.04. With the support of new products and new versions of already supported products, we also removed old, unsupported software. As of this release, we are no longer supporting Ubuntu 16.04 in PMM according to recent anouncements","title":"Release Highlights"},{"location":"release-notes/2.25.0.html#new-features","text":"PMM-9050 : Connect PMM Server to Percona Platform for additional account info in PMM and value added content","title":"New Features"},{"location":"release-notes/2.25.0.html#improvements","text":"PMM-8545 : DBaaS: Support of Percona Distribution for MySQL Operator 1.9.0 in PMM PMM-7677 : Docker container for pmm-client : Option to change behavior and follow sidecar pattern The flags PMM_AGENT_SIDECAR and PMM_AGENT_SIDECAR_SLEEP does this. Read more in documentation PMM-3516 : Optimize provisioning mechanism of plugins, dashboards, and datasources in PMM PMM-8674 : Integrated Alerting: Add Tooltips to Add Alert Rule fields to make it easier to understand what information is needed PMM-8505 : Integrated Alerting: Clarify description of the \u2018Low memory\u2019 Alert Template PMM-8503 : Integrated Alerting: Field validation in Email and Slack tabs when updating settings PMM-7527 : Integrated Alerting: Improvements to overall user experience for action buttons in Alerting PMM-7079 : Integrated Alerting: New \u2018information\u2019 icon to give additional details about Alerts without cluttering screen PMM-8259 : Better clarification of error messages in pmm-admin when PMM server can\u2019t be unregistered PMM-8972 : Add ability to specify custom base path to exporters and tools using pmm-admin command PMM-8282 : Improved messaging for TLS option when adding Remote instances in PMM over UI","title":"Improvements"},{"location":"release-notes/2.25.0.html#bugs-fixed","text":"PMM-9169 : Security Advisor Checks are not working for MongoDB instances PMM-8982 : Backup Management: User is not able to see MongoDB backup logs if backup was taken on older version of pmm-server PMM-9157 : Dashboards: Changing the timezone on dashboards does not persist navigation PMM-7116 : Dashboards: Incorrect STARTUP state on MongoDB ReplSet Summary dashboard PMM-8993 : Integrated Alerting: Sending email using Gmail fails PMM-7802 : PMM can\u2019t monitor MongoDB arbiter nodes (Thanks to Artem Meshcheryakov for reporting this issue) PMM-6937 : Can\u2019t add PostgreSQL instances to PMM without postgres DB in PostgreSQL server (Thanks to Daniel Kowalewski for reporting this issue) PMM-7447 : Can\u2019t add into PMM instances of PostgreSQL with SCRAM-SHA-256 authentication PMM-9085 : PMM Server crashes after upgrading to 2.22 every 4 hours PMM-9156 : pmm-agent paths-base option not working for pmm2-client binary installation in PMM 2.23.0 PMM-8461 : DBaaS: Confusing error when accessing DBaaS pages when it\u2019s disabled PMM-8110 : DBaaS: Registering K8s cluster with operators already installed can cause error PMM-8694 : Query Analytics: URLs in Query Analytics with a selected query and a timestamp range does not select the query PMM-9227 : Pagination Reset on QAN after Time Range change doesn\u2019t work, results in wrong results PMM-9298 : PMM AMI image in 2.24.0 has only 8GB space for data and Volume Size Check fails while upgrading to 2.25.0 Important note for users of PMM 2.24.0 2.24.0 AMI image has only 8GB available for the data, it is a bug (see PMM-9298 ). To resize a disk to full size you need to login to AMI instance with SSH and use the following command: curl https://raw.githubusercontent.com/percona/pmm-update/main/ansible/playbook/tasks/create-lvm.yml -o lvn-fix.yml && sudo ansible-playbook lvn-fix.yml For instructions about how to access your instances by using an SSH client, see Connecting to Your Linux Instance Using SSH Make sure to replace the user name ec2-user used in this document with admin. What this command does: Downloads Ansible playbook and runs it Copy your data from /srv to the temporary directory Create lvm partition Copy data from system disk to a new LVM partition","title":"Bugs Fixed"},{"location":"release-notes/2.25.0.html#known-issues","text":"PMM-9255 : After connecting PMM to Percona Platform, PMM occasionally shows a false permission issue notification, incorrectly suggesting that the connection could not be established due to missing permissions. Reload the page to remove the incorrect notification and confirm the connection. PMM-9312 : It\u2019s not possible to enable collStats, indexStats and -max-collections-limit for MongoDB","title":"Known issues"},{"location":"release-notes/2.26.0.html","text":"Percona Monitoring and Management 2.26.0 \u00b6 Date: February 8, 2022 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. Release Highlights \u00b6 Experimental Dashboards New experimental dashboards are introduced that will be shipped to the PMM users. These dashboards will be uploaded to the Experimental folder to enable the users to test them in their environment and provide feedback. The following Dashboards are being shipped as part of this release: K8s monitoring dashboard Environment Overview dashboard Environment Summary dashboard Important These experimental dashboards are subject to change. It is recommended to use these dashboards for testing purposes only. SMTP Configuration Verification When configuring an SMTP email server for sending out email notifications, you now have the option to test that your specified settings are correct. The Email tab under Configuration > Settings > Communication now includes a Test button to send a test alert through the specified server. For more information about setting up an SMTP email server, see Configure . Breaking change for Integrated Alerting (Technical Preview) This release introduces major changes to the core structure of rule templates. As a result, alert rules and templates created in previous PMM versions are not compatible with PMM 2.26.0 and cannot be migrated to this new version. After upgrading, make sure to manually recreate any custom alert rules and rule templates that you want to transfer to PMM 2.26.0. Disclaimer Integrated Alerting is still a Preview functionality and, as such, subject to change. We recommend that early adopters use this feature for testing purposes only. New Features \u00b6 PMM-9059 : Following the recently introduced support for connecting PMM to Percona Platform, you now have the option to also unlink any servers that are no longer relevant to your Platform organization. To disconnect a PMM server, go to Configuration > Settings > Percona Platform and click Disconnect . You can check the list of servers connected to an organization in Percona Platform by clicking View instances on the Dashboard page. For more information, see Configure for more details. PMM-9312 : Tech Preview Feature: PMM now captures the MongoDB metrics such as dbStats, collStats, indexStats, and topmetrics. See Documentation for more details. Improvements \u00b6 PMM-9176 : DBaaS - PMM now supports Percona Distribution for MongoDB Operator 1.10.0. PMM-9159 : DBaaS - PMM now supports Percona Kubernetes Operator for Percona XtraDB Cluster 1.10.0. PMM-9180 : Integrated Alerting > Add Alert Rule - Added the Template expression in a collapsible panel for an enhanced user experience (default view as collapsed) as the technical message could confuse the users. PMM-7781 : Integrated Alerting - Alert rules no longer depend on their source rule template after creation. This means that you can now update or delete rule templates without impacting existing rules that are based on that template. For more information, see Integrated Alerting . PMM-9356 : Added new experimental Environment dashboards in PMM. PMM-9296 : Disclaimer about Technical Preview feature added to Percona Platform - Connect PMM to Percona portal page. Bugs Fixed \u00b6 PMM-9416 : Upgrading to PMM 2.25.0 using docker (replacing the image) fails when upgrading from versions less than or equal to 2.23.0. Caution It is recommended to upgrade directly to PMM 2.26.0 instead of 2.25.0 when updating from versions less than or equal to PMM 2.23.0 if your PMM doesn\u2019t have external access (access to repo.percona.com ). PMM-8867 : Fixed an issue for PMM Client installation using the tarball script (without using RPMs) where the configuration was getting lost due to the configuration file pmm-agent.yml being recreated. PMM-8094 : DBaaS - Fixed an issue for paused clusters that froze with PSMDB v1.8 operators when all the pods were terminated, providing no cluster resumption option. PMM-8535 : DBaaS - Repeating error after force unregister PMM-9144 : Dashboard - Fixed the Add inventory page issue that indicated AWS RDS/Aurora supported only MySQL. PMM-9289 : Get from Browser on settings page does not fetch port in Public address field breaking the integration for Platform authentication. PMM-9255 : On connecting the PMM server to Percona Platform for an admin user insufficient access rights error message is thrown. PMM-9049 : Eliminated confusion around the current and available version date by adding a tooltip with an explanation for these dates. PMM-9181 : Integrated Alerting - Modified the label for the enable/disable button in order to avoid confusion. PMM-5405 : Fixed an issue where the pmm-admin summary command fails if a null value is passed for the --filename parameter. PMM-8141 : Fixed an issue where the metrics were not captured as the cleanup of the temporary folder on the client node deleted the requisite configuration file.","title":"PMM 2.26.0"},{"location":"release-notes/2.26.0.html#percona-monitoring-and-management-2260","text":"Date: February 8, 2022 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance.","title":"Percona Monitoring and Management 2.26.0"},{"location":"release-notes/2.26.0.html#release-highlights","text":"Experimental Dashboards New experimental dashboards are introduced that will be shipped to the PMM users. These dashboards will be uploaded to the Experimental folder to enable the users to test them in their environment and provide feedback. The following Dashboards are being shipped as part of this release: K8s monitoring dashboard Environment Overview dashboard Environment Summary dashboard Important These experimental dashboards are subject to change. It is recommended to use these dashboards for testing purposes only. SMTP Configuration Verification When configuring an SMTP email server for sending out email notifications, you now have the option to test that your specified settings are correct. The Email tab under Configuration > Settings > Communication now includes a Test button to send a test alert through the specified server. For more information about setting up an SMTP email server, see Configure . Breaking change for Integrated Alerting (Technical Preview) This release introduces major changes to the core structure of rule templates. As a result, alert rules and templates created in previous PMM versions are not compatible with PMM 2.26.0 and cannot be migrated to this new version. After upgrading, make sure to manually recreate any custom alert rules and rule templates that you want to transfer to PMM 2.26.0. Disclaimer Integrated Alerting is still a Preview functionality and, as such, subject to change. We recommend that early adopters use this feature for testing purposes only.","title":"Release Highlights"},{"location":"release-notes/2.26.0.html#new-features","text":"PMM-9059 : Following the recently introduced support for connecting PMM to Percona Platform, you now have the option to also unlink any servers that are no longer relevant to your Platform organization. To disconnect a PMM server, go to Configuration > Settings > Percona Platform and click Disconnect . You can check the list of servers connected to an organization in Percona Platform by clicking View instances on the Dashboard page. For more information, see Configure for more details. PMM-9312 : Tech Preview Feature: PMM now captures the MongoDB metrics such as dbStats, collStats, indexStats, and topmetrics. See Documentation for more details.","title":"New Features"},{"location":"release-notes/2.26.0.html#improvements","text":"PMM-9176 : DBaaS - PMM now supports Percona Distribution for MongoDB Operator 1.10.0. PMM-9159 : DBaaS - PMM now supports Percona Kubernetes Operator for Percona XtraDB Cluster 1.10.0. PMM-9180 : Integrated Alerting > Add Alert Rule - Added the Template expression in a collapsible panel for an enhanced user experience (default view as collapsed) as the technical message could confuse the users. PMM-7781 : Integrated Alerting - Alert rules no longer depend on their source rule template after creation. This means that you can now update or delete rule templates without impacting existing rules that are based on that template. For more information, see Integrated Alerting . PMM-9356 : Added new experimental Environment dashboards in PMM. PMM-9296 : Disclaimer about Technical Preview feature added to Percona Platform - Connect PMM to Percona portal page.","title":"Improvements"},{"location":"release-notes/2.26.0.html#bugs-fixed","text":"PMM-9416 : Upgrading to PMM 2.25.0 using docker (replacing the image) fails when upgrading from versions less than or equal to 2.23.0. Caution It is recommended to upgrade directly to PMM 2.26.0 instead of 2.25.0 when updating from versions less than or equal to PMM 2.23.0 if your PMM doesn\u2019t have external access (access to repo.percona.com ). PMM-8867 : Fixed an issue for PMM Client installation using the tarball script (without using RPMs) where the configuration was getting lost due to the configuration file pmm-agent.yml being recreated. PMM-8094 : DBaaS - Fixed an issue for paused clusters that froze with PSMDB v1.8 operators when all the pods were terminated, providing no cluster resumption option. PMM-8535 : DBaaS - Repeating error after force unregister PMM-9144 : Dashboard - Fixed the Add inventory page issue that indicated AWS RDS/Aurora supported only MySQL. PMM-9289 : Get from Browser on settings page does not fetch port in Public address field breaking the integration for Platform authentication. PMM-9255 : On connecting the PMM server to Percona Platform for an admin user insufficient access rights error message is thrown. PMM-9049 : Eliminated confusion around the current and available version date by adding a tooltip with an explanation for these dates. PMM-9181 : Integrated Alerting - Modified the label for the enable/disable button in order to avoid confusion. PMM-5405 : Fixed an issue where the pmm-admin summary command fails if a null value is passed for the --filename parameter. PMM-8141 : Fixed an issue where the metrics were not captured as the cleanup of the temporary folder on the client node deleted the requisite configuration file.","title":"Bugs Fixed"},{"location":"release-notes/2.27.0.html","text":"Percona Monitoring and Management 2.27.0 \u00b6 Date: April 14, 2022 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is an open source database monitoring, management, and observability solution for MySQL, PostgreSQL, and MongoDB. Important We recommend using the latest version of the PMM instance. This ensures that you access the latest features of the underlying components, such as VictoriaMetrics, with all the bug fixes in place. One of the critical features of VictoriaMetrics is stream parsing mode , which enhances the performance of PMM and saves memory when scraping targets expose millions of metrics. Release Highlights \u00b6 PMM and Percona Portal Integration \u00b6 Enhanced UX for connecting PMM server to Percona Portal To leverage Percona Platform\u2019s recent support for federated identity, PMM now uses access-token authorization for connecting PMM instances to Percona Platform. This replaces the former username/password authentication used in PMM 2.26.0 and older versions. For more information, see Integrate PMM with Percona Platform . Access to account information for Percona customers When you connect your PMM instances to Percona Platform as a Percona customer, you can now check all your Percona Platform account information on the new Entitlements and Support Tickets tab on the main menu. For more information, see Check your account information in the online Help. Advisors \u00b6 Broader database health assessments with Advisors With this release, we are renaming Security Checks to Advisors and grouping them according to the functionality and recommendations they provide. To reflect these changes, the old Security Threat Tool option is now called Advisors . In addition, we have added new checks for MySQL and MongoDB. To check the list of checks and the tier for which they are available, see Advisor checks for PMM in the Percona Platform online Help. New Paid tier and special Advisor checks for customers In addition to Registered Checks, Percona customers who connect PMM to Pecona Platform now also have access to Paid Advisor checks, which offer more advanced database health information. For more information, see Working with Advisor checks . Advisor checks have been extended with two new query types: GetDiagnosticData and replSetGetStatus . Improved the documentation around developing checks . DBaaS \u00b6 Simplified experience in registering EKS cluster with kubeconfig generated by eksctl. Components upgrade \u00b6 Grafana : PMM 2.27.0 has now migrated to Grafana 8.3.5. This version of Grafana is loaded with a gamut of exciting features. For more information, see What\u2019s new in Grafana v8.0 . VictoriaMetrics : VictoriaMetrics has been upgraded to 1.72.0. New Features \u00b6 PMM-9718 : PMM and Percona Portal Integration: Federated connections to Percona Platform. PMM-9305 , PMM-8661 : PMM and Percona Portal Integration: Visibility over Percona Platform Entitlements and Support Tickets. PMM-9473 : Advisors: Additional abilities for MongoDB Advisor Checks. PMM-8800 : DBaaS: With PMM, you can now have a simplified experience in registering your EKS cluster with kubeconfig generated by eksctl. Copy-paste the configuration by selecting Using Amazon Elastic Kubernetes Service (EKS) checkbox, and your K8s cluster is registered. For more information, see Documentation . PMM-8434 : Support for passing PMM Server Public Address as an environment variable while starting the PMM server. For more information, see Documentation . Improvements \u00b6 PMM-9319 : PMM and Percona Portal Integration: Synchronized Platform and PMM roles: We have updated PMM permissions to ensure that Administrators of Percona Portal organizations are also granted Admin role in PMM. - PMM-9339 : Integrated Alerting: The Use TLS option in webhook settings has been renamed to Show TLS setting to better reflect its functionality. PMM-9182 : Integrated Alerting: Added Silence All option for when you want to stop notifications from all alerting rules at once. PMM-9164 : Integrated Alerting: You can now use an existing rule as a source for new ones instead of using a template. PMM-9635 : Advisors: Extended security checks to Advisors to cover broader database health checks. PMM-9148 : QAN: You can now share a link for Query Analytics at the click of a button with the Copy Link . PMM-8045 : DBaaS: With this version of PMM, we have added a warning about the deletion of API keys so that the user is forewarned before deleting the API key. PMM-9452 : With this release of PMM, we have implemented a simplified password change method for the default admin user using the command line parameter change-admin-password . PMM-9542 : PMM now predefines certain flags that allow users to set all other VictoriaMetrics parameters as environment variables. For more information see Documentation . PMM-8794 : For consistency, we have implemented a unified 24 hours time format for backup management. PMM-9306 : VictoriaMetrics has been upgraded to 1.72.0. PMM-8412 : Grafana has been upgraded to 8.x. PMM-9648 : With PMM 2.27.0 migrating to Grafana 8.0, the Singlestat Panel has been deprecated and replaced with Stat Panel in Grafana for an enhanced user experience. Bugs Fixed \u00b6 PMM-9797 : Fixed an issue where the data on the Home Dashboard was represented incorrectly on the Stat Panel, which could potentially confuse the user. PMM-9757 : Fixed an issue where the metrics for MongoDB were not being exposed when the database connection was getting lost. PMM-9603 : PMM and Percona Portal Integration: Fixed \u201cInsufficient access rights\u201d error that was displayed for admin user after connecting PMM server connect to Percona Portal. PMM-9323 : PMM and Percona Portal Integration: Ensured PMM no longer downloads checks and templates from Percona Portal when the Telemetry option is disabled in the PMM Advanced Settings. PMM-8986 : Advisors: Fixed an issue where the Advisors check on the PMM servers monitoring a large number of database services was causing a timeout. PMM-9570 : DBaaS: Create DB Cluster dialog box was closing automatically while trying to create a database cluster and had to be opened twice. This issue has been fixed now. PMM-9496 : DBaaS: Fixed an issue where the host and database were not being monitored in DBaas. PMM-9783 : QAN: Fixed an issue where QAN failed to work after an upgrade. PMM-9661 : QAN: Fixed an issue where QAN layout breaks while resizing the window. PMM-9797 : Dashboard: Fixed an issue where the data on the Home Dashboard was represented incorrectly on the Stat Panel, which could potentially confuse the user. PMM-9757 : Fixed an issue where the metrics for MongoDB were not being exposed when the database connection was getting lost. PMM-9671 : Fixed an issue where an upgrade to 2.26.0 failed as PMM crashed. PMM-9413 : Fixed an issue where the PMM management daemon was getting deadlocked when a PMM agent was getting connected with a duplicate agent_id . PMM-9015 : Fixed an issue where PMM does not display when PostgreSQL is down, thus failing to capture the metrics for PostgreSQL. PMM-8203 : Fixed an issue where the pmm-agent.log is cluttered with unnecessary errors when MariaDB database versions 10.2, 10.3, 10.4, and 10.5 are added for monitoring. PMM-5831 : Fixed an issue where pmm-admin uses the default value of \u2018listen port\u2019 rather than picking up the value from the agent configuration file. Fixed the following CVE\u2019s: PMM-9726 : Fixed a critical CVE that was affecting some versions of Go. PMM-9722 : Fixed a CVE for ClickHouse DBMS. PMM-9327 : Fixed a vulnerability in the Network Security Services (NSS) package. PMM-9502 : Fixed multiple JavaScript Common Vulnerabilities and Exposures (CVE) for PMM AMI setup. Known Issues \u00b6 PMM-9992 : Error while using reverse proxy (like Nginx) While using a reverse proxy (for example, Nginx) in front of PMM, you can run into the error origin not allowed after upgrading to PMM 2.27.0 or newer versions. Solution Add the Host header to the reverse proxy config file. Example For Nginx, add the following: proxy_set_header Host $http_host;","title":"PMM 2.27.0"},{"location":"release-notes/2.27.0.html#percona-monitoring-and-management-2270","text":"Date: April 14, 2022 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is an open source database monitoring, management, and observability solution for MySQL, PostgreSQL, and MongoDB. Important We recommend using the latest version of the PMM instance. This ensures that you access the latest features of the underlying components, such as VictoriaMetrics, with all the bug fixes in place. One of the critical features of VictoriaMetrics is stream parsing mode , which enhances the performance of PMM and saves memory when scraping targets expose millions of metrics.","title":"Percona Monitoring and Management 2.27.0"},{"location":"release-notes/2.27.0.html#release-highlights","text":"","title":"Release Highlights"},{"location":"release-notes/2.27.0.html#pmm-and-percona-portal-integration","text":"Enhanced UX for connecting PMM server to Percona Portal To leverage Percona Platform\u2019s recent support for federated identity, PMM now uses access-token authorization for connecting PMM instances to Percona Platform. This replaces the former username/password authentication used in PMM 2.26.0 and older versions. For more information, see Integrate PMM with Percona Platform . Access to account information for Percona customers When you connect your PMM instances to Percona Platform as a Percona customer, you can now check all your Percona Platform account information on the new Entitlements and Support Tickets tab on the main menu. For more information, see Check your account information in the online Help.","title":"PMM and Percona Portal Integration"},{"location":"release-notes/2.27.0.html#advisors","text":"Broader database health assessments with Advisors With this release, we are renaming Security Checks to Advisors and grouping them according to the functionality and recommendations they provide. To reflect these changes, the old Security Threat Tool option is now called Advisors . In addition, we have added new checks for MySQL and MongoDB. To check the list of checks and the tier for which they are available, see Advisor checks for PMM in the Percona Platform online Help. New Paid tier and special Advisor checks for customers In addition to Registered Checks, Percona customers who connect PMM to Pecona Platform now also have access to Paid Advisor checks, which offer more advanced database health information. For more information, see Working with Advisor checks . Advisor checks have been extended with two new query types: GetDiagnosticData and replSetGetStatus . Improved the documentation around developing checks .","title":"Advisors"},{"location":"release-notes/2.27.0.html#dbaas","text":"Simplified experience in registering EKS cluster with kubeconfig generated by eksctl.","title":"DBaaS"},{"location":"release-notes/2.27.0.html#components-upgrade","text":"Grafana : PMM 2.27.0 has now migrated to Grafana 8.3.5. This version of Grafana is loaded with a gamut of exciting features. For more information, see What\u2019s new in Grafana v8.0 . VictoriaMetrics : VictoriaMetrics has been upgraded to 1.72.0.","title":"Components upgrade"},{"location":"release-notes/2.27.0.html#new-features","text":"PMM-9718 : PMM and Percona Portal Integration: Federated connections to Percona Platform. PMM-9305 , PMM-8661 : PMM and Percona Portal Integration: Visibility over Percona Platform Entitlements and Support Tickets. PMM-9473 : Advisors: Additional abilities for MongoDB Advisor Checks. PMM-8800 : DBaaS: With PMM, you can now have a simplified experience in registering your EKS cluster with kubeconfig generated by eksctl. Copy-paste the configuration by selecting Using Amazon Elastic Kubernetes Service (EKS) checkbox, and your K8s cluster is registered. For more information, see Documentation . PMM-8434 : Support for passing PMM Server Public Address as an environment variable while starting the PMM server. For more information, see Documentation .","title":"New Features"},{"location":"release-notes/2.27.0.html#improvements","text":"PMM-9319 : PMM and Percona Portal Integration: Synchronized Platform and PMM roles: We have updated PMM permissions to ensure that Administrators of Percona Portal organizations are also granted Admin role in PMM. - PMM-9339 : Integrated Alerting: The Use TLS option in webhook settings has been renamed to Show TLS setting to better reflect its functionality. PMM-9182 : Integrated Alerting: Added Silence All option for when you want to stop notifications from all alerting rules at once. PMM-9164 : Integrated Alerting: You can now use an existing rule as a source for new ones instead of using a template. PMM-9635 : Advisors: Extended security checks to Advisors to cover broader database health checks. PMM-9148 : QAN: You can now share a link for Query Analytics at the click of a button with the Copy Link . PMM-8045 : DBaaS: With this version of PMM, we have added a warning about the deletion of API keys so that the user is forewarned before deleting the API key. PMM-9452 : With this release of PMM, we have implemented a simplified password change method for the default admin user using the command line parameter change-admin-password . PMM-9542 : PMM now predefines certain flags that allow users to set all other VictoriaMetrics parameters as environment variables. For more information see Documentation . PMM-8794 : For consistency, we have implemented a unified 24 hours time format for backup management. PMM-9306 : VictoriaMetrics has been upgraded to 1.72.0. PMM-8412 : Grafana has been upgraded to 8.x. PMM-9648 : With PMM 2.27.0 migrating to Grafana 8.0, the Singlestat Panel has been deprecated and replaced with Stat Panel in Grafana for an enhanced user experience.","title":"Improvements"},{"location":"release-notes/2.27.0.html#bugs-fixed","text":"PMM-9797 : Fixed an issue where the data on the Home Dashboard was represented incorrectly on the Stat Panel, which could potentially confuse the user. PMM-9757 : Fixed an issue where the metrics for MongoDB were not being exposed when the database connection was getting lost. PMM-9603 : PMM and Percona Portal Integration: Fixed \u201cInsufficient access rights\u201d error that was displayed for admin user after connecting PMM server connect to Percona Portal. PMM-9323 : PMM and Percona Portal Integration: Ensured PMM no longer downloads checks and templates from Percona Portal when the Telemetry option is disabled in the PMM Advanced Settings. PMM-8986 : Advisors: Fixed an issue where the Advisors check on the PMM servers monitoring a large number of database services was causing a timeout. PMM-9570 : DBaaS: Create DB Cluster dialog box was closing automatically while trying to create a database cluster and had to be opened twice. This issue has been fixed now. PMM-9496 : DBaaS: Fixed an issue where the host and database were not being monitored in DBaas. PMM-9783 : QAN: Fixed an issue where QAN failed to work after an upgrade. PMM-9661 : QAN: Fixed an issue where QAN layout breaks while resizing the window. PMM-9797 : Dashboard: Fixed an issue where the data on the Home Dashboard was represented incorrectly on the Stat Panel, which could potentially confuse the user. PMM-9757 : Fixed an issue where the metrics for MongoDB were not being exposed when the database connection was getting lost. PMM-9671 : Fixed an issue where an upgrade to 2.26.0 failed as PMM crashed. PMM-9413 : Fixed an issue where the PMM management daemon was getting deadlocked when a PMM agent was getting connected with a duplicate agent_id . PMM-9015 : Fixed an issue where PMM does not display when PostgreSQL is down, thus failing to capture the metrics for PostgreSQL. PMM-8203 : Fixed an issue where the pmm-agent.log is cluttered with unnecessary errors when MariaDB database versions 10.2, 10.3, 10.4, and 10.5 are added for monitoring. PMM-5831 : Fixed an issue where pmm-admin uses the default value of \u2018listen port\u2019 rather than picking up the value from the agent configuration file. Fixed the following CVE\u2019s: PMM-9726 : Fixed a critical CVE that was affecting some versions of Go. PMM-9722 : Fixed a CVE for ClickHouse DBMS. PMM-9327 : Fixed a vulnerability in the Network Security Services (NSS) package. PMM-9502 : Fixed multiple JavaScript Common Vulnerabilities and Exposures (CVE) for PMM AMI setup.","title":"Bugs Fixed"},{"location":"release-notes/2.27.0.html#known-issues","text":"PMM-9992 : Error while using reverse proxy (like Nginx) While using a reverse proxy (for example, Nginx) in front of PMM, you can run into the error origin not allowed after upgrading to PMM 2.27.0 or newer versions. Solution Add the Host header to the reverse proxy config file. Example For Nginx, add the following: proxy_set_header Host $http_host;","title":"Known Issues"},{"location":"release-notes/2.28.0.html","text":"Percona Monitoring and Management 2.28.0 \u00b6 Release date: May 12, 2022 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is an open source database monitoring, management, and observability solution for MySQL, PostgreSQL and MongoDB. Important We recommend using the latest version of PMM. This ensures that you have access to the latest PMM features and that your environment runs on the latest version of the underlying components, such as VictoriaMetrics, with all the bug fixes in place. Release Highlights \u00b6 Advisors \u00b6 Advisor checks enabled by default Starting with the previous release and continuing with this one, we have added significant improvements to the Advisors Checks functionality in performance, assessment coverage, and user experience. As a mature and generally useful feature, this option is now enabled by default for easier access to automatic checks and better insight into database health and performance, delivered by Percona Platform. Note Upgrading to PMM will automatically enable this feature for existing PMM instances. You can disable it at any time from your PMM dashboard on the Advanced Settings page. Run individual advisor checks In addition to running all available advisors at once, you now have the option to run each advisor check individually. This gives you more granular control over the checks on your connected databases. Running checks individually also means that you get the results for relevant advisors faster and that you can focus on resolving failed checks one at a time. For more information, see Working with Advisor checks . Enhanced Advisor checks PMM 2.28 includes a new major version of Advisors that features some important enhancements. The most significant changes are: Support for multiple queries Support for Victoria Metrics as a data source In a nutshell, these changes will allow experts to create more intelligent advisor checks to continue delivering more value to your connected PMM instances. The file format in which Advisors checks are written has been updated to support the new functionality provided by the Advisors service part of Percona Platform. This is a breaking change, so we recommend upgrading your PMM instance to benefit from these enhancements. For more information, see Develop Advisors . Ubuntu 22.04 LTS support \u00b6 We are providing binaries for the recently released version of Ubuntu from this release. Components upgrade \u00b6 VictoriaMetrics : VictoriaMetrics has been upgraded to 1.76.1 . Node exporter : Node Exporter has now been updated to 1.3.1. Important If you customized the disabled collectors , the list could change. Check the available collectors in Documentation . New Features \u00b6 PMM-9749 : Advisors: Possibility to run individual advisor checks separately. PMM-9469 : Advisors: Ability to have multiple queries in a single check. PMM-9468 : Advisors: Ability to query VictoriaMetrics as a data source. Improvements \u00b6 PMM-9841 : Advisors: Advisor checks are now enabled by default. PMM-8326 : Advisors: Changed the icon for the Edit Check Rule option with a more suggestive one that better reflects this functionality. PMM-9907 : pmm2-client now supports Ubuntu 22.04 LTS. PMM-9780 : VictoriaMetrics has been upgraded to 1.76.1. PMM-5871 : Node Exporter has now been updated to 1.3.1. PMM-9958 : The PMM logs button, which is used to download PMM logs for troubleshooting, is added to the help panel for better accessibility and enhanced user experience. PMM-9672 : Minor UI improvements to the visual elements in the breadcrumb trails to visually align them to the look-and-feel of Grafana pages and improve overall UI consistency. Bugs Fixed \u00b6 PMM-9854 : Advisors: In some scenarios, PMM was not displaying the complete list of advisors available for instances connected to Percona Platform. This issue is now fixed. PMM-9848 : Advisors: Fixed text contrast issue on the Failed Advisor Checks page that was visible when navigating the list of results while using PMM with the Light theme. PMM-9426 : DBaaS: Fixed an issue related to K8s monitoring where the K8s monitoring failed with K8s version 1.22 and higher. PMM-9885 : Dashboard: Fixed the documentation links on the Advanced settings page on the PMM dashboard. PMM-9828 : Fixed an issue with the QAN dashboard navigator/explorer where if you open QAN from a dashboard and try to navigate to a different dashboard, the explorer keeps closing/refreshing, making it impossible to navigate. PMM-9363 : PMM users logged in via SSO would still have access to PMM after disconnecting. This issue is now fixed and PMM correctly terminates SSO sessions after disconnecting. PMM-9415 : Backup Management: Fixed an issue where initial data restore on AWS instances fails. However, consecutive data restore attempts were successful. Known Issues \u00b6 PMM-9992 : Error while using reverse proxy (like Nginx) While using a reverse proxy (for example, Nginx) in front of PMM, you can run into the error origin not allowed after upgrading to PMM 2.27.0 or newer versions. Solution Add the Host header to the reverse proxy config file. Example For Nginx, add the following: proxy_set_header Host $http_host;","title":"PMM 2.28.0"},{"location":"release-notes/2.28.0.html#percona-monitoring-and-management-2280","text":"Release date: May 12, 2022 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is an open source database monitoring, management, and observability solution for MySQL, PostgreSQL and MongoDB. Important We recommend using the latest version of PMM. This ensures that you have access to the latest PMM features and that your environment runs on the latest version of the underlying components, such as VictoriaMetrics, with all the bug fixes in place.","title":"Percona Monitoring and Management 2.28.0"},{"location":"release-notes/2.28.0.html#release-highlights","text":"","title":"Release Highlights"},{"location":"release-notes/2.28.0.html#advisors","text":"Advisor checks enabled by default Starting with the previous release and continuing with this one, we have added significant improvements to the Advisors Checks functionality in performance, assessment coverage, and user experience. As a mature and generally useful feature, this option is now enabled by default for easier access to automatic checks and better insight into database health and performance, delivered by Percona Platform. Note Upgrading to PMM will automatically enable this feature for existing PMM instances. You can disable it at any time from your PMM dashboard on the Advanced Settings page. Run individual advisor checks In addition to running all available advisors at once, you now have the option to run each advisor check individually. This gives you more granular control over the checks on your connected databases. Running checks individually also means that you get the results for relevant advisors faster and that you can focus on resolving failed checks one at a time. For more information, see Working with Advisor checks . Enhanced Advisor checks PMM 2.28 includes a new major version of Advisors that features some important enhancements. The most significant changes are: Support for multiple queries Support for Victoria Metrics as a data source In a nutshell, these changes will allow experts to create more intelligent advisor checks to continue delivering more value to your connected PMM instances. The file format in which Advisors checks are written has been updated to support the new functionality provided by the Advisors service part of Percona Platform. This is a breaking change, so we recommend upgrading your PMM instance to benefit from these enhancements. For more information, see Develop Advisors .","title":"Advisors"},{"location":"release-notes/2.28.0.html#ubuntu-2204-lts-support","text":"We are providing binaries for the recently released version of Ubuntu from this release.","title":"Ubuntu 22.04 LTS support"},{"location":"release-notes/2.28.0.html#components-upgrade","text":"VictoriaMetrics : VictoriaMetrics has been upgraded to 1.76.1 . Node exporter : Node Exporter has now been updated to 1.3.1. Important If you customized the disabled collectors , the list could change. Check the available collectors in Documentation .","title":"Components upgrade"},{"location":"release-notes/2.28.0.html#new-features","text":"PMM-9749 : Advisors: Possibility to run individual advisor checks separately. PMM-9469 : Advisors: Ability to have multiple queries in a single check. PMM-9468 : Advisors: Ability to query VictoriaMetrics as a data source.","title":"New Features"},{"location":"release-notes/2.28.0.html#improvements","text":"PMM-9841 : Advisors: Advisor checks are now enabled by default. PMM-8326 : Advisors: Changed the icon for the Edit Check Rule option with a more suggestive one that better reflects this functionality. PMM-9907 : pmm2-client now supports Ubuntu 22.04 LTS. PMM-9780 : VictoriaMetrics has been upgraded to 1.76.1. PMM-5871 : Node Exporter has now been updated to 1.3.1. PMM-9958 : The PMM logs button, which is used to download PMM logs for troubleshooting, is added to the help panel for better accessibility and enhanced user experience. PMM-9672 : Minor UI improvements to the visual elements in the breadcrumb trails to visually align them to the look-and-feel of Grafana pages and improve overall UI consistency.","title":"Improvements"},{"location":"release-notes/2.28.0.html#bugs-fixed","text":"PMM-9854 : Advisors: In some scenarios, PMM was not displaying the complete list of advisors available for instances connected to Percona Platform. This issue is now fixed. PMM-9848 : Advisors: Fixed text contrast issue on the Failed Advisor Checks page that was visible when navigating the list of results while using PMM with the Light theme. PMM-9426 : DBaaS: Fixed an issue related to K8s monitoring where the K8s monitoring failed with K8s version 1.22 and higher. PMM-9885 : Dashboard: Fixed the documentation links on the Advanced settings page on the PMM dashboard. PMM-9828 : Fixed an issue with the QAN dashboard navigator/explorer where if you open QAN from a dashboard and try to navigate to a different dashboard, the explorer keeps closing/refreshing, making it impossible to navigate. PMM-9363 : PMM users logged in via SSO would still have access to PMM after disconnecting. This issue is now fixed and PMM correctly terminates SSO sessions after disconnecting. PMM-9415 : Backup Management: Fixed an issue where initial data restore on AWS instances fails. However, consecutive data restore attempts were successful.","title":"Bugs Fixed"},{"location":"release-notes/2.28.0.html#known-issues","text":"PMM-9992 : Error while using reverse proxy (like Nginx) While using a reverse proxy (for example, Nginx) in front of PMM, you can run into the error origin not allowed after upgrading to PMM 2.27.0 or newer versions. Solution Add the Host header to the reverse proxy config file. Example For Nginx, add the following: proxy_set_header Host $http_host;","title":"Known Issues"},{"location":"release-notes/2.29.0.html","text":"Percona Monitoring and Management 2.29.0 \u00b6 Release date: July 19, 2022 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is an open source database monitoring, management, and observability solution for MySQL, PostgreSQL, and MongoDB. Important We recommend using the latest version of PMM. This ensures that you have access to the latest PMM features and that your environment runs on the latest version of the underlying components, such as VictoriaMetrics, with all the bug fixes in place. Release Highlights \u00b6 Guided tour of PMM \u00b6 PMM now includes a short in-app tutorial that automatically pops up when you first launch the application. If you are a new user, this is your virtual unboxing of key PMM features to help you get started easier. If you are an Intermediate or Advanced user, use the tour as a checklist of features worth exploring to ensure you\u2019re making the most out of your PMM. Deploying PMM in various environments \u00b6 K8s \u00b6 Disclaimer Deploying PMM on Kubernetes is still technical preview and is subject to change. We recommend that early adopters use this feature for testing purposes only. Starting with PMM 2.29.0, we have introduced the helm chart supported by Percona to seamlessly deploy your PMM instances on Kubernetes (k8s), a prominent container orchestration system. With the Helm chart, you can customize your deployment. You can tweak your PMM installation by setting these various parameters . For more information, see the documentation . Before introducing the helm chart for PMM, if you wanted to deploy PMM, you would need a separate virtual machine to run PMM, but with the helm chart, that is not required. Helm Charts provide the ability to leverage Kubernetes packages at the click of a button or a single CLI command. Podman \u00b6 Disclaimer Podman support is still technical preview and is subject to change. We recommend that early adopters use this feature for testing purposes only. Starting with PMM 2.29.0, get your PMM instances up and running quickly with the Podman . Using Podman makes it easy to run, build, and deploy PMM. Deployment of PMM with the Podman is far more secure as it does not require root privileges to create, run and manage containers. It lets you run containers as a non-root user, so you never have to give a user root permission on the host. Thus it adds a new security layer, which is crucial in the event of a security breach. Podman also allows multiple unprivileged users to run containers on the same machine. For more information on deploying PMM on Podman, see the documentation . Local file system \u00b6 You can start your PMM instances without a data container using your local file system . You can use docker volume or host directory with the -v docker-cli option for storing PMM state (databases, configs, etc.) RHEL 9 support (client): \u00b6 PMM client now supports RHEL 9. Monitoring \u00b6 Meta metrics \u00b6 We have added some metrics for Mongodb exporters to monitor the time it takes to collect data for different collectors. With these metrics, you can identify the monitoring cost of various collectors by the MongoDB exporters. PXC Cluster dashboard \u00b6 Important This experimental dashboard is subject to change. It is recommended to use this dashboard for testing purposes only. Created a new experimental PXC/Galera Cluster Summary dashboard to make it simple, intuitive, and provide at-a-glance visibility for better decision making. Critical data for the PXC cluster is put together in this dashboard to obtain feedback from our users. For more information, refer to the documentation . Earlier, the PXC Cluster data was distributed over different dashboards. The users had to browse the MySQL compare dashboard to check for the data like slow queries and network overviews that were not in the PXC nodes compare dashboard. This made it time-consuming to identify any possible issues with the database. With the new PXC dashboard, we aim to solve this problem and gain insightful data about the cluster and services. Now the users can have an overview of the PXC clusters from this dashboard. Also, users with beginner to intermediate levels of expertise can effortlessly analyze the data on this dashboard. Troubleshooting \u00b6 Managing logs \u00b6 As a PMM user, you can explicitly set a detailed level of logging so that, by default, you will have meaningful logs to see, thereby enhancing your troubleshooting experience. If you want detailed logging, you can set --log-level to INFO. By default, INFO is not logged. Thus, you have more control over the logs you want to view with the --log-level parameter with the flexibility to choose options such as warning, error, fatal, and info. For example, to set the log level to fatal for a MySQL database: pmm-admin add mysql --username=XXX --password=XXX --log-level=fatal Additional parameters added in commands \u00b6 Added the agent (exporter) port to the output of the pmm-admin commands, which is crucial during debugging. pmm-admin list pmm-admin status pmm-admin inventory list agents Added new flag --agent-password to the following commands to support a custom password via the CLI to make it more secure: pmm-admin register --agent-password = AGENT-PASSWORD pmm-admin config --agent-password = AGENT-PASSWORD ` You can use the pprof parameter in the pmm-admin summary command. pmm-admin summary --pprof pprof reads a collection of profiling data from the PMM server, enabling you to generate reports in textual and graphical formats to visualize and analyze data. You can analyze CPU, Memory usage, heap size, and thread count for your PMM components with the data obtained. Percona Platform Integration \u00b6 Terminate Percona Platform connections as an Admin \u00b6 When regular users are not able to disconnect a server (for example, when PMM was moved to a network segment without outbound connections to public networks), PMM Admins can now terminate such connections instead. The Disconnect option under Settings > Percona Platform now enables PMM Admins to retire PMM instances from Percona Platform even if they are not logged in with a Percona Account. When disconnecting servers without a Percona Account, the lingering servers are not automatically removed from the list of PMM instances in Percona Platform. As a Percona Platform Admin, you can ensure that your list of PMM instances stays up-to-date by manually removing those unavailable servers via PMM instances . Easier access to your Percona Support Specialist \u00b6 If you are a Percona customer, you can now find the contact details of your dedicated Percona Support Specialist on the new Environment Overview tab on the main menu. The Percona Contacts section here shows the name and email of the Customer Success Manager assigned to your organization, who can help with any PMM queries. This new tab will soon be populated with more useful information about your PMM environment. Advisors \u00b6 Extended severities for advisor checks and alerts \u00b6 We have extended and unified the list of severity levels for the notifications coming from Integrated Alerting and Advisor Checks. When you create a new alert rule, you can now choose from a broader range of severity labels: Emergency , Alert , Critical , Error , Warning , Notice , Info , Debug . The same severities are now also available for notifications coming from failed checks under Advisor Checks > Failed Checks . We\u2019ve also made a small UX improvement to the way failed checks are grouped by severity in the table. Use filters to explore available advisors checks \u00b6 As we\u2019re constantly adding more advisors for checking the health of your connected databases, you may find it useful to now be able to drill-down through this constantly growing list. Use the filters on the Advisors Checks > All Checks page to search advisors by Name , Description , Status or Interval . If you need to share the filtered results with your team members, send them the PMM URL, which saves your search criteria and results. DBaaS \u00b6 Simplified the API such that it requires fewer parameters to run the API. With this new implementation, the only mandatory field is the Kubernetes cluster name. All other fields such as resource parameters, image field, params field, and name filed are optional. The API sets these to default values if these fields are not specified. Also, the documentation has been enhanced for these APIs. Components upgrade \u00b6 Upgraded VictoriaMetrics operator to version 0.24. Upgraded the ClickHouse plugin to 2.4.4, which fixes some of the CVEs. With this release, we are upgrading grpc-gateway to version 2.0, which deprecates the error field in error responses. If you are using the PMM API, ensure to replace this with the new message field from the Google Cloud API Error model. Upgraded the mysqld exporter to the upstream version 0.14.0. Important bug fixes \u00b6 PMM-9981 : Fixed excessive space usage for Group replication monitoring. Services with enabled group replication and custom queries now take up less disk space and perform faster. To achieve this, we removed the Transactions Details table from the Group Replication dashboard, as these were based on labels that were generating high-cardinality data. If you still have issues with performance in Group Replication dashboard, log into PMM as an admin, and use https://pmm_server_url/prometheus/api/v1/admin/tsdb/delete_series?match[]= to remove all time series for the following metrics: mysql_perf_schema_replication_group_worker_rep_delay_seconds mysql_perf_schema_replication_group_worker_transport_time_seconds mysql_perf_schema_replication_group_worker_time_RL_seconds mysql_perf_schema_replication_group_worker_apply_time_seconds Mysql_perf_schema_replication_group_worker_lag_in_secondds For example, to remove all time series for the mysql_perf_schema_replication_group_worker metric use the following URL: https://PMM_SERVER_URL/prometheus/api/v1/admin/tsdb/delete_series?match[]=mysql_perf_schema_replication_group_worker_rep_delay_seconds PMM-9510 : Fixed incorrect and duplicate information displayed on the Dashboard for MongoDB Replica Set. New Features \u00b6 PMM-10133 : New User Onboarding: Added a guided tour that highlights the main PMM components on your first onboarding journey. PMM-10059 : Advisors: Extended and unified list of severity levels for the notifications coming from Integrated Alerting and Advisor Checks. PMM-7110 : You can now obtain profiling data with the pprof tool that helps analyze CPU, Memory usage, heap size, and thread count for your PMM components. PMM-8660 : Percona customers can now find the contact details of their dedicated Percona Support Specialist on the new Environment Overview tab on the main menu. PMM-7925 : [Technical Preview]: Starting with PMM 2.29.0, you can now deploy your PMM instances using Podman, which is considered far more secure as it does not require root access. PMM-9613 : [Technical Preview]: You can now scale and deploy your PMM instances faster using the Kubernetes clusters. PMM-9919 : We have added some additional metrics to Mongodb exporters to monitor the time it takes to collect data for different collectors that would allow us to identify the monitoring cost of various collectors by MongoDB exporters. Improvements \u00b6 PMM-9766 : Advisors: Added filters on the Advisors Checks > All Checks page to narrow the list of available advisor checks. PMM-7491 : Alerting: We\u2019ve redefined the Filters option under Integrated Alerting > Alert Rules > New Rule to simplify the way you target specific services or nodes for your alert rules. Instead of typing an exact filter pattern, you can now intuitively create a filter by choosing from the available operators and filling in the predefined fields. So instead of typing: service_name=name123 , just fill in the following fields: PMM-9704 : With the implementation of a simplified API for DBaaS, you only have to specify the Kubernetes cluster name. All the other fields are optional, and the API will set the default values for these fields if not specified. PMM-8222 : DBaaS: Enhanced the documentation for the API. PMM-9611 : PMM now warns you if your current domain is different than the one specified in the Grafana .INI configuration file. Since this can generate incorrect short URLs when sharing the link to a PMM dashboard, make sure to correct this mismatch if you see a warning on the Share Panel > Link page . PMM-7326 : You now have more control over the logs you want to view with the \u2013log-level parameter. With this enhancement, you can experience more pronounced logging with the flexibility to choose options such as warning, error, and fatal. PMM-8566 : For enhanced security, you can now specify the custom Basic Auth password for agents when adding a Node. PMM-9362 : PMM Admins can now disconnect servers from Percona Platform even if they are not logged in with a Percona Account Percona Account. PMM-6592 : PMM now displays the ports used by the exporters in the output of the pmm- admin list and status commands, which are crucial while debugging. PMM-7110 : You can now use the pprof parameter in the pmm-admin summary command to obtain profiling data from the PMM server, thereby enabling you to generate reports in textual and graphical formats to visualize and analyze data. PMM-7186 : The pmm-admin summary command now retrieves a list of targets scraped by Victoriametrics on the client-side. PMM-8308 : To enhance the troubleshooting experience, in addition to the information summary and archive, you can now view the paths for all the exporters managed by the pmm-agent in the diagnostic data. PMM-9650 : Created a new experimental dashboard for PXC Cluster Summary to make it simple and intuitive. PMM-10039 : VictoriaMetrics operator has been upgraded to 0.24. PMM-10083 : Upgraded the ClickHouse plugin to 2.4.4, which fixes some of the CVEs. PMM-10103 : Added RHEL 9 support for PMM client. PMM-10155 : Access QAN from the main menu instead of the PMM dashboard for better reach and visibility. PMM-2038 : Starting with PMM 2.29.0, we have upgraded the mysqld exporter to the upstream version. PMM-9913 : PMM removes all the unnecessary temporary files created while adding a monitoring service, which might impact your PMM server\u2019s performance if not deleted. PMM-10067 : Updated the version of Percona Toolkit to 3.4.0. Bugs Fixed \u00b6 PMM-10127 : Alerting: Fixed issue where changing the page under Integrated Alerting > Alerts would not update the list of results. PMM-10110 : DBaas: Fixed an issue where an incorrect operator version was installed. PMM-9965 : DBaas: Fixed an issue where re-registering a K8s cluster returned an error. PMM-7143 : DBaaS: Fixed an issue where proper node metrics were not exposed for database instances created with DBaaS. PMM-8677 : DBaaS: Fixed the names of the operators used in PMM on the DBaaS UI to reflect their official names rather than short forms. PMM-9998 : Fixed an issue where \u2018Unknown column\u2026\u2019 errors are thrown when monitoring MariaDB from the Debian package (10.2, 10.3, 10.4, 10.5). PMM-10144 : Fixed an issue where if an already existing service was added via CLI, incorrect output was being displayed. PMM-9320 : Fixed an issue where it was impossible to add MongoDB to PMM when the password contained \u201c+.\u201d PMM-9909 : Fixed an issue where the redirection from Home Dashboard Stats Panel to Node Summary did not work. PMM-10049 : Removed unnecessary error messages from the pmm-agent log. PMM-8819 : Fixed an issue where the metric data displayed in the table was not appropriately aligned. PMM-10087 : PMM would show an \u201cInternal server error\u201d for custom alert rules created with a \u201cia=1\u201d label. This issue is now fixed. PMM-7462 : Fixed an issue where you can update the PMM settings via the UI if they are not provided as an Environment variable. PMM-9510 : Fixed incorrect and duplicate information displayed on the Dashboard for MongoDB Replica Set. PMM-9910 : Fixed an issue where upgrading PMM from 2.26.0 to 2.27.0 via the UI failed due to a GRPC error. PMM-9981 : Enormous space usage and slow queries for Group replication monitoring PMM-10069 : Fixed a typo in the tooltip for InnoDB Random Read Ahead. PMM-10161 : Removed unnecessary breadcrumb navigation panel from all dashboards.","title":"PMM 2.29.0"},{"location":"release-notes/2.29.0.html#percona-monitoring-and-management-2290","text":"Release date: July 19, 2022 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is an open source database monitoring, management, and observability solution for MySQL, PostgreSQL, and MongoDB. Important We recommend using the latest version of PMM. This ensures that you have access to the latest PMM features and that your environment runs on the latest version of the underlying components, such as VictoriaMetrics, with all the bug fixes in place.","title":"Percona Monitoring and Management 2.29.0"},{"location":"release-notes/2.29.0.html#release-highlights","text":"","title":"Release Highlights"},{"location":"release-notes/2.29.0.html#guided-tour-of-pmm","text":"PMM now includes a short in-app tutorial that automatically pops up when you first launch the application. If you are a new user, this is your virtual unboxing of key PMM features to help you get started easier. If you are an Intermediate or Advanced user, use the tour as a checklist of features worth exploring to ensure you\u2019re making the most out of your PMM.","title":"Guided tour of PMM"},{"location":"release-notes/2.29.0.html#deploying-pmm-in-various-environments","text":"","title":"Deploying PMM in various environments"},{"location":"release-notes/2.29.0.html#k8s","text":"Disclaimer Deploying PMM on Kubernetes is still technical preview and is subject to change. We recommend that early adopters use this feature for testing purposes only. Starting with PMM 2.29.0, we have introduced the helm chart supported by Percona to seamlessly deploy your PMM instances on Kubernetes (k8s), a prominent container orchestration system. With the Helm chart, you can customize your deployment. You can tweak your PMM installation by setting these various parameters . For more information, see the documentation . Before introducing the helm chart for PMM, if you wanted to deploy PMM, you would need a separate virtual machine to run PMM, but with the helm chart, that is not required. Helm Charts provide the ability to leverage Kubernetes packages at the click of a button or a single CLI command.","title":"K8s"},{"location":"release-notes/2.29.0.html#podman","text":"Disclaimer Podman support is still technical preview and is subject to change. We recommend that early adopters use this feature for testing purposes only. Starting with PMM 2.29.0, get your PMM instances up and running quickly with the Podman . Using Podman makes it easy to run, build, and deploy PMM. Deployment of PMM with the Podman is far more secure as it does not require root privileges to create, run and manage containers. It lets you run containers as a non-root user, so you never have to give a user root permission on the host. Thus it adds a new security layer, which is crucial in the event of a security breach. Podman also allows multiple unprivileged users to run containers on the same machine. For more information on deploying PMM on Podman, see the documentation .","title":"Podman"},{"location":"release-notes/2.29.0.html#local-file-system","text":"You can start your PMM instances without a data container using your local file system . You can use docker volume or host directory with the -v docker-cli option for storing PMM state (databases, configs, etc.)","title":"Local file system"},{"location":"release-notes/2.29.0.html#rhel-9-support-client","text":"PMM client now supports RHEL 9.","title":"RHEL 9 support (client):"},{"location":"release-notes/2.29.0.html#monitoring","text":"","title":"Monitoring"},{"location":"release-notes/2.29.0.html#meta-metrics","text":"We have added some metrics for Mongodb exporters to monitor the time it takes to collect data for different collectors. With these metrics, you can identify the monitoring cost of various collectors by the MongoDB exporters.","title":"Meta metrics"},{"location":"release-notes/2.29.0.html#pxc-cluster-dashboard","text":"Important This experimental dashboard is subject to change. It is recommended to use this dashboard for testing purposes only. Created a new experimental PXC/Galera Cluster Summary dashboard to make it simple, intuitive, and provide at-a-glance visibility for better decision making. Critical data for the PXC cluster is put together in this dashboard to obtain feedback from our users. For more information, refer to the documentation . Earlier, the PXC Cluster data was distributed over different dashboards. The users had to browse the MySQL compare dashboard to check for the data like slow queries and network overviews that were not in the PXC nodes compare dashboard. This made it time-consuming to identify any possible issues with the database. With the new PXC dashboard, we aim to solve this problem and gain insightful data about the cluster and services. Now the users can have an overview of the PXC clusters from this dashboard. Also, users with beginner to intermediate levels of expertise can effortlessly analyze the data on this dashboard.","title":"PXC Cluster dashboard"},{"location":"release-notes/2.29.0.html#troubleshooting","text":"","title":"Troubleshooting"},{"location":"release-notes/2.29.0.html#managing-logs","text":"As a PMM user, you can explicitly set a detailed level of logging so that, by default, you will have meaningful logs to see, thereby enhancing your troubleshooting experience. If you want detailed logging, you can set --log-level to INFO. By default, INFO is not logged. Thus, you have more control over the logs you want to view with the --log-level parameter with the flexibility to choose options such as warning, error, fatal, and info. For example, to set the log level to fatal for a MySQL database: pmm-admin add mysql --username=XXX --password=XXX --log-level=fatal","title":"Managing logs"},{"location":"release-notes/2.29.0.html#additional-parameters-added-in-commands","text":"Added the agent (exporter) port to the output of the pmm-admin commands, which is crucial during debugging. pmm-admin list pmm-admin status pmm-admin inventory list agents Added new flag --agent-password to the following commands to support a custom password via the CLI to make it more secure: pmm-admin register --agent-password = AGENT-PASSWORD pmm-admin config --agent-password = AGENT-PASSWORD ` You can use the pprof parameter in the pmm-admin summary command. pmm-admin summary --pprof pprof reads a collection of profiling data from the PMM server, enabling you to generate reports in textual and graphical formats to visualize and analyze data. You can analyze CPU, Memory usage, heap size, and thread count for your PMM components with the data obtained.","title":"Additional parameters added in commands"},{"location":"release-notes/2.29.0.html#percona-platform-integration","text":"","title":"Percona Platform Integration"},{"location":"release-notes/2.29.0.html#terminate-percona-platform-connections-as-an-admin","text":"When regular users are not able to disconnect a server (for example, when PMM was moved to a network segment without outbound connections to public networks), PMM Admins can now terminate such connections instead. The Disconnect option under Settings > Percona Platform now enables PMM Admins to retire PMM instances from Percona Platform even if they are not logged in with a Percona Account. When disconnecting servers without a Percona Account, the lingering servers are not automatically removed from the list of PMM instances in Percona Platform. As a Percona Platform Admin, you can ensure that your list of PMM instances stays up-to-date by manually removing those unavailable servers via PMM instances .","title":"Terminate Percona Platform connections as an Admin"},{"location":"release-notes/2.29.0.html#easier-access-to-your-percona-support-specialist","text":"If you are a Percona customer, you can now find the contact details of your dedicated Percona Support Specialist on the new Environment Overview tab on the main menu. The Percona Contacts section here shows the name and email of the Customer Success Manager assigned to your organization, who can help with any PMM queries. This new tab will soon be populated with more useful information about your PMM environment.","title":"Easier access to your Percona Support Specialist"},{"location":"release-notes/2.29.0.html#advisors","text":"","title":"Advisors"},{"location":"release-notes/2.29.0.html#extended-severities-for-advisor-checks-and-alerts","text":"We have extended and unified the list of severity levels for the notifications coming from Integrated Alerting and Advisor Checks. When you create a new alert rule, you can now choose from a broader range of severity labels: Emergency , Alert , Critical , Error , Warning , Notice , Info , Debug . The same severities are now also available for notifications coming from failed checks under Advisor Checks > Failed Checks . We\u2019ve also made a small UX improvement to the way failed checks are grouped by severity in the table.","title":"Extended severities for advisor checks and alerts"},{"location":"release-notes/2.29.0.html#use-filters-to-explore-available-advisors-checks","text":"As we\u2019re constantly adding more advisors for checking the health of your connected databases, you may find it useful to now be able to drill-down through this constantly growing list. Use the filters on the Advisors Checks > All Checks page to search advisors by Name , Description , Status or Interval . If you need to share the filtered results with your team members, send them the PMM URL, which saves your search criteria and results.","title":"Use filters to explore available advisors checks"},{"location":"release-notes/2.29.0.html#dbaas","text":"Simplified the API such that it requires fewer parameters to run the API. With this new implementation, the only mandatory field is the Kubernetes cluster name. All other fields such as resource parameters, image field, params field, and name filed are optional. The API sets these to default values if these fields are not specified. Also, the documentation has been enhanced for these APIs.","title":"DBaaS"},{"location":"release-notes/2.29.0.html#components-upgrade","text":"Upgraded VictoriaMetrics operator to version 0.24. Upgraded the ClickHouse plugin to 2.4.4, which fixes some of the CVEs. With this release, we are upgrading grpc-gateway to version 2.0, which deprecates the error field in error responses. If you are using the PMM API, ensure to replace this with the new message field from the Google Cloud API Error model. Upgraded the mysqld exporter to the upstream version 0.14.0.","title":"Components upgrade"},{"location":"release-notes/2.29.0.html#important-bug-fixes","text":"PMM-9981 : Fixed excessive space usage for Group replication monitoring. Services with enabled group replication and custom queries now take up less disk space and perform faster. To achieve this, we removed the Transactions Details table from the Group Replication dashboard, as these were based on labels that were generating high-cardinality data. If you still have issues with performance in Group Replication dashboard, log into PMM as an admin, and use https://pmm_server_url/prometheus/api/v1/admin/tsdb/delete_series?match[]= to remove all time series for the following metrics: mysql_perf_schema_replication_group_worker_rep_delay_seconds mysql_perf_schema_replication_group_worker_transport_time_seconds mysql_perf_schema_replication_group_worker_time_RL_seconds mysql_perf_schema_replication_group_worker_apply_time_seconds Mysql_perf_schema_replication_group_worker_lag_in_secondds For example, to remove all time series for the mysql_perf_schema_replication_group_worker metric use the following URL: https://PMM_SERVER_URL/prometheus/api/v1/admin/tsdb/delete_series?match[]=mysql_perf_schema_replication_group_worker_rep_delay_seconds PMM-9510 : Fixed incorrect and duplicate information displayed on the Dashboard for MongoDB Replica Set.","title":"Important bug fixes"},{"location":"release-notes/2.29.0.html#new-features","text":"PMM-10133 : New User Onboarding: Added a guided tour that highlights the main PMM components on your first onboarding journey. PMM-10059 : Advisors: Extended and unified list of severity levels for the notifications coming from Integrated Alerting and Advisor Checks. PMM-7110 : You can now obtain profiling data with the pprof tool that helps analyze CPU, Memory usage, heap size, and thread count for your PMM components. PMM-8660 : Percona customers can now find the contact details of their dedicated Percona Support Specialist on the new Environment Overview tab on the main menu. PMM-7925 : [Technical Preview]: Starting with PMM 2.29.0, you can now deploy your PMM instances using Podman, which is considered far more secure as it does not require root access. PMM-9613 : [Technical Preview]: You can now scale and deploy your PMM instances faster using the Kubernetes clusters. PMM-9919 : We have added some additional metrics to Mongodb exporters to monitor the time it takes to collect data for different collectors that would allow us to identify the monitoring cost of various collectors by MongoDB exporters.","title":"New Features"},{"location":"release-notes/2.29.0.html#improvements","text":"PMM-9766 : Advisors: Added filters on the Advisors Checks > All Checks page to narrow the list of available advisor checks. PMM-7491 : Alerting: We\u2019ve redefined the Filters option under Integrated Alerting > Alert Rules > New Rule to simplify the way you target specific services or nodes for your alert rules. Instead of typing an exact filter pattern, you can now intuitively create a filter by choosing from the available operators and filling in the predefined fields. So instead of typing: service_name=name123 , just fill in the following fields: PMM-9704 : With the implementation of a simplified API for DBaaS, you only have to specify the Kubernetes cluster name. All the other fields are optional, and the API will set the default values for these fields if not specified. PMM-8222 : DBaaS: Enhanced the documentation for the API. PMM-9611 : PMM now warns you if your current domain is different than the one specified in the Grafana .INI configuration file. Since this can generate incorrect short URLs when sharing the link to a PMM dashboard, make sure to correct this mismatch if you see a warning on the Share Panel > Link page . PMM-7326 : You now have more control over the logs you want to view with the \u2013log-level parameter. With this enhancement, you can experience more pronounced logging with the flexibility to choose options such as warning, error, and fatal. PMM-8566 : For enhanced security, you can now specify the custom Basic Auth password for agents when adding a Node. PMM-9362 : PMM Admins can now disconnect servers from Percona Platform even if they are not logged in with a Percona Account Percona Account. PMM-6592 : PMM now displays the ports used by the exporters in the output of the pmm- admin list and status commands, which are crucial while debugging. PMM-7110 : You can now use the pprof parameter in the pmm-admin summary command to obtain profiling data from the PMM server, thereby enabling you to generate reports in textual and graphical formats to visualize and analyze data. PMM-7186 : The pmm-admin summary command now retrieves a list of targets scraped by Victoriametrics on the client-side. PMM-8308 : To enhance the troubleshooting experience, in addition to the information summary and archive, you can now view the paths for all the exporters managed by the pmm-agent in the diagnostic data. PMM-9650 : Created a new experimental dashboard for PXC Cluster Summary to make it simple and intuitive. PMM-10039 : VictoriaMetrics operator has been upgraded to 0.24. PMM-10083 : Upgraded the ClickHouse plugin to 2.4.4, which fixes some of the CVEs. PMM-10103 : Added RHEL 9 support for PMM client. PMM-10155 : Access QAN from the main menu instead of the PMM dashboard for better reach and visibility. PMM-2038 : Starting with PMM 2.29.0, we have upgraded the mysqld exporter to the upstream version. PMM-9913 : PMM removes all the unnecessary temporary files created while adding a monitoring service, which might impact your PMM server\u2019s performance if not deleted. PMM-10067 : Updated the version of Percona Toolkit to 3.4.0.","title":"Improvements"},{"location":"release-notes/2.29.0.html#bugs-fixed","text":"PMM-10127 : Alerting: Fixed issue where changing the page under Integrated Alerting > Alerts would not update the list of results. PMM-10110 : DBaas: Fixed an issue where an incorrect operator version was installed. PMM-9965 : DBaas: Fixed an issue where re-registering a K8s cluster returned an error. PMM-7143 : DBaaS: Fixed an issue where proper node metrics were not exposed for database instances created with DBaaS. PMM-8677 : DBaaS: Fixed the names of the operators used in PMM on the DBaaS UI to reflect their official names rather than short forms. PMM-9998 : Fixed an issue where \u2018Unknown column\u2026\u2019 errors are thrown when monitoring MariaDB from the Debian package (10.2, 10.3, 10.4, 10.5). PMM-10144 : Fixed an issue where if an already existing service was added via CLI, incorrect output was being displayed. PMM-9320 : Fixed an issue where it was impossible to add MongoDB to PMM when the password contained \u201c+.\u201d PMM-9909 : Fixed an issue where the redirection from Home Dashboard Stats Panel to Node Summary did not work. PMM-10049 : Removed unnecessary error messages from the pmm-agent log. PMM-8819 : Fixed an issue where the metric data displayed in the table was not appropriately aligned. PMM-10087 : PMM would show an \u201cInternal server error\u201d for custom alert rules created with a \u201cia=1\u201d label. This issue is now fixed. PMM-7462 : Fixed an issue where you can update the PMM settings via the UI if they are not provided as an Environment variable. PMM-9510 : Fixed incorrect and duplicate information displayed on the Dashboard for MongoDB Replica Set. PMM-9910 : Fixed an issue where upgrading PMM from 2.26.0 to 2.27.0 via the UI failed due to a GRPC error. PMM-9981 : Enormous space usage and slow queries for Group replication monitoring PMM-10069 : Fixed a typo in the tooltip for InnoDB Random Read Ahead. PMM-10161 : Removed unnecessary breadcrumb navigation panel from all dashboards.","title":"Bugs Fixed"},{"location":"release-notes/2.29.1.html","text":"Percona Monitoring and Management 2.29.1 \u00b6 Release date: July 28, 2022 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is an open source database monitoring, management, and observability solution for MySQL, PostgreSQL, and MongoDB. Important We recommend using the latest version of PMM. This ensures that you have access to the latest PMM features and that your environment runs on the latest version of the underlying components, such as VictoriaMetrics, with all the bug fixes in place. Release Highlights \u00b6 Fixed the following CVE: PMM-10338 : We have updated Grafana to fix critical CVE\u2019s impacting Alerting in PMM.","title":"PMM 2.29.1"},{"location":"release-notes/2.29.1.html#percona-monitoring-and-management-2291","text":"Release date: July 28, 2022 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is an open source database monitoring, management, and observability solution for MySQL, PostgreSQL, and MongoDB. Important We recommend using the latest version of PMM. This ensures that you have access to the latest PMM features and that your environment runs on the latest version of the underlying components, such as VictoriaMetrics, with all the bug fixes in place.","title":"Percona Monitoring and Management 2.29.1"},{"location":"release-notes/2.29.1.html#release-highlights","text":"Fixed the following CVE: PMM-10338 : We have updated Grafana to fix critical CVE\u2019s impacting Alerting in PMM.","title":"Release Highlights"},{"location":"release-notes/2.3.0.html","text":"Percona Monitoring and Management 2.3.0 \u00b6 Date: February 19, 2020 Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. For PMM install instructions, see Installing PMM Server and Installing PMM Client . Caution PMM 2 is designed to be used as a new installation \u2014 please don\u2019t try to upgrade your existing PMM 1 environment. Improvements and new features \u00b6 PMM-5064 and PMM-5065 : Starting from this release, users will be able to integrate PMM with an external Alertmanager by specifying the Alertmanager URL and the Alert Rules to be executed inside the PMM server ( This feature is for advanced users only at this point ) PMM-4954 : Query Analytics dashboard now shows units both in the list of queries in a summary table and in the Details section to ease understanding of the presented data PMM-5179 : Relations between metrics are now specified in the Query Analytics Details section PMM-5115 : The CPU frequency and temperature graphs were added to the CPU Utilization dashboard PMM-5394 : A special treatment for the node-related dashboards was implemented for the situations when the data resolution change causes new metrics to be generated for existing nodes and services, to make graphs show continuous lines of the same colors Fixed bugs \u00b6 PMM-4620 : The high CPU usage by the pmm-agent process related to MongoDB Query Analytics was fixed PMM-5377 : singlestats showing percentage had sparklines scaled vertically along with the graph swing, which made it difficult to visually notice the difference between neighboring singlestats . PMM-5204 : Changing resolution on the PMM settings page was breaking some singlestats on the Home and MySQL Overview dashboards PMM-5251 : Vertical scroll bars on the graph elements were not allowed to do a full scroll, making last rows of the legend unavailable for some graphs PMM-5410 : The \u201cAvailable Downtime before SST Required\u201d chart on the PXC/Galera Node Summary dashboard was not showing data because it was unable to use metrics available with different scraping intervals","title":"PMM 2.3.0"},{"location":"release-notes/2.3.0.html#percona-monitoring-and-management-230","text":"Date: February 19, 2020 Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. For PMM install instructions, see Installing PMM Server and Installing PMM Client . Caution PMM 2 is designed to be used as a new installation \u2014 please don\u2019t try to upgrade your existing PMM 1 environment.","title":"Percona Monitoring and Management 2.3.0"},{"location":"release-notes/2.3.0.html#improvements-and-new-features","text":"PMM-5064 and PMM-5065 : Starting from this release, users will be able to integrate PMM with an external Alertmanager by specifying the Alertmanager URL and the Alert Rules to be executed inside the PMM server ( This feature is for advanced users only at this point ) PMM-4954 : Query Analytics dashboard now shows units both in the list of queries in a summary table and in the Details section to ease understanding of the presented data PMM-5179 : Relations between metrics are now specified in the Query Analytics Details section PMM-5115 : The CPU frequency and temperature graphs were added to the CPU Utilization dashboard PMM-5394 : A special treatment for the node-related dashboards was implemented for the situations when the data resolution change causes new metrics to be generated for existing nodes and services, to make graphs show continuous lines of the same colors","title":"Improvements and new features"},{"location":"release-notes/2.3.0.html#fixed-bugs","text":"PMM-4620 : The high CPU usage by the pmm-agent process related to MongoDB Query Analytics was fixed PMM-5377 : singlestats showing percentage had sparklines scaled vertically along with the graph swing, which made it difficult to visually notice the difference between neighboring singlestats . PMM-5204 : Changing resolution on the PMM settings page was breaking some singlestats on the Home and MySQL Overview dashboards PMM-5251 : Vertical scroll bars on the graph elements were not allowed to do a full scroll, making last rows of the legend unavailable for some graphs PMM-5410 : The \u201cAvailable Downtime before SST Required\u201d chart on the PXC/Galera Node Summary dashboard was not showing data because it was unable to use metrics available with different scraping intervals","title":"Fixed bugs"},{"location":"release-notes/2.30.0.html","text":"Percona Monitoring and Management 2.30.0 \u00b6 Release date: Aug 24, 2022 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is an open source database monitoring, management, and observability solution for MySQL, PostgreSQL, and MongoDB. Important We recommend using the latest version of PMM. This ensures that you have access to the latest PMM features and that your environment runs on the latest version of the underlying components, such as VictoriaMetrics, with all the bug fixes in place. Release Highlights \u00b6 New Experimental dashboards \u00b6 Important These experimental dashboards are subject to change. It is recommended to use these dashboards for testing purposes only. We have introduced a more user-friendly and insightful experimental Home Dashboard in PMM version 2.30.0. With this new Home Dashboard, you can experience a boost in performance while monitoring a large number of nodes or services in PMM. Repeated rows (for a large number of nodes) on the dashboard can take a considerable time to load the Home Dashboard UI. Our new experimental dashboard ensures fast retrieval of data on the Home Dashboard. For more information, see documentation . We have introduced the following new real-time experimental dashboards to collect more metrics and data for MongoDB: Experimental MongoDB Collection Overview This dashboard contains panels for data about the Hottest Collections in the MongoDB database. For more information, see documentation . Experimental MongoDB Collection Details This experimental dashboard provides detailed information about the top collections by document count, size, and document read for MongoDB databases. For more information, see documentation . Experimental MongoDB Oplog Details This real-time dashboard contains Oplog details such as Recovery Window, Processing Time, Buffer Capacity, and Oplog Operations. For more information, see documentation . Database as a Service (DBaaS) \u00b6 Enhanced the user experience by simplifying the configuration and deployment of DBaaS database clusters: With PMM 2.30.0, you can add a DB cluster literally at the click of a button. All the fields will be automatically populated with the default value. Click Register , and your DB cluster is added. For more information, see documentation . Starting with PMM 2.30.0, we have simplified the DBaaS configuration and deployment of database clusters by providing suggestions on the k8s cluster name based on the kubeconfig file. We have simplified DBaaS and Percona Platform connection configuration by suggesting and automatically setting the public address of the PMM server if it\u2019s not set up. This happens when connecting to Percona Platform, adding a Kubernetes (K8s) cluster, or adding a DB cluster. Operators support \u00b6 PMM now supports Percona Operator for MySQL based on Percona XtraDB Cluster (1.11). New Features \u00b6 PMM-9652 : Dashboard: New experimental dashboards for MongoDB are now available with more MongoDB metrics and data like Hottest collections, Data Size, Recovery Window, Processing Time, Buffer Capacity, Oplog Operations, and so on. PMM-10367 : Dashboard: We have introduced a new user-friendly and insightful experimental Home Dashboard in PMM version 2.30.0. This Home Dashboard improves the performance while monitoring a large number of nodes or services in PMM. PMM-10327 : You can add a free K8s cluster via Percona Platform Portal and use it for testing DBaaS in PMM. Read more in the Private DBaaS with Free Kubernetes Cluster blogpost . This feature will be available on https://portal.percona.com/ with the PMM 2.30.0 GA release. Improvements \u00b6 PMM-10340 : DBaaS: Enhanced user experience to simplify DBaaS configuration and deployment of Database clusters with suggestions on k8s cluster name based on k8s config file. PMM-10368 : Simpler DBaaS and Percona Platform connection configuration by suggesting the public address of the PMM server. PMM-5680 : Logs: For an enhanced and simplified troubleshooting experience, we have added the following logs in a ZIP file in the pmm-agent folder, which you can obtain from the pmm-admin summary command: exporter logs qan agent logs pmm-agent log PMM-5492 : Logs: We have added the profiling data files to the logs folder to boost the troubleshooting process. You can obtain this by executing the pmm-admin summary --pprof command. PMM-5005 : Logs: Added a more pronounced error message for the PMM agent to help you distinguish between potential problems, such as connectivity issues, unconfigured agent issues, connection time-out issues, etc. PMM-10147 : To minimize resource consumption and DB connections, we have limited the number of concurrent jobs and actions that PMM Agent can execute concurrently. By default, PMM Agent will now run maximum 32 jobs and actions in parralel. You can configure this limit with the flag --runner-capacity=<some number> or environment variable PMM_AGENT_RUNNER_CAPACITY=<some number> . PMM-9914 : PMM generates config files into the /tmp folder when a service is added to monitoring. Files in /tmp can get deleted by the Operating system. With PMM 2.30.0, we have ensured that the pmm-agent successfully restarts the child agents even though the config files in the /tmp folder are deleted and the scraper/exporter processes are killed, thereby ensuring that the exporter is not broken. PMM-9994 : QAN: Improved the description of the QAN filter in the user guide for better clarity. PMM-9632 : For flexibility, we have added the ability to change some of the pg_stat_monitor settings. Previously, this was hardcoded. PMM-10122 : Applied the pmm-agent --log-level to the exporter\u2019s output so that each message from the exporter with log-level=WARN will be logged in pmm-agent with the same log-level=WARN . PMM-9921 : Migrated pmm2-client Docker image to ubi-micro to reduce the number of packages and the image size. PMM-10085 : Support for Percona XtraDB Cluster (PXC) 1.11.0 in PMM. PMM-10351 : DBaaS: Full form of DBaaS (Database as a Service) is displayed on the Dashboard for clarity for the new users. Bugs Fixed \u00b6 PMM-10474 : Fixed an issue where the PMM tour gets in the way when sharing a dashboard via a rendered image with an error message at the top (HTTP 404). PMM-10483 : Fixed an issue where the QAN dashboard was not reloading after modifying the filters, refresh rate, sorting, etc. PMM-10299 : Fixed an issue where the MongoDB ReplSet Dashboard overview section shows incorrect information for the Cluster and ReplSet members count and does not change even after selecting a different cluster name. PMM-10494 : Fixed an issue where the Confirm action modal is stuck until the cluster is registered. PMM-9987 : Fixed an issue in QAN where the metrics values did not match those on the Query details panel. PMM-10159 : Fixed an issue where the logs.zip folder contained the credenatials. PMM-9398 : Fixed an issue where an error was thrown on the MySQL InnoDB Details dashboard for too many data points.","title":"PMM 2.30.0"},{"location":"release-notes/2.30.0.html#percona-monitoring-and-management-2300","text":"Release date: Aug 24, 2022 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is an open source database monitoring, management, and observability solution for MySQL, PostgreSQL, and MongoDB. Important We recommend using the latest version of PMM. This ensures that you have access to the latest PMM features and that your environment runs on the latest version of the underlying components, such as VictoriaMetrics, with all the bug fixes in place.","title":"Percona Monitoring and Management 2.30.0"},{"location":"release-notes/2.30.0.html#release-highlights","text":"","title":"Release Highlights"},{"location":"release-notes/2.30.0.html#new-experimental-dashboards","text":"Important These experimental dashboards are subject to change. It is recommended to use these dashboards for testing purposes only. We have introduced a more user-friendly and insightful experimental Home Dashboard in PMM version 2.30.0. With this new Home Dashboard, you can experience a boost in performance while monitoring a large number of nodes or services in PMM. Repeated rows (for a large number of nodes) on the dashboard can take a considerable time to load the Home Dashboard UI. Our new experimental dashboard ensures fast retrieval of data on the Home Dashboard. For more information, see documentation . We have introduced the following new real-time experimental dashboards to collect more metrics and data for MongoDB: Experimental MongoDB Collection Overview This dashboard contains panels for data about the Hottest Collections in the MongoDB database. For more information, see documentation . Experimental MongoDB Collection Details This experimental dashboard provides detailed information about the top collections by document count, size, and document read for MongoDB databases. For more information, see documentation . Experimental MongoDB Oplog Details This real-time dashboard contains Oplog details such as Recovery Window, Processing Time, Buffer Capacity, and Oplog Operations. For more information, see documentation .","title":"New Experimental dashboards"},{"location":"release-notes/2.30.0.html#database-as-a-service-dbaas","text":"Enhanced the user experience by simplifying the configuration and deployment of DBaaS database clusters: With PMM 2.30.0, you can add a DB cluster literally at the click of a button. All the fields will be automatically populated with the default value. Click Register , and your DB cluster is added. For more information, see documentation . Starting with PMM 2.30.0, we have simplified the DBaaS configuration and deployment of database clusters by providing suggestions on the k8s cluster name based on the kubeconfig file. We have simplified DBaaS and Percona Platform connection configuration by suggesting and automatically setting the public address of the PMM server if it\u2019s not set up. This happens when connecting to Percona Platform, adding a Kubernetes (K8s) cluster, or adding a DB cluster.","title":"Database as a Service (DBaaS)"},{"location":"release-notes/2.30.0.html#operators-support","text":"PMM now supports Percona Operator for MySQL based on Percona XtraDB Cluster (1.11).","title":"Operators support"},{"location":"release-notes/2.30.0.html#new-features","text":"PMM-9652 : Dashboard: New experimental dashboards for MongoDB are now available with more MongoDB metrics and data like Hottest collections, Data Size, Recovery Window, Processing Time, Buffer Capacity, Oplog Operations, and so on. PMM-10367 : Dashboard: We have introduced a new user-friendly and insightful experimental Home Dashboard in PMM version 2.30.0. This Home Dashboard improves the performance while monitoring a large number of nodes or services in PMM. PMM-10327 : You can add a free K8s cluster via Percona Platform Portal and use it for testing DBaaS in PMM. Read more in the Private DBaaS with Free Kubernetes Cluster blogpost . This feature will be available on https://portal.percona.com/ with the PMM 2.30.0 GA release.","title":"New Features"},{"location":"release-notes/2.30.0.html#improvements","text":"PMM-10340 : DBaaS: Enhanced user experience to simplify DBaaS configuration and deployment of Database clusters with suggestions on k8s cluster name based on k8s config file. PMM-10368 : Simpler DBaaS and Percona Platform connection configuration by suggesting the public address of the PMM server. PMM-5680 : Logs: For an enhanced and simplified troubleshooting experience, we have added the following logs in a ZIP file in the pmm-agent folder, which you can obtain from the pmm-admin summary command: exporter logs qan agent logs pmm-agent log PMM-5492 : Logs: We have added the profiling data files to the logs folder to boost the troubleshooting process. You can obtain this by executing the pmm-admin summary --pprof command. PMM-5005 : Logs: Added a more pronounced error message for the PMM agent to help you distinguish between potential problems, such as connectivity issues, unconfigured agent issues, connection time-out issues, etc. PMM-10147 : To minimize resource consumption and DB connections, we have limited the number of concurrent jobs and actions that PMM Agent can execute concurrently. By default, PMM Agent will now run maximum 32 jobs and actions in parralel. You can configure this limit with the flag --runner-capacity=<some number> or environment variable PMM_AGENT_RUNNER_CAPACITY=<some number> . PMM-9914 : PMM generates config files into the /tmp folder when a service is added to monitoring. Files in /tmp can get deleted by the Operating system. With PMM 2.30.0, we have ensured that the pmm-agent successfully restarts the child agents even though the config files in the /tmp folder are deleted and the scraper/exporter processes are killed, thereby ensuring that the exporter is not broken. PMM-9994 : QAN: Improved the description of the QAN filter in the user guide for better clarity. PMM-9632 : For flexibility, we have added the ability to change some of the pg_stat_monitor settings. Previously, this was hardcoded. PMM-10122 : Applied the pmm-agent --log-level to the exporter\u2019s output so that each message from the exporter with log-level=WARN will be logged in pmm-agent with the same log-level=WARN . PMM-9921 : Migrated pmm2-client Docker image to ubi-micro to reduce the number of packages and the image size. PMM-10085 : Support for Percona XtraDB Cluster (PXC) 1.11.0 in PMM. PMM-10351 : DBaaS: Full form of DBaaS (Database as a Service) is displayed on the Dashboard for clarity for the new users.","title":"Improvements"},{"location":"release-notes/2.30.0.html#bugs-fixed","text":"PMM-10474 : Fixed an issue where the PMM tour gets in the way when sharing a dashboard via a rendered image with an error message at the top (HTTP 404). PMM-10483 : Fixed an issue where the QAN dashboard was not reloading after modifying the filters, refresh rate, sorting, etc. PMM-10299 : Fixed an issue where the MongoDB ReplSet Dashboard overview section shows incorrect information for the Cluster and ReplSet members count and does not change even after selecting a different cluster name. PMM-10494 : Fixed an issue where the Confirm action modal is stuck until the cluster is registered. PMM-9987 : Fixed an issue in QAN where the metrics values did not match those on the Query details panel. PMM-10159 : Fixed an issue where the logs.zip folder contained the credenatials. PMM-9398 : Fixed an issue where an error was thrown on the MySQL InnoDB Details dashboard for too many data points.","title":"Bugs Fixed"},{"location":"release-notes/2.31.0.html","text":"Percona Monitoring and Management 2.31.0 \u00b6 Release date: Sep 26, 2022 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is an open source database monitoring, management, and observability solution for MySQL, PostgreSQL, and MongoDB. Important We recommend using the latest version of PMM. This ensures that you have access to the latest PMM features and that your environment runs on the latest version of the underlying components, such as VictoriaMetrics, with all the bug fixes in place. Release Highlights \u00b6 Enhaced query building, navigation and Alerting \u00b6 We have upgraded Grafana to version 9.1 in PMM 2.31.0. With this upgrade, you can get a simplified user interface, enhanced visualizations, and a default unified Alerting experience. For more information, see What\u2019s new in Grafana. The following features have been introduced with PMM 2.31.0: Expandable main menu For an enhanced user experience, we have expanded and designed the main menu by moving the dashboard navigation from the PMM Dashboards menu to the main menu for quick access. So you can now browse dashboards like Operating System (OS), MySQL, MongoDB, PostgreSQL, ProxySQL, and HAProxy directly from the navbar. Prometheus Query Builder We have introduced a new UI query builder using which you can write Prometheus queries effortlessly. Prometheus queries can be complex to grasp, but with this Grafana release, it becomes easier. Starred dashboards To enhance user experience further, we have introduced Starred , where you can access your favorite dashboards directly from the main menu. Explore to dashboard You can now create panels and dashboards directly from Explore . You can also create a panel in a new or existing dashboard by clicking Add to dashboard in the Explore toolbar. The generated panel contains all the pane\u2019s queries, and the default visualization automatically picked from the current results shown in Explore . Improvements to dashboard search For a more effective search experience, we have introduced search by title as well as panel. With this, the overall performance has been enhanced as querying and sorting have become much faster. Command Palette You can now pull up a palette of commands for easier navigation using cmd+k (macOS) or ctrl+k (Linux/Windows). General availability of Percona Alerting \u00b6 With this release, we are streamlining the PMM alert setup process with an overhauled, unified alerting system based on Grafana. All previous Integrated Alerting and Grafana Alerting functionality are now consolidated on the Alerting page. From here, you can configure, create and monitor alerts based on Percona or Grafana templates. The Alert Rules tab has also evolved into a more efficient interface with an added layer for simplifying complex Grafana rules. You\u2019ll find that the new Percona templated alert option here offers the same functionality available in the old Integrated Alerting, but uses a friendlier interface, powered by Grafana\u2019s advanced alerting capabilities. As an important and generally useful feature, this new Alerting feature is now enabled by default and ready to use in production! For more information about Percona Alerting, check out Percona Alerting . Deprecated Integrated Alerting \u00b6 The new Percona Alerting feature fully replaces the old Integrated Alerting available in previous PMM versions. The new alerting brings full feature parity with Integrated Alerting, along with additional benefits like Grafana-based alert rules and a unified alerting command center. If you have been using Integrated Alerting in the previous PMM version, your custom alert rule templates will be automatically migrated to PMM 2.31. After upgrading to this new version, you will find all your alert templates under Alerting > Alert Templates . These templates are also available for creating new alert rules using the new Percona templated alerts option on the Alert Rules tab. However, alert rules created with Integrated Alerting are not automatically migrated to Percona Alerting. After upgrading, make sure to manually migrate any custom alert rules that you want to transfer to PMM 2.31 using the Integrated Alerting Migration Script . Vacuum monitoring dashboard \u00b6 Important This experimental dashboard is subject to change. It is recommended to use this dashboard for testing purposes only. A new experimental dashboard has been released to help our users gain timely insights into the autovacuum process in PostgreSQL. This dashboard is designed to help fine-tune the vacuum configuration and to prevent a XID wraparound - both of which directly contribute to improved performance of the database. This dashboard contains the following: Dead tuples - Identifies the number of dead rows in each table even though the rows are physically removed from the table. Last time vacuum ran - Tracks the last time a vacuum or autovacuum process successfully ran on each of your tables. Number of rows modified since last Analyze - The number of rows changed since the last time ANALYZE ran. Manual vacuum events - Tracks the number of times a manual vacuum was run on each table. Table disk usage - Tracking the disk space used by each table is crucial as it enables you to gauge expected changes in the query performance over time - but it can also help you detect potential vacuuming-related issues. For more information, see documentation New official (GA) deployment method - Podman \u00b6 We are excited to announce the General Availability (GA) of Podman support for Deploying PMM 2.31.0. We had introduced it in 2.29.0 as a preview feature, but now we are production ready with this feature. Simplied deployment with Database as a Service (DBaaS) \u00b6 In our constant endeavor and focus on an enhanced user experience, in PMM 2.31.0, we have simplified the deployment and configuration of DBaaS as follows: With PMM 2.31.0, you can easily add a DB cluster from the newly created K8s cluster. All the DB cluster window fields are auto-populated with the values based on the existing K8s cluster. For PMM 2.31.0, while accessing DbaaS, if you have an existing Kubernetes cluster configured for DBaaS, you will be automatically redirected to the DB Cluster page. Otherwise, you would be redirected to the Kubernetes Cluster page. New Features \u00b6 PMM-10089 : We have upgraded Grafana to 9.1 in PMM 2.31.0. PMM-10092 : We have expanded and improved the main menu for PMM 2.31.0 based on Grafana 9.1. The main menu contains crucial elements, one of them being starred dashboards. PMM-10467 : Integrated Alerting: Alert rules created with the Integrated Alerting feature in previous PMM versions can be migrated to the new Percona Alerting using the script that will be provided for the public release of PMM 2.31. PMM-10443 : Dashboards: We have released a new experimental dashboard for collecting metrics for vacuum monitoring to help our users gain insight into the autovacuum process in PostgreSQL. Improvements \u00b6 PMM-10560 : Dashboard: We have improved the MongoDB Collection Details Dashboard. The Collection Activity panel is now collapsed by default, and the graphs are visible only when the --enable-all-collectors parameter is passed in pmm-admin command. PMM-10349 : DBaaS: For Simplified DBaaS experience, you will be automatically redirected to the DB Cluster page if you have an existing Kubernetes cluster configured for DBaaS. Otherwise, you would be redirected to the Kubernetes Cluster page. PMM-10064 : Introduced UI changes on the PMM Inventory pages for better readability and user experience. PMM-10076 : For an improved UI experience, we have enforced the enter button for the date picker, which performs the same action as clicking apply time range . PMM-10018 : Lowered the default values of CPU and memory for haproxy while deploying a DBaaS Percona XtraDB Cluster (PXC) cluster to avoid wasting resources. PMM-9118 : Refined the error message thrown when a secure connection cannot be established while adding a monitoring database to PMM, thus making it much more contextual and easy to troubleshoot. PMM-10516 : Added support for MongoDB physical backups. With this, PMM backup management now supports both logical and physical backups for MongoDB services that are based on Percona Server for MongoDB . Percona Backup for MongoDB (PBM) compatibility : MongoDB backups in PMM require Percona Backup for MongoDB version 1.8.x. Make sure not to install the latest BPM version via the percona-release tool, as this automatically installs BPM v.2.0 which is not yet compatible with PMM. PMM-10095 : While removing the service with pmm-admin, now you do not have to specify the service name for a single service. PMM will automatically look up this service and remove it. PMM-10495 : By default, Swagger UI attempts to validate specs against swagger.io\u2019s online validator. For PMM 2.31.0, we have disabled it to safeguard your privacy. PMM-7806 : Upgraded postgres_exporter version used in pmm from 0.8.0 to 0.10.1. With this upgrade, you can access the latest features with all the bug fixes in place. Bugs Fixed \u00b6 PMM-10628 : While adding MongoDB or PostgreSQL database via socket, PMM was throwing an error. This has been fixed in PMM 2.31.0. PMM-10624 : Empty dataname label after PostgreSQL upgraded in PMM. PMM-10608 : Fixed issues pertaining to the new Home Dashboard, such as high CPU panel unreadable for a higher number of nodes (more than ten nodes), filtering for the environment as well as node name not working correctly, time range not working as expected, some panels on the dashboard not displaying data for selected filters, and so on. PMM-10587 : Fixed an issue where if you hover over the telemetry icon stacktrace with error appears. PMM-10513 : Fixed an issue where pmm-admin summary stored PMM password in client/status.json, thus posing a security risk. PMM-10065 : Fixed an issue encountered while monitoring a large number of nodes (1000) and services. Here, the changing scrape frequency led to data not being captured on the dashboard and throwing a templating error. PMM-9973 : Fixed an issue where the CPU utilization for Postgres Exporter was on the higher side. PMM-10044 : Fixed an issue where QAN displayed enormously high values. PMM-10218 : Fixed an issue for corrupted pprof file on concurrent pprof request.","title":"PMM 2.31.0"},{"location":"release-notes/2.31.0.html#percona-monitoring-and-management-2310","text":"Release date: Sep 26, 2022 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is an open source database monitoring, management, and observability solution for MySQL, PostgreSQL, and MongoDB. Important We recommend using the latest version of PMM. This ensures that you have access to the latest PMM features and that your environment runs on the latest version of the underlying components, such as VictoriaMetrics, with all the bug fixes in place.","title":"Percona Monitoring and Management 2.31.0"},{"location":"release-notes/2.31.0.html#release-highlights","text":"","title":"Release Highlights"},{"location":"release-notes/2.31.0.html#enhaced-query-building-navigation-and-alerting","text":"We have upgraded Grafana to version 9.1 in PMM 2.31.0. With this upgrade, you can get a simplified user interface, enhanced visualizations, and a default unified Alerting experience. For more information, see What\u2019s new in Grafana. The following features have been introduced with PMM 2.31.0: Expandable main menu For an enhanced user experience, we have expanded and designed the main menu by moving the dashboard navigation from the PMM Dashboards menu to the main menu for quick access. So you can now browse dashboards like Operating System (OS), MySQL, MongoDB, PostgreSQL, ProxySQL, and HAProxy directly from the navbar. Prometheus Query Builder We have introduced a new UI query builder using which you can write Prometheus queries effortlessly. Prometheus queries can be complex to grasp, but with this Grafana release, it becomes easier. Starred dashboards To enhance user experience further, we have introduced Starred , where you can access your favorite dashboards directly from the main menu. Explore to dashboard You can now create panels and dashboards directly from Explore . You can also create a panel in a new or existing dashboard by clicking Add to dashboard in the Explore toolbar. The generated panel contains all the pane\u2019s queries, and the default visualization automatically picked from the current results shown in Explore . Improvements to dashboard search For a more effective search experience, we have introduced search by title as well as panel. With this, the overall performance has been enhanced as querying and sorting have become much faster. Command Palette You can now pull up a palette of commands for easier navigation using cmd+k (macOS) or ctrl+k (Linux/Windows).","title":"Enhaced query building, navigation and Alerting"},{"location":"release-notes/2.31.0.html#general-availability-of-percona-alerting","text":"With this release, we are streamlining the PMM alert setup process with an overhauled, unified alerting system based on Grafana. All previous Integrated Alerting and Grafana Alerting functionality are now consolidated on the Alerting page. From here, you can configure, create and monitor alerts based on Percona or Grafana templates. The Alert Rules tab has also evolved into a more efficient interface with an added layer for simplifying complex Grafana rules. You\u2019ll find that the new Percona templated alert option here offers the same functionality available in the old Integrated Alerting, but uses a friendlier interface, powered by Grafana\u2019s advanced alerting capabilities. As an important and generally useful feature, this new Alerting feature is now enabled by default and ready to use in production! For more information about Percona Alerting, check out Percona Alerting .","title":"General availability of Percona Alerting"},{"location":"release-notes/2.31.0.html#deprecated-integrated-alerting","text":"The new Percona Alerting feature fully replaces the old Integrated Alerting available in previous PMM versions. The new alerting brings full feature parity with Integrated Alerting, along with additional benefits like Grafana-based alert rules and a unified alerting command center. If you have been using Integrated Alerting in the previous PMM version, your custom alert rule templates will be automatically migrated to PMM 2.31. After upgrading to this new version, you will find all your alert templates under Alerting > Alert Templates . These templates are also available for creating new alert rules using the new Percona templated alerts option on the Alert Rules tab. However, alert rules created with Integrated Alerting are not automatically migrated to Percona Alerting. After upgrading, make sure to manually migrate any custom alert rules that you want to transfer to PMM 2.31 using the Integrated Alerting Migration Script .","title":"Deprecated Integrated Alerting"},{"location":"release-notes/2.31.0.html#vacuum-monitoring-dashboard","text":"Important This experimental dashboard is subject to change. It is recommended to use this dashboard for testing purposes only. A new experimental dashboard has been released to help our users gain timely insights into the autovacuum process in PostgreSQL. This dashboard is designed to help fine-tune the vacuum configuration and to prevent a XID wraparound - both of which directly contribute to improved performance of the database. This dashboard contains the following: Dead tuples - Identifies the number of dead rows in each table even though the rows are physically removed from the table. Last time vacuum ran - Tracks the last time a vacuum or autovacuum process successfully ran on each of your tables. Number of rows modified since last Analyze - The number of rows changed since the last time ANALYZE ran. Manual vacuum events - Tracks the number of times a manual vacuum was run on each table. Table disk usage - Tracking the disk space used by each table is crucial as it enables you to gauge expected changes in the query performance over time - but it can also help you detect potential vacuuming-related issues. For more information, see documentation","title":"Vacuum monitoring dashboard"},{"location":"release-notes/2.31.0.html#new-official-ga-deployment-method-podman","text":"We are excited to announce the General Availability (GA) of Podman support for Deploying PMM 2.31.0. We had introduced it in 2.29.0 as a preview feature, but now we are production ready with this feature.","title":"New official (GA) deployment method - Podman"},{"location":"release-notes/2.31.0.html#simplied-deployment-with-database-as-a-service-dbaas","text":"In our constant endeavor and focus on an enhanced user experience, in PMM 2.31.0, we have simplified the deployment and configuration of DBaaS as follows: With PMM 2.31.0, you can easily add a DB cluster from the newly created K8s cluster. All the DB cluster window fields are auto-populated with the values based on the existing K8s cluster. For PMM 2.31.0, while accessing DbaaS, if you have an existing Kubernetes cluster configured for DBaaS, you will be automatically redirected to the DB Cluster page. Otherwise, you would be redirected to the Kubernetes Cluster page.","title":"Simplied deployment with Database as a Service (DBaaS)"},{"location":"release-notes/2.31.0.html#new-features","text":"PMM-10089 : We have upgraded Grafana to 9.1 in PMM 2.31.0. PMM-10092 : We have expanded and improved the main menu for PMM 2.31.0 based on Grafana 9.1. The main menu contains crucial elements, one of them being starred dashboards. PMM-10467 : Integrated Alerting: Alert rules created with the Integrated Alerting feature in previous PMM versions can be migrated to the new Percona Alerting using the script that will be provided for the public release of PMM 2.31. PMM-10443 : Dashboards: We have released a new experimental dashboard for collecting metrics for vacuum monitoring to help our users gain insight into the autovacuum process in PostgreSQL.","title":"New Features"},{"location":"release-notes/2.31.0.html#improvements","text":"PMM-10560 : Dashboard: We have improved the MongoDB Collection Details Dashboard. The Collection Activity panel is now collapsed by default, and the graphs are visible only when the --enable-all-collectors parameter is passed in pmm-admin command. PMM-10349 : DBaaS: For Simplified DBaaS experience, you will be automatically redirected to the DB Cluster page if you have an existing Kubernetes cluster configured for DBaaS. Otherwise, you would be redirected to the Kubernetes Cluster page. PMM-10064 : Introduced UI changes on the PMM Inventory pages for better readability and user experience. PMM-10076 : For an improved UI experience, we have enforced the enter button for the date picker, which performs the same action as clicking apply time range . PMM-10018 : Lowered the default values of CPU and memory for haproxy while deploying a DBaaS Percona XtraDB Cluster (PXC) cluster to avoid wasting resources. PMM-9118 : Refined the error message thrown when a secure connection cannot be established while adding a monitoring database to PMM, thus making it much more contextual and easy to troubleshoot. PMM-10516 : Added support for MongoDB physical backups. With this, PMM backup management now supports both logical and physical backups for MongoDB services that are based on Percona Server for MongoDB . Percona Backup for MongoDB (PBM) compatibility : MongoDB backups in PMM require Percona Backup for MongoDB version 1.8.x. Make sure not to install the latest BPM version via the percona-release tool, as this automatically installs BPM v.2.0 which is not yet compatible with PMM. PMM-10095 : While removing the service with pmm-admin, now you do not have to specify the service name for a single service. PMM will automatically look up this service and remove it. PMM-10495 : By default, Swagger UI attempts to validate specs against swagger.io\u2019s online validator. For PMM 2.31.0, we have disabled it to safeguard your privacy. PMM-7806 : Upgraded postgres_exporter version used in pmm from 0.8.0 to 0.10.1. With this upgrade, you can access the latest features with all the bug fixes in place.","title":"Improvements"},{"location":"release-notes/2.31.0.html#bugs-fixed","text":"PMM-10628 : While adding MongoDB or PostgreSQL database via socket, PMM was throwing an error. This has been fixed in PMM 2.31.0. PMM-10624 : Empty dataname label after PostgreSQL upgraded in PMM. PMM-10608 : Fixed issues pertaining to the new Home Dashboard, such as high CPU panel unreadable for a higher number of nodes (more than ten nodes), filtering for the environment as well as node name not working correctly, time range not working as expected, some panels on the dashboard not displaying data for selected filters, and so on. PMM-10587 : Fixed an issue where if you hover over the telemetry icon stacktrace with error appears. PMM-10513 : Fixed an issue where pmm-admin summary stored PMM password in client/status.json, thus posing a security risk. PMM-10065 : Fixed an issue encountered while monitoring a large number of nodes (1000) and services. Here, the changing scrape frequency led to data not being captured on the dashboard and throwing a templating error. PMM-9973 : Fixed an issue where the CPU utilization for Postgres Exporter was on the higher side. PMM-10044 : Fixed an issue where QAN displayed enormously high values. PMM-10218 : Fixed an issue for corrupted pprof file on concurrent pprof request.","title":"Bugs Fixed"},{"location":"release-notes/2.4.0.html","text":"Percona Monitoring and Management 2.4.0 \u00b6 Date: March 18, 2020 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. New Features \u00b6 PMM-3387 : Prometheus custom configuration is now supported by PMM Server. The feature is targeted at experienced users and is done by adding the base configuration file into the PMM Server container to be parsed and included into the managed Prometheus configuration. PMM-5186 : Including \u2013-pprof option in the pmm-admin summary command adds pprof debug profiles to the diagnostics data archive PMM-5102 : The new \u201cNode Details\u201d dashboard now displays data from the hardware monitoring sensors in hwmon . The new dashboard is based on the hwmon collector data from the node_exporter . Please note that data may be unavailable for some nodes because of the configuration or virtualization parameters. Improvements \u00b6 PMM-4915 : The Query Analytics dashboard now shows Time Metrics in the Profile Section as \u201cAVG per query\u201d instead of \u201cAVG per second\u201d PMM-5470 : ClickHouse query optimized for Query Analytics to improve its speed and reduce the load on the back-end PMM-5448 : The default high and medium metrics resolutions were changed to 1-5-30 and 5-10-60 sec. To reduce the effect of this change on existing installations, systems having the \u201cold\u201d high resolution chosen on the PMM Settings page (5-5-60 sec.) will be automatically re-configured to the medium one during an upgrade. If the resolution was changed to some custom values via API, it will not be affected PMM-5531 : A health check indicator was implemented for the PMM Server Docker image. It is based on the Docker HEALTHCHECK . This feature can be used as follows: docker inspect -f {{ .State.Health.Status }} until [ \"`docker inspect -f {{.State.Health.Status}} pmm-server`\" == \"healthy\" ] ; do sleep 1 ; done PMM-5489 : The \u201cTotal\u201d line in all charts is now drawn with the same red color for better consistency PMM-5461 : Memory graphs on the node-related dashboards were adjusted to have fixed colors that are more distinguishable from each other PMM-5329 : Prometheus in PMM Server was updated to version 2.16.0. This update has brought several improvements. Among them are significantly reduced memory footprint of the loaded TSDB blocks, lower memory footprint for the compaction process (caused by the more balanced choice of what to buffer during compaction), and improved query performance for the queries that only touch the most recent 2 hours of data. PMM-5210 : Data Retention is now specified in days instead of seconds on the PMM Settings page. Please note this is a UI-only change, so the actual data retention precision is not changed PMM-5182 : The logs.zip archive available on the PMM Settings page now includes additional self-monitoring information in a separate client subfolder. This subfolder contains information collected on the PMM Server and is equivalent to the one collected on a node by the pmm-admin summary command. PMM-5112 : The Inventory API List requests now can be filtered by the Node/Service/Agent type Bugs Fixed \u00b6 PMM-5178 : Query Detail Section of the Query Analytics dashboard didn\u2019t show tables definitions and indexes for the internal PostgreSQL database PMM-5465 : MySQL Instance related dashboards had row names not always matching the actual contents. To fix this, elements were re-ordered and additional rows were added for better matching of the row name and the corresponding elements PMM-5455 : Dashboards from the Insight menu were fixed to work correctly when the low resolution is set on the PMM Settings page PMM-5446 : A number of the Compare Dashboards were fixed to work correctly when the low resolution is set on the PMM Settings page PMM-5430 : MySQL Exporter section on the Prometheus Exporter Status dashboard now collapsed by default to be consistent with other database-related sections PMM-5445 , PMM-5439 , PMM-5427 , PMM-5426 , PMM-5419 : Labels change (which occurs e.g. when the metrics resolution is changed on the PMM Settings page) was breaking dashboards PMM-5347 : Selecting queries on the Query Analytics dashboard was generating errors in the browser console PMM-5305 : Some applied filters on the Query Analytics dashboard were not preserved after changing the time range PMM-5267 : The Refresh button was not working on the Query Analytics dashboard PMM-5003 : pmm-admin list and status use different JSON naming for the same data PMM-5526 : A typo was fixed in the Replication Dashboard description tooltip Help us improve our software quality by reporting any bugs you encounter using our bug tracking system .","title":"PMM 2.4.0"},{"location":"release-notes/2.4.0.html#percona-monitoring-and-management-240","text":"Date: March 18, 2020 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance.","title":"Percona Monitoring and Management 2.4.0"},{"location":"release-notes/2.4.0.html#new-features","text":"PMM-3387 : Prometheus custom configuration is now supported by PMM Server. The feature is targeted at experienced users and is done by adding the base configuration file into the PMM Server container to be parsed and included into the managed Prometheus configuration. PMM-5186 : Including \u2013-pprof option in the pmm-admin summary command adds pprof debug profiles to the diagnostics data archive PMM-5102 : The new \u201cNode Details\u201d dashboard now displays data from the hardware monitoring sensors in hwmon . The new dashboard is based on the hwmon collector data from the node_exporter . Please note that data may be unavailable for some nodes because of the configuration or virtualization parameters.","title":"New Features"},{"location":"release-notes/2.4.0.html#improvements","text":"PMM-4915 : The Query Analytics dashboard now shows Time Metrics in the Profile Section as \u201cAVG per query\u201d instead of \u201cAVG per second\u201d PMM-5470 : ClickHouse query optimized for Query Analytics to improve its speed and reduce the load on the back-end PMM-5448 : The default high and medium metrics resolutions were changed to 1-5-30 and 5-10-60 sec. To reduce the effect of this change on existing installations, systems having the \u201cold\u201d high resolution chosen on the PMM Settings page (5-5-60 sec.) will be automatically re-configured to the medium one during an upgrade. If the resolution was changed to some custom values via API, it will not be affected PMM-5531 : A health check indicator was implemented for the PMM Server Docker image. It is based on the Docker HEALTHCHECK . This feature can be used as follows: docker inspect -f {{ .State.Health.Status }} until [ \"`docker inspect -f {{.State.Health.Status}} pmm-server`\" == \"healthy\" ] ; do sleep 1 ; done PMM-5489 : The \u201cTotal\u201d line in all charts is now drawn with the same red color for better consistency PMM-5461 : Memory graphs on the node-related dashboards were adjusted to have fixed colors that are more distinguishable from each other PMM-5329 : Prometheus in PMM Server was updated to version 2.16.0. This update has brought several improvements. Among them are significantly reduced memory footprint of the loaded TSDB blocks, lower memory footprint for the compaction process (caused by the more balanced choice of what to buffer during compaction), and improved query performance for the queries that only touch the most recent 2 hours of data. PMM-5210 : Data Retention is now specified in days instead of seconds on the PMM Settings page. Please note this is a UI-only change, so the actual data retention precision is not changed PMM-5182 : The logs.zip archive available on the PMM Settings page now includes additional self-monitoring information in a separate client subfolder. This subfolder contains information collected on the PMM Server and is equivalent to the one collected on a node by the pmm-admin summary command. PMM-5112 : The Inventory API List requests now can be filtered by the Node/Service/Agent type","title":"Improvements"},{"location":"release-notes/2.4.0.html#bugs-fixed","text":"PMM-5178 : Query Detail Section of the Query Analytics dashboard didn\u2019t show tables definitions and indexes for the internal PostgreSQL database PMM-5465 : MySQL Instance related dashboards had row names not always matching the actual contents. To fix this, elements were re-ordered and additional rows were added for better matching of the row name and the corresponding elements PMM-5455 : Dashboards from the Insight menu were fixed to work correctly when the low resolution is set on the PMM Settings page PMM-5446 : A number of the Compare Dashboards were fixed to work correctly when the low resolution is set on the PMM Settings page PMM-5430 : MySQL Exporter section on the Prometheus Exporter Status dashboard now collapsed by default to be consistent with other database-related sections PMM-5445 , PMM-5439 , PMM-5427 , PMM-5426 , PMM-5419 : Labels change (which occurs e.g. when the metrics resolution is changed on the PMM Settings page) was breaking dashboards PMM-5347 : Selecting queries on the Query Analytics dashboard was generating errors in the browser console PMM-5305 : Some applied filters on the Query Analytics dashboard were not preserved after changing the time range PMM-5267 : The Refresh button was not working on the Query Analytics dashboard PMM-5003 : pmm-admin list and status use different JSON naming for the same data PMM-5526 : A typo was fixed in the Replication Dashboard description tooltip Help us improve our software quality by reporting any bugs you encounter using our bug tracking system .","title":"Bugs Fixed"},{"location":"release-notes/2.5.0.html","text":"Percona Monitoring and Management 2.5.0 \u00b6 Date: April 14, 2020 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. New Features \u00b6 PMM-5042 and PMM-5272 : PMM can now connect to MySQL instances by specifying a UNIX socket. This can be done with a new --socket option of the pmm-admin add mysql command. (Note: Updates to both PMM Client and PMM Server were done to allow UNIX socket connections.) PMM-4145 : Amazon RDS instance metrics can now be independently enabled/disabled for Basic and/or Enhanced metrics. Improvements \u00b6 PMM-5581 : PMM Server Grafana plugins can now be updated on the command line with the grafana-cli command-line utility. PMM-5536 : Three Grafana plugins were updated to the latest versions: vertamedia-clickhouse-datasource to 1.9.5, grafana-polystat-panel to 1.1.0, and grafana-piechart-panel to 1.4.0. PMM-4252 : The resolution of the PMM Server favicon image has been improved. Bugs Fixed \u00b6 PMM-5547 : PMM dashboards were failing when presenting data from more than 100 monitored instances (error message proxy error: context canceled ). PMM-5624 : Empty charts were being shown in some Node Temperature dashboards. PMM-5637 : The Data retention value in Settings was incorrectly showing the value as minutes instead of days. PMM-5613 : Sorting data by Query Time was not working properly in Query Analytics. PMM-5554 : Totals in charts were inconsistently plotted with different colors across charts. PMM-4919 : The force option ( --force ) in pmm-admin config was not always working. PMM-5351 : The documentation on MongoDB user privileges has been corrected. Help us improve our software quality by reporting any bugs you encounter using our bug tracking system .","title":"PMM 2.5.0"},{"location":"release-notes/2.5.0.html#percona-monitoring-and-management-250","text":"Date: April 14, 2020 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance.","title":"Percona Monitoring and Management 2.5.0"},{"location":"release-notes/2.5.0.html#new-features","text":"PMM-5042 and PMM-5272 : PMM can now connect to MySQL instances by specifying a UNIX socket. This can be done with a new --socket option of the pmm-admin add mysql command. (Note: Updates to both PMM Client and PMM Server were done to allow UNIX socket connections.) PMM-4145 : Amazon RDS instance metrics can now be independently enabled/disabled for Basic and/or Enhanced metrics.","title":"New Features"},{"location":"release-notes/2.5.0.html#improvements","text":"PMM-5581 : PMM Server Grafana plugins can now be updated on the command line with the grafana-cli command-line utility. PMM-5536 : Three Grafana plugins were updated to the latest versions: vertamedia-clickhouse-datasource to 1.9.5, grafana-polystat-panel to 1.1.0, and grafana-piechart-panel to 1.4.0. PMM-4252 : The resolution of the PMM Server favicon image has been improved.","title":"Improvements"},{"location":"release-notes/2.5.0.html#bugs-fixed","text":"PMM-5547 : PMM dashboards were failing when presenting data from more than 100 monitored instances (error message proxy error: context canceled ). PMM-5624 : Empty charts were being shown in some Node Temperature dashboards. PMM-5637 : The Data retention value in Settings was incorrectly showing the value as minutes instead of days. PMM-5613 : Sorting data by Query Time was not working properly in Query Analytics. PMM-5554 : Totals in charts were inconsistently plotted with different colors across charts. PMM-4919 : The force option ( --force ) in pmm-admin config was not always working. PMM-5351 : The documentation on MongoDB user privileges has been corrected. Help us improve our software quality by reporting any bugs you encounter using our bug tracking system .","title":"Bugs Fixed"},{"location":"release-notes/2.6.0.html","text":"Percona Monitoring and Management 2.6.0 \u00b6 Date: May 11, 2020 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. New Features \u00b6 PMM-5728 : Technical preview of External Services monitoring feature. A new command provides integration with hundreds of third-party systems ( https://prometheus.io/docs/instrumenting/exporters/ ) via the Prometheus protocol so that you can monitor external services on a node where PMM agent is installed. PMM-5822 : PMM now includes a Security Threat Tool to help users avoid the most common database security issues. Read more here . PMM-5559 : Global annotations can now be set with the pmm-admin annotate command. PMM-4931 : PMM now checks Docker environment variables and warns about invalid ones. Improvements \u00b6 PMM-1962 : The PMM Server API (via /v1/readyz ) now also returns Grafana status information in addition to that for Prometheus. PMM-5854 : The Service Details dashboards were cleaned up and some unused selectors were removed. PMM-5775 : It is now clearer which nodes are Primary and which are Secondary on MongoDB Instance dashboards. PMM-5549 : PMM\u2019s Grafana component is now the latest, 6.7.3. PMM-5393 : There\u2019s a new \u2018Node Summary\u2019 row in the services Summary and Details dashboards summarizing the system update, load average, RAM and memory. PMM-4778 : mongodb_exporter is now the latest version, 0.11.0. PMM-5734 : Temporary files activity and utilization charts ( rate & irate ) were added to the PostgreSQL Instance overview. PMM-5695 : The error message explains better when using the \u2013-socket option incorrectly. Bugs Fixed \u00b6 PMM-4829 : The MongoDB Exporter wasn\u2019t able to collect metrics from hidden nodes without either the latest driver or using the connect-direct parameter. PMM-5056 : The average values for Query time in the Details and Profile sections were different. PMM-2717 : Updating MongoDB Exporter resolves an error ( Failed to execute find query on 'config.locks': not found. ) when used with shardedCluster 3.6.4. PMM-4541 : MongoDB exporter metrics collection was including system collections from collStats and indexStats , causing \u201clog bloat\u201d. PMM-5913 : Only totals were shown in QAN when filtering on Cluster=MongoDB . PMM-5903 : When applying a filter the QAN Overview was being refreshed twice. PMM-5821 : The Compare button was missing from HA Dashboard main menus. PMM-5687 : Cumulative charts for Disk Details were not showing any data if metrics were returning NaN results. PMM-5663 : The \u2018version\u2019 value was not being refreshed in various MySQL dashboards. PMM-5643 : Advanced Data Exploration charts were showing \u2018N/A\u2019 for Metric Resolution and \u2018No data to show\u2019 in the Metric Data Table. PMM-4756 : Dashboards were not showing services with empty environments. PMM-4562 : MongoDB and MySQL registered instances with empty cluster labels ( \u2013environment=<label> ) were not visible in the dashboard despite being added instances. PMM-4906 : The MongoDB exporter for MongoDB 4.0 and above was causing a \u201clog bloat\u201d condition. Help us improve our software quality by reporting any bugs you encounter using our bug tracking system .","title":"PMM 2.6.0"},{"location":"release-notes/2.6.0.html#percona-monitoring-and-management-260","text":"Date: May 11, 2020 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance.","title":"Percona Monitoring and Management 2.6.0"},{"location":"release-notes/2.6.0.html#new-features","text":"PMM-5728 : Technical preview of External Services monitoring feature. A new command provides integration with hundreds of third-party systems ( https://prometheus.io/docs/instrumenting/exporters/ ) via the Prometheus protocol so that you can monitor external services on a node where PMM agent is installed. PMM-5822 : PMM now includes a Security Threat Tool to help users avoid the most common database security issues. Read more here . PMM-5559 : Global annotations can now be set with the pmm-admin annotate command. PMM-4931 : PMM now checks Docker environment variables and warns about invalid ones.","title":"New Features"},{"location":"release-notes/2.6.0.html#improvements","text":"PMM-1962 : The PMM Server API (via /v1/readyz ) now also returns Grafana status information in addition to that for Prometheus. PMM-5854 : The Service Details dashboards were cleaned up and some unused selectors were removed. PMM-5775 : It is now clearer which nodes are Primary and which are Secondary on MongoDB Instance dashboards. PMM-5549 : PMM\u2019s Grafana component is now the latest, 6.7.3. PMM-5393 : There\u2019s a new \u2018Node Summary\u2019 row in the services Summary and Details dashboards summarizing the system update, load average, RAM and memory. PMM-4778 : mongodb_exporter is now the latest version, 0.11.0. PMM-5734 : Temporary files activity and utilization charts ( rate & irate ) were added to the PostgreSQL Instance overview. PMM-5695 : The error message explains better when using the \u2013-socket option incorrectly.","title":"Improvements"},{"location":"release-notes/2.6.0.html#bugs-fixed","text":"PMM-4829 : The MongoDB Exporter wasn\u2019t able to collect metrics from hidden nodes without either the latest driver or using the connect-direct parameter. PMM-5056 : The average values for Query time in the Details and Profile sections were different. PMM-2717 : Updating MongoDB Exporter resolves an error ( Failed to execute find query on 'config.locks': not found. ) when used with shardedCluster 3.6.4. PMM-4541 : MongoDB exporter metrics collection was including system collections from collStats and indexStats , causing \u201clog bloat\u201d. PMM-5913 : Only totals were shown in QAN when filtering on Cluster=MongoDB . PMM-5903 : When applying a filter the QAN Overview was being refreshed twice. PMM-5821 : The Compare button was missing from HA Dashboard main menus. PMM-5687 : Cumulative charts for Disk Details were not showing any data if metrics were returning NaN results. PMM-5663 : The \u2018version\u2019 value was not being refreshed in various MySQL dashboards. PMM-5643 : Advanced Data Exploration charts were showing \u2018N/A\u2019 for Metric Resolution and \u2018No data to show\u2019 in the Metric Data Table. PMM-4756 : Dashboards were not showing services with empty environments. PMM-4562 : MongoDB and MySQL registered instances with empty cluster labels ( \u2013environment=<label> ) were not visible in the dashboard despite being added instances. PMM-4906 : The MongoDB exporter for MongoDB 4.0 and above was causing a \u201clog bloat\u201d condition. Help us improve our software quality by reporting any bugs you encounter using our bug tracking system .","title":"Bugs Fixed"},{"location":"release-notes/2.6.1.html","text":"Percona Monitoring and Management 2.6.1 \u00b6 Date: May 18, 2020 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. Improvements \u00b6 PMM-5936 : Improved Summary dashboard for Security Threat Tool \u2018Failed Checks\u2019 PMM-5937 : Improved Details dashboard for Security Threat Tool \u2018Failed Database Checks\u2019 Bugs Fixed \u00b6 PMM-5924 : Alertmanager not running after PMM Server upgrade via Docker PMM-5915 : supervisord not restarting after restart of PMM Server virtual appliances (OVF/AMI) PMM-5945 : \u2018Updates\u2019 dashboard not showing available updates PMM-5870 : MySQL Table Details dashboard not showing separate service names for tables","title":"PMM 2.6.1"},{"location":"release-notes/2.6.1.html#percona-monitoring-and-management-261","text":"Date: May 18, 2020 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance.","title":"Percona Monitoring and Management 2.6.1"},{"location":"release-notes/2.6.1.html#improvements","text":"PMM-5936 : Improved Summary dashboard for Security Threat Tool \u2018Failed Checks\u2019 PMM-5937 : Improved Details dashboard for Security Threat Tool \u2018Failed Database Checks\u2019","title":"Improvements"},{"location":"release-notes/2.6.1.html#bugs-fixed","text":"PMM-5924 : Alertmanager not running after PMM Server upgrade via Docker PMM-5915 : supervisord not restarting after restart of PMM Server virtual appliances (OVF/AMI) PMM-5945 : \u2018Updates\u2019 dashboard not showing available updates PMM-5870 : MySQL Table Details dashboard not showing separate service names for tables","title":"Bugs Fixed"},{"location":"release-notes/2.7.0.html","text":"Percona Monitoring and Management 2.7.0 \u00b6 Date: June 9, 2020 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. In this release, we have updated Grafana to version 6.7.4 to fix CVE-2020-13379 . We recommend updating to the latest version of PMM as soon as possible. New Features \u00b6 PMM-5257 , PMM-5256 , & PMM-5243 : pmm-admin socket option ( \u2013-socket ) to specify UNIX socket path for connecting to MongoDB, PostgreSQL, and ProxySQL instances Improvements \u00b6 PMM-2244 : pmm-admin status command output shows both pmm-admin and pmm-agent versions PMM-5968 : Disallow PMM Server node or agent removal via API PMM-5946 : MySQL Table Details dashboard filter on Service Name prevents display of services without data PMM-5926 : Expose PMM agent version in pmm-admin status command PMM-5891 : PMM Home page now includes News panel PMM-5906 : Independent update of PMM components deactivated Bugs Fixed \u00b6 PMM-6004 : MySQL exporter reporting wrong values for cluster status ( wsrep_cluster_status ) PMM-4547 : MongoDB dashboard replication lag count incorrect PMM-5524 : Prometheus alerting rule changes needs docker restart to activate PMM-5949 : Unwanted filters applied when moving from QAN to Add Instance page PMM-5870 : MySQL Table Details dashboard not showing separate service names for tables PMM-5839 : PostgreSQL metrics disparity between query time and block read/write time PMM-5348 : Inventory page has inaccessible tabs that need reload to access PMM-5348 : Incorrect access control vulnerability fix (CVE-2020-13379) by upgrading Grafana to 6.7.4","title":"PMM 2.7.0"},{"location":"release-notes/2.7.0.html#percona-monitoring-and-management-270","text":"Date: June 9, 2020 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. In this release, we have updated Grafana to version 6.7.4 to fix CVE-2020-13379 . We recommend updating to the latest version of PMM as soon as possible.","title":"Percona Monitoring and Management 2.7.0"},{"location":"release-notes/2.7.0.html#new-features","text":"PMM-5257 , PMM-5256 , & PMM-5243 : pmm-admin socket option ( \u2013-socket ) to specify UNIX socket path for connecting to MongoDB, PostgreSQL, and ProxySQL instances","title":"New Features"},{"location":"release-notes/2.7.0.html#improvements","text":"PMM-2244 : pmm-admin status command output shows both pmm-admin and pmm-agent versions PMM-5968 : Disallow PMM Server node or agent removal via API PMM-5946 : MySQL Table Details dashboard filter on Service Name prevents display of services without data PMM-5926 : Expose PMM agent version in pmm-admin status command PMM-5891 : PMM Home page now includes News panel PMM-5906 : Independent update of PMM components deactivated","title":"Improvements"},{"location":"release-notes/2.7.0.html#bugs-fixed","text":"PMM-6004 : MySQL exporter reporting wrong values for cluster status ( wsrep_cluster_status ) PMM-4547 : MongoDB dashboard replication lag count incorrect PMM-5524 : Prometheus alerting rule changes needs docker restart to activate PMM-5949 : Unwanted filters applied when moving from QAN to Add Instance page PMM-5870 : MySQL Table Details dashboard not showing separate service names for tables PMM-5839 : PostgreSQL metrics disparity between query time and block read/write time PMM-5348 : Inventory page has inaccessible tabs that need reload to access PMM-5348 : Incorrect access control vulnerability fix (CVE-2020-13379) by upgrading Grafana to 6.7.4","title":"Bugs Fixed"},{"location":"release-notes/2.8.0.html","text":"Percona Monitoring and Management 2.8.0 \u00b6 Date: June 25, 2020 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. Improvements \u00b6 PMM-544 : Agents, Services and Nodes can now be removed via the \u2018PMM Inventory\u2019 page PMM-5706 : User-installed Grafana plugins unaffected by PMM upgrade Bugs Fixed \u00b6 PMM-6153 : PMM 2.7.0 inoperable when no Internet connectivity PMM-5365 : Client fails to send non-UTF-8 query analytics content to server (Thanks to user romulus for reporting this issue) PMM-5920 : Incorrect metric used in formula for \u201cTop Users by Rows Fetched/Read\u201d graph PMM-6084 : Annotations not showing consistently on dashboards PMM-6011 : No data in MongoDB Cluster summary, RocksDB & MMAPv1 details PMM-5987 : Incorrect total value for virtual memory utilization","title":"PMM 2.8.0"},{"location":"release-notes/2.8.0.html#percona-monitoring-and-management-280","text":"Date: June 25, 2020 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance.","title":"Percona Monitoring and Management 2.8.0"},{"location":"release-notes/2.8.0.html#improvements","text":"PMM-544 : Agents, Services and Nodes can now be removed via the \u2018PMM Inventory\u2019 page PMM-5706 : User-installed Grafana plugins unaffected by PMM upgrade","title":"Improvements"},{"location":"release-notes/2.8.0.html#bugs-fixed","text":"PMM-6153 : PMM 2.7.0 inoperable when no Internet connectivity PMM-5365 : Client fails to send non-UTF-8 query analytics content to server (Thanks to user romulus for reporting this issue) PMM-5920 : Incorrect metric used in formula for \u201cTop Users by Rows Fetched/Read\u201d graph PMM-6084 : Annotations not showing consistently on dashboards PMM-6011 : No data in MongoDB Cluster summary, RocksDB & MMAPv1 details PMM-5987 : Incorrect total value for virtual memory utilization","title":"Bugs Fixed"},{"location":"release-notes/2.9.0.html","text":"Percona Monitoring and Management 2.9.0 \u00b6 Date: July 14, 2020 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. Release Highlights \u00b6 This release brings a major rework of the Query Analytics (QAN) component, completing the migration from Angular to React, and adding new UI functionality and features. For details, see: PMM-5125 : Implement new version of QAN PMM-5516 : QAN migration to React and new UI implementation You can read more in the accompanying blog post ( here ). New Features \u00b6 PMM-6124 : New dashboards: MongoDB Replica Set Summary and MongoDB Cluster Summary PMM-1027 : New dashboard: MySQL User Details ( INFORMATION_SCHEMA.CLIENT_STATISTICS ) PMM-5604 : User interface for MongoDB EXPLAIN PMM-5563 : Per-Service and per-Node Annotations (This completes the work on improvements to the Annotation functionality.) Improvements \u00b6 PMM-6114 : Sort Agents, Nodes, and Services alphabetically by name in Inventory page (Thanks to user debug for reporting this issue) PMM-6147 : Update Grafana plugins to latest versions Bugs Fixed \u00b6 PMM-5800 : QAN explain and tables tabs not working after removing MySQL metrics agent PMM-5812 : Prometheus relabeling broken ( relabel_configs un-marshal errors) (Thanks to user b4bufr1k for reporting this issue) PMM-6184 : MongoDB Instances Compare dashboard shows MySQL metric PMM-5941 : Stacked Incoming/Outgoing Network Traffic graphs in MySQL Instances Overview dashboard prevents comparison PMM-6194 : Missing UID for Advanced Data Exploration dashboard PMM-6191 : Incorrect computation for Prometheus Process CPU Usage panel values in Prometheus dashboard PMM-6175 : Node Overview dashboard shows unit for unit-less value \u2018Top I/O Load\u2019","title":"PMM 2.9.0"},{"location":"release-notes/2.9.0.html#percona-monitoring-and-management-290","text":"Date: July 14, 2020 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance.","title":"Percona Monitoring and Management 2.9.0"},{"location":"release-notes/2.9.0.html#release-highlights","text":"This release brings a major rework of the Query Analytics (QAN) component, completing the migration from Angular to React, and adding new UI functionality and features. For details, see: PMM-5125 : Implement new version of QAN PMM-5516 : QAN migration to React and new UI implementation You can read more in the accompanying blog post ( here ).","title":"Release Highlights"},{"location":"release-notes/2.9.0.html#new-features","text":"PMM-6124 : New dashboards: MongoDB Replica Set Summary and MongoDB Cluster Summary PMM-1027 : New dashboard: MySQL User Details ( INFORMATION_SCHEMA.CLIENT_STATISTICS ) PMM-5604 : User interface for MongoDB EXPLAIN PMM-5563 : Per-Service and per-Node Annotations (This completes the work on improvements to the Annotation functionality.)","title":"New Features"},{"location":"release-notes/2.9.0.html#improvements","text":"PMM-6114 : Sort Agents, Nodes, and Services alphabetically by name in Inventory page (Thanks to user debug for reporting this issue) PMM-6147 : Update Grafana plugins to latest versions","title":"Improvements"},{"location":"release-notes/2.9.0.html#bugs-fixed","text":"PMM-5800 : QAN explain and tables tabs not working after removing MySQL metrics agent PMM-5812 : Prometheus relabeling broken ( relabel_configs un-marshal errors) (Thanks to user b4bufr1k for reporting this issue) PMM-6184 : MongoDB Instances Compare dashboard shows MySQL metric PMM-5941 : Stacked Incoming/Outgoing Network Traffic graphs in MySQL Instances Overview dashboard prevents comparison PMM-6194 : Missing UID for Advanced Data Exploration dashboard PMM-6191 : Incorrect computation for Prometheus Process CPU Usage panel values in Prometheus dashboard PMM-6175 : Node Overview dashboard shows unit for unit-less value \u2018Top I/O Load\u2019","title":"Bugs Fixed"},{"location":"release-notes/2.9.1.html","text":"Percona Monitoring and Management 2.9.1 \u00b6 Date: August 4, 2020 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. Improvements \u00b6 PMM-6230 : Custom dashboards set as Home remain so after update PMM-6300 : Query Analytics Dashboard: Column sorting arrows made easier to use (Thanks to user debug for reporting this issue) PMM-6208 : Security Threat Tool: Temporarily silence viewed but un-actioned alerts PMM-6315 : Query Analytics Dashboard: Improved metrics names and descriptions PMM-6274 : MySQL User Details Dashboard: View selected user\u2019s queries in Query Analytics Dashboard PMM-6266 : Query Analytics Dashboard: Pagination device menu lists 25, 50 or 100 items per page PMM-6262 : PostgreSQL Instance Summary Dashboard: Descriptions for all \u2018Temp Files\u2019 views PMM-6253 : Query Analytics Dashboard: Improved SQL formatting in Examples panel PMM-6211 : Query Analytics Dashboard: Loading activity spinner added to Example, Explain and Tables tabs PMM-6162 : Consistent sort order in dashboard drop-down filter lists PMM-5132 : Better message when filter search returns nothing Bugs Fixed \u00b6 PMM-5783 : Bulk failure of SHOW ALL SLAVES STATUS scraping on PS/MySQL distributions triggers errors PMM-6294 : Query Analytics Dashboard doesn\u2019t resize well for some screen resolutions (Thanks to user debug for reporting this issue) PMM-6420 : Wrong version in successful update pop-up window PMM-6319 : Query Analytics Dashboard: Query scrolls out of view when selected PMM-6302 : Query Analytics Dashboard: Unnecessary EXPLAIN requests PMM-6256 : Query Analytics Dashboard: InvalidNamespace EXPLAIN error with some MongoDB queries PMM-6329 : Query Analytics Dashboard: Unclear origin of sparkline tool-tip on mouse-over PMM-6259 : Query Analytics Dashboard: Slow appearance of query time distribution graph for some queries PMM-6189 : Disk Details Dashboard: Disk IO Size chart larger by factor of 512 PMM-6269 : Query Analytics Dashboard: Metrics drop-down list obscured when opened PMM-6247 : Query Analytics Dashboard: Overview table not resizing on window size change PMM-6227 : Home Dashboard redirection to Node Summary Dashboard not working","title":"PMM 2.9.1"},{"location":"release-notes/2.9.1.html#percona-monitoring-and-management-291","text":"Date: August 4, 2020 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance.","title":"Percona Monitoring and Management 2.9.1"},{"location":"release-notes/2.9.1.html#improvements","text":"PMM-6230 : Custom dashboards set as Home remain so after update PMM-6300 : Query Analytics Dashboard: Column sorting arrows made easier to use (Thanks to user debug for reporting this issue) PMM-6208 : Security Threat Tool: Temporarily silence viewed but un-actioned alerts PMM-6315 : Query Analytics Dashboard: Improved metrics names and descriptions PMM-6274 : MySQL User Details Dashboard: View selected user\u2019s queries in Query Analytics Dashboard PMM-6266 : Query Analytics Dashboard: Pagination device menu lists 25, 50 or 100 items per page PMM-6262 : PostgreSQL Instance Summary Dashboard: Descriptions for all \u2018Temp Files\u2019 views PMM-6253 : Query Analytics Dashboard: Improved SQL formatting in Examples panel PMM-6211 : Query Analytics Dashboard: Loading activity spinner added to Example, Explain and Tables tabs PMM-6162 : Consistent sort order in dashboard drop-down filter lists PMM-5132 : Better message when filter search returns nothing","title":"Improvements"},{"location":"release-notes/2.9.1.html#bugs-fixed","text":"PMM-5783 : Bulk failure of SHOW ALL SLAVES STATUS scraping on PS/MySQL distributions triggers errors PMM-6294 : Query Analytics Dashboard doesn\u2019t resize well for some screen resolutions (Thanks to user debug for reporting this issue) PMM-6420 : Wrong version in successful update pop-up window PMM-6319 : Query Analytics Dashboard: Query scrolls out of view when selected PMM-6302 : Query Analytics Dashboard: Unnecessary EXPLAIN requests PMM-6256 : Query Analytics Dashboard: InvalidNamespace EXPLAIN error with some MongoDB queries PMM-6329 : Query Analytics Dashboard: Unclear origin of sparkline tool-tip on mouse-over PMM-6259 : Query Analytics Dashboard: Slow appearance of query time distribution graph for some queries PMM-6189 : Disk Details Dashboard: Disk IO Size chart larger by factor of 512 PMM-6269 : Query Analytics Dashboard: Metrics drop-down list obscured when opened PMM-6247 : Query Analytics Dashboard: Overview table not resizing on window size change PMM-6227 : Home Dashboard redirection to Node Summary Dashboard not working","title":"Bugs Fixed"},{"location":"setting-up/index.html","text":"Setting up \u00b6 There are three stages to installing and setting up PMM. Summary Set up a PMM Server . Set up PMM Client(s) . Add services . Set up PMM Server \u00b6 Install and run at least one PMM Server. Choose from: Use Benefits Drawbacks Docker 1. Quick. 2. Simple. 1. Docker installation required. 2. Additional network configuration required. Podman 1. Quick. 2. Simple. 3. Rootless. 1. Podman installation required. Helm Technical Preview 1. Quick. 2. Simple. 3. Cloud. 1. Requires running Kubernetes cluster. Virtual appliance 1. Easily import into Hypervisor of your choice 1. More system resources compared to Docker footprint. Amazon AWS 1. Wizard-driven install. 1. Non-free solution (infrastructure costs). Set up PMM Client \u00b6 Install and run PMM Client on every node where there is a service you want to monitor. The choices: With Docker ; Natively, installed from: Linux package (installed with apt , apt-get , dnf , yum ); Binary package (a downloaded .tar.gz file). Binary is only way to install PMM client without root permissions Add services \u00b6 On each PMM Client, you configure then add to PMM Server\u2019s inventory the node or service you want to monitor. How you do this depends on the type of service. You can monitor: MySQL (and variants: Percona Server for MySQL, Percona XtraDB Cluster, MariaDB); MongoDB ; PostgreSQL ; ProxySQL ; Amazon RDS ; Microsoft Azure ; Google Cloud Platform (MySQL and PostgreSQL); Linux ; External services ; HAProxy ; Remote instances .","title":"Setting up"},{"location":"setting-up/index.html#setting-up","text":"There are three stages to installing and setting up PMM. Summary Set up a PMM Server . Set up PMM Client(s) . Add services .","title":"Setting up"},{"location":"setting-up/index.html#set-up-pmm-server","text":"Install and run at least one PMM Server. Choose from: Use Benefits Drawbacks Docker 1. Quick. 2. Simple. 1. Docker installation required. 2. Additional network configuration required. Podman 1. Quick. 2. Simple. 3. Rootless. 1. Podman installation required. Helm Technical Preview 1. Quick. 2. Simple. 3. Cloud. 1. Requires running Kubernetes cluster. Virtual appliance 1. Easily import into Hypervisor of your choice 1. More system resources compared to Docker footprint. Amazon AWS 1. Wizard-driven install. 1. Non-free solution (infrastructure costs).","title":"Set up PMM Server"},{"location":"setting-up/index.html#set-up-pmm-client","text":"Install and run PMM Client on every node where there is a service you want to monitor. The choices: With Docker ; Natively, installed from: Linux package (installed with apt , apt-get , dnf , yum ); Binary package (a downloaded .tar.gz file). Binary is only way to install PMM client without root permissions","title":"Set up PMM Client"},{"location":"setting-up/index.html#add-services","text":"On each PMM Client, you configure then add to PMM Server\u2019s inventory the node or service you want to monitor. How you do this depends on the type of service. You can monitor: MySQL (and variants: Percona Server for MySQL, Percona XtraDB Cluster, MariaDB); MongoDB ; PostgreSQL ; ProxySQL ; Amazon RDS ; Microsoft Azure ; Google Cloud Platform (MySQL and PostgreSQL); Linux ; External services ; HAProxy ; Remote instances .","title":"Add services"},{"location":"setting-up/client/index.html","text":"Set up PMM Client \u00b6 There are different ways to install PMM Client on a node and register it with PMM Server. Choose from: Docker : Run PMM Client as a Docker container. Package manager : On Debian or Red Hat Linux, install percona-release and use a Linux package manager ( apt / dnf ) to install PMM Client. On Debian or Red Hat, download .deb / .rpm PMM Client packages and manually install them. Binary is only way to install PMM client without root permissions Binary package : For other Linux distributions, download and unpack generic PMM Client Linux binaries. When you have installed PMM Client, you must: Register the node with PMM Server . Configure and add services according to type . If you need to, you can unregister , remove services or remove PMM Client . Here\u2019s an overview of the choices. Before you start \u00b6 Set up PMM Server with a known IP address accessible from the client node. You have superuser (root) access on the client host. You have superuser access to any database servers that you want to monitor. These Linux packages are installed: curl , gnupg , sudo , wget . If using it, install Docker . System requirements: Operating system \u2013 PMM Client runs on any modern 64-bit Linux distribution. It is tested on supported versions of Debian, Ubuntu, CentOS, and Red Hat Enterprise Linux. (See Percona software support life cycle ). Disk \u2013 A minimum of 100 MB of storage is required for installing the PMM Client package. With a good connection to PMM Server, additional storage is not required. However, the client needs to store any collected data that it cannot dispatch immediately, so additional storage may be required if the connection is unstable or the throughput is low. (Caching only applies to Query Analytics data; VictoriaMetrics data is never cached on the client side.) Install \u00b6 Docker \u00b6 The PMM Client Docker image is a convenient way to run PMM Client as a preconfigured Docker container. Pull the PMM Client docker image. docker pull \\ percona/pmm-client:2 Use the image as a template to create a persistent data store that preserves local data when the image is updated. docker create \\ --volume /srv \\ --name pmm-client-data \\ percona/pmm-client:2 /bin/true Run the container to start PMM Agent in setup mode. Set X.X.X.X to the IP address of your PMM Server. (Do not use the docker --detach option as PMM agent only logs to the console.) PMM_SERVER = X.X.X.X:443 docker run \\ --rm \\ --name pmm-client \\ -e PMM_AGENT_SERVER_ADDRESS = ${ PMM_SERVER } \\ -e PMM_AGENT_SERVER_USERNAME = admin \\ -e PMM_AGENT_SERVER_PASSWORD = admin \\ -e PMM_AGENT_SERVER_INSECURE_TLS = 1 \\ -e PMM_AGENT_SETUP = 1 \\ -e PMM_AGENT_CONFIG_FILE = config/pmm-agent.yaml \\ --volumes-from pmm-client-data \\ percona/pmm-client:2 Tips You can find a complete list of compatible environment variables here . Check status. docker exec pmm-client \\ pmm-admin status In the PMM user interface you will also see an increase in the number of monitored nodes. You can now add services with pmm-admin by prefixing commands with docker exec pmm-client . Tips Adjust host firewall and routing rules to allow Docker communications. ( Read more ) For help: docker run --rm percona/pmm-client:2 --help In the GUI. Select PMM Dashboards \u2192 System (Node) \u2192 Node Overview . In the Node Names menu, select the new node. Change the time range to see data. Danger pmm-agent.yaml contains sensitive credentials and should not be shared. Package manager \u00b6 Tip If you have used percona-release before, disable and re-enable the repository: percona-release disable all percona-release enable original release Debian-based \u00b6 Configure repositories. wget https://repo.percona.com/apt/percona-release_latest.generic_all.deb dpkg -i percona-release_latest.generic_all.deb Install the PMM Client package. Root permissions apt update apt install -y pmm2-client Check. pmm-admin --version Register the node . Red Hat-based \u00b6 Configure repositories. yum install -y https://repo.percona.com/yum/percona-release-latest.noarch.rpm Install the PMM Client package. yum install -y pmm2-client Check. pmm-admin --version Register the node . Package manager \u2013 manual download \u00b6 Visit the Percona Monitoring and Management 2 download page. Under Version: , select the one you want (usually the latest). Under Software: , select the item matching your software platform. Click to download the package file: For Debian, Ubuntu: .deb For Red Hat, CentOS, Oracle Linux: .rpm (Alternatively, copy the link and use wget to download it.) Here are the download page links for each supported platform. Debian 9 (Stretch) Debian 10 (Buster) Debian 11 (Bullseye) Red Hat/CentOS/Oracle 7 Red Hat/CentOS/Oracle 8 Ubuntu 18.04 (Bionic Beaver) Ubuntu 20.04 (Focal Fossa) Ubuntu 22.04 (Jammy Jellyfish) Debian-based \u00b6 dpkg -i *.deb Red Hat-based \u00b6 dnf localinstall *.rpm Binary package \u00b6 Download the PMM Client package: wget https://downloads.percona.com/downloads/pmm2/2.31.0/binary/tarball/pmm2-client-2.31.0.tar.gz Download the PMM Client package checksum file: wget https://downloads.percona.com/downloads/pmm2/2.31.0/binary/tarball/pmm2-client-2.31.0.tar.gz.sha256sum Verify the download. sha256sum -c pmm2-client-2.31.0.tar.gz.sha256sum Unpack the package and move into the directory. tar xfz pmm2-client-2.31.0.tar.gz && cd pmm2-client-2.31.0 Choose one of these two commands (depends on your permissions): Without root permissions export PMM_DIR = YOURPATH where YOURPATH replace with you real path, where you have required access. With root permissions export PMM_DIR = /usr/local/percona/pmm2 Run the installer. Root permissions (if you skipped step 5 for non root users) ./install_tarball Change the path. PATH = $PATH : $PMM_DIR /bin Set up the agent (pick the command for you depending on permissions) Root permissions pmm-agent setup --config-file = /usr/local/percona/pmm2/config/pmm-agent.yaml --server-address = 192 .168.1.123 --server-insecure-tls --server-username = admin --server-password = admin Non root users pmm-agent setup --config-file = ${ PMM_DIR } /config/pmm-agent.yaml --server-address = 192 .168.1.123 --server-insecure-tls --server-username = admin --server-password = admin --paths-tempdir = ${ PMM_DIR } /tmp --paths-base = ${ PMM_DIR } Run the agent. pmm-agent --config-file = ${ PMM_DIR } /config/pmm-agent.yaml Open a new terminal and check. pmm-admin status !!! hint PMM-Agent can be updated from tarball: Download tar.gz with pmm2-client. Extract it. Run ./install_tarball script with the \u201c-u\u201d flag. The configuration file will be overwritten if you do not provide the \u201c-u\u201d flag while the pmm-agent is updated. Register \u00b6 Register your client node with PMM Server. pmm-admin config --server-insecure-tls --server-url = https://admin:admin@X.X.X.X:443 X.X.X.X is the address of your PMM Server. 443 is the default port number. admin / admin is the default PMM username and password. This is the same account you use to log into the PMM user interface, which you had the option to change when first logging in. Important Clients must be registered with the PMM Server using a secure channel. If you use http as your server URL, PMM will try to connect via https on port 443. If a TLS connection can\u2019t be established you will get an error and you must use https along with the appropriate secure port. Examples \u00b6 Register on PMM Server with IP address 192.168.33.14 using the default admin/admin username and password, a node with IP address 192.168.33.23 , type generic , and name mynode . pmm-admin config --server-insecure-tls --server-url = https://admin:admin@192.168.33.14:443 192 .168.33.23 generic mynode Add services \u00b6 You must configure and adding services according to the service type. MySQL (and variants Percona Server for MySQL, Percona XtraDB Cluster, MariaDB) MongoDB PostgreSQL ProxySQL Amazon RDS Microsoft Azure Google Cloud Platform (MySQL and PostgreSQL) Linux External services HAProxy Remote instances Tip To change the parameters of a previously-added service, remove the service and re-add it with new parameters. Remove \u00b6 How to remove (uninstall) PMM Client. Docker \u00b6 Caution These steps delete the PMM Client Docker image and client services configuration data. Stop pmm-client container. docker stop pmm-client Remove containers. docker rm pmm-client Remove the image. docker rmi $( docker images | grep \"percona/pmm-client\" | awk { 'print $3' } ) Remove the volume. docker volume rm pmm-client-data Package manager \u00b6 Debian-based distributions \u00b6 Uninstall the PMM Client package. apt remove -y pmm2-client Remove the Percona repository dpkg -r percona-release Red Hat-based distributions \u00b6 Uninstall the PMM Client package. yum remove -y pmm2-client Remove the Percona repository yum remove -y percona-release Unregister \u00b6 How to unregister PMM Client from PMM Server. pmm-admin unregister --force All services monitored by this node will be removed from monitoring. Remove services \u00b6 You must specify the service type and service name to remove services from monitoring. pmm-admin remove <service-type> <service-name> service-type One of mysql , mongodb , postgresql , proxysql , haproxy , external . See also Percona release PMM Client architecture","title":"Client"},{"location":"setting-up/client/index.html#set-up-pmm-client","text":"There are different ways to install PMM Client on a node and register it with PMM Server. Choose from: Docker : Run PMM Client as a Docker container. Package manager : On Debian or Red Hat Linux, install percona-release and use a Linux package manager ( apt / dnf ) to install PMM Client. On Debian or Red Hat, download .deb / .rpm PMM Client packages and manually install them. Binary is only way to install PMM client without root permissions Binary package : For other Linux distributions, download and unpack generic PMM Client Linux binaries. When you have installed PMM Client, you must: Register the node with PMM Server . Configure and add services according to type . If you need to, you can unregister , remove services or remove PMM Client . Here\u2019s an overview of the choices.","title":"Set up PMM Client"},{"location":"setting-up/client/index.html#before-you-start","text":"Set up PMM Server with a known IP address accessible from the client node. You have superuser (root) access on the client host. You have superuser access to any database servers that you want to monitor. These Linux packages are installed: curl , gnupg , sudo , wget . If using it, install Docker . System requirements: Operating system \u2013 PMM Client runs on any modern 64-bit Linux distribution. It is tested on supported versions of Debian, Ubuntu, CentOS, and Red Hat Enterprise Linux. (See Percona software support life cycle ). Disk \u2013 A minimum of 100 MB of storage is required for installing the PMM Client package. With a good connection to PMM Server, additional storage is not required. However, the client needs to store any collected data that it cannot dispatch immediately, so additional storage may be required if the connection is unstable or the throughput is low. (Caching only applies to Query Analytics data; VictoriaMetrics data is never cached on the client side.)","title":"Before you start"},{"location":"setting-up/client/index.html#install","text":"","title":"Install"},{"location":"setting-up/client/index.html#docker","text":"The PMM Client Docker image is a convenient way to run PMM Client as a preconfigured Docker container. Pull the PMM Client docker image. docker pull \\ percona/pmm-client:2 Use the image as a template to create a persistent data store that preserves local data when the image is updated. docker create \\ --volume /srv \\ --name pmm-client-data \\ percona/pmm-client:2 /bin/true Run the container to start PMM Agent in setup mode. Set X.X.X.X to the IP address of your PMM Server. (Do not use the docker --detach option as PMM agent only logs to the console.) PMM_SERVER = X.X.X.X:443 docker run \\ --rm \\ --name pmm-client \\ -e PMM_AGENT_SERVER_ADDRESS = ${ PMM_SERVER } \\ -e PMM_AGENT_SERVER_USERNAME = admin \\ -e PMM_AGENT_SERVER_PASSWORD = admin \\ -e PMM_AGENT_SERVER_INSECURE_TLS = 1 \\ -e PMM_AGENT_SETUP = 1 \\ -e PMM_AGENT_CONFIG_FILE = config/pmm-agent.yaml \\ --volumes-from pmm-client-data \\ percona/pmm-client:2 Tips You can find a complete list of compatible environment variables here . Check status. docker exec pmm-client \\ pmm-admin status In the PMM user interface you will also see an increase in the number of monitored nodes. You can now add services with pmm-admin by prefixing commands with docker exec pmm-client . Tips Adjust host firewall and routing rules to allow Docker communications. ( Read more ) For help: docker run --rm percona/pmm-client:2 --help In the GUI. Select PMM Dashboards \u2192 System (Node) \u2192 Node Overview . In the Node Names menu, select the new node. Change the time range to see data. Danger pmm-agent.yaml contains sensitive credentials and should not be shared.","title":"Docker"},{"location":"setting-up/client/index.html#package-manager","text":"Tip If you have used percona-release before, disable and re-enable the repository: percona-release disable all percona-release enable original release","title":"Package manager"},{"location":"setting-up/client/index.html#debian-based","text":"Configure repositories. wget https://repo.percona.com/apt/percona-release_latest.generic_all.deb dpkg -i percona-release_latest.generic_all.deb Install the PMM Client package. Root permissions apt update apt install -y pmm2-client Check. pmm-admin --version Register the node .","title":"Debian-based"},{"location":"setting-up/client/index.html#red-hat-based","text":"Configure repositories. yum install -y https://repo.percona.com/yum/percona-release-latest.noarch.rpm Install the PMM Client package. yum install -y pmm2-client Check. pmm-admin --version Register the node .","title":"Red Hat-based"},{"location":"setting-up/client/index.html#package-manager-manual-download","text":"Visit the Percona Monitoring and Management 2 download page. Under Version: , select the one you want (usually the latest). Under Software: , select the item matching your software platform. Click to download the package file: For Debian, Ubuntu: .deb For Red Hat, CentOS, Oracle Linux: .rpm (Alternatively, copy the link and use wget to download it.) Here are the download page links for each supported platform. Debian 9 (Stretch) Debian 10 (Buster) Debian 11 (Bullseye) Red Hat/CentOS/Oracle 7 Red Hat/CentOS/Oracle 8 Ubuntu 18.04 (Bionic Beaver) Ubuntu 20.04 (Focal Fossa) Ubuntu 22.04 (Jammy Jellyfish)","title":"Package manager -- manual download"},{"location":"setting-up/client/index.html#debian-based_1","text":"dpkg -i *.deb","title":"Debian-based"},{"location":"setting-up/client/index.html#red-hat-based_1","text":"dnf localinstall *.rpm","title":"Red Hat-based"},{"location":"setting-up/client/index.html#binary-package","text":"Download the PMM Client package: wget https://downloads.percona.com/downloads/pmm2/2.31.0/binary/tarball/pmm2-client-2.31.0.tar.gz Download the PMM Client package checksum file: wget https://downloads.percona.com/downloads/pmm2/2.31.0/binary/tarball/pmm2-client-2.31.0.tar.gz.sha256sum Verify the download. sha256sum -c pmm2-client-2.31.0.tar.gz.sha256sum Unpack the package and move into the directory. tar xfz pmm2-client-2.31.0.tar.gz && cd pmm2-client-2.31.0 Choose one of these two commands (depends on your permissions): Without root permissions export PMM_DIR = YOURPATH where YOURPATH replace with you real path, where you have required access. With root permissions export PMM_DIR = /usr/local/percona/pmm2 Run the installer. Root permissions (if you skipped step 5 for non root users) ./install_tarball Change the path. PATH = $PATH : $PMM_DIR /bin Set up the agent (pick the command for you depending on permissions) Root permissions pmm-agent setup --config-file = /usr/local/percona/pmm2/config/pmm-agent.yaml --server-address = 192 .168.1.123 --server-insecure-tls --server-username = admin --server-password = admin Non root users pmm-agent setup --config-file = ${ PMM_DIR } /config/pmm-agent.yaml --server-address = 192 .168.1.123 --server-insecure-tls --server-username = admin --server-password = admin --paths-tempdir = ${ PMM_DIR } /tmp --paths-base = ${ PMM_DIR } Run the agent. pmm-agent --config-file = ${ PMM_DIR } /config/pmm-agent.yaml Open a new terminal and check. pmm-admin status !!! hint PMM-Agent can be updated from tarball: Download tar.gz with pmm2-client. Extract it. Run ./install_tarball script with the \u201c-u\u201d flag. The configuration file will be overwritten if you do not provide the \u201c-u\u201d flag while the pmm-agent is updated.","title":"Binary package"},{"location":"setting-up/client/index.html#register","text":"Register your client node with PMM Server. pmm-admin config --server-insecure-tls --server-url = https://admin:admin@X.X.X.X:443 X.X.X.X is the address of your PMM Server. 443 is the default port number. admin / admin is the default PMM username and password. This is the same account you use to log into the PMM user interface, which you had the option to change when first logging in. Important Clients must be registered with the PMM Server using a secure channel. If you use http as your server URL, PMM will try to connect via https on port 443. If a TLS connection can\u2019t be established you will get an error and you must use https along with the appropriate secure port.","title":"Register"},{"location":"setting-up/client/index.html#examples","text":"Register on PMM Server with IP address 192.168.33.14 using the default admin/admin username and password, a node with IP address 192.168.33.23 , type generic , and name mynode . pmm-admin config --server-insecure-tls --server-url = https://admin:admin@192.168.33.14:443 192 .168.33.23 generic mynode","title":"Examples"},{"location":"setting-up/client/index.html#add-services","text":"You must configure and adding services according to the service type. MySQL (and variants Percona Server for MySQL, Percona XtraDB Cluster, MariaDB) MongoDB PostgreSQL ProxySQL Amazon RDS Microsoft Azure Google Cloud Platform (MySQL and PostgreSQL) Linux External services HAProxy Remote instances Tip To change the parameters of a previously-added service, remove the service and re-add it with new parameters.","title":"Add services"},{"location":"setting-up/client/index.html#remove","text":"How to remove (uninstall) PMM Client.","title":"Remove"},{"location":"setting-up/client/index.html#docker_1","text":"Caution These steps delete the PMM Client Docker image and client services configuration data. Stop pmm-client container. docker stop pmm-client Remove containers. docker rm pmm-client Remove the image. docker rmi $( docker images | grep \"percona/pmm-client\" | awk { 'print $3' } ) Remove the volume. docker volume rm pmm-client-data","title":"Docker"},{"location":"setting-up/client/index.html#package-manager_1","text":"","title":"Package manager"},{"location":"setting-up/client/index.html#debian-based-distributions","text":"Uninstall the PMM Client package. apt remove -y pmm2-client Remove the Percona repository dpkg -r percona-release","title":"Debian-based distributions"},{"location":"setting-up/client/index.html#red-hat-based-distributions","text":"Uninstall the PMM Client package. yum remove -y pmm2-client Remove the Percona repository yum remove -y percona-release","title":"Red Hat-based distributions"},{"location":"setting-up/client/index.html#unregister","text":"How to unregister PMM Client from PMM Server. pmm-admin unregister --force All services monitored by this node will be removed from monitoring.","title":"Unregister"},{"location":"setting-up/client/index.html#remove-services","text":"You must specify the service type and service name to remove services from monitoring. pmm-admin remove <service-type> <service-name> service-type One of mysql , mongodb , postgresql , proxysql , haproxy , external . See also Percona release PMM Client architecture","title":"Remove services"},{"location":"setting-up/client/aws.html","text":"Amazon RDS \u00b6 Required settings \u00b6 It is possible to use PMM for monitoring Amazon RDS . In this case, the PMM Client is not installed on the host where the database server is deployed. By using the PMM web interface, you connect to the Amazon RDS DB instance. You only need to provide the IAM user access key (or assign an IAM role) and PMM discovers the Amazon RDS DB instances available for monitoring. First of all, ensure that there is the minimal latency between PMM Server and the Amazon RDS instance. Network connectivity can become an issue for VictoriaMetrics to scrape metrics with 1 second resolution. We strongly suggest that you run PMM Server on AWS (Amazon Web Services) in the same availability zone as Amazon RDS instances. It is crucial that enhanced monitoring be enabled for the Amazon RDS DB instances you intend to monitor. Set the Enable Enhanced Monitoring option in the settings of your Amazon RDS DB instance. Creating an IAM user with permission to access Amazon RDS DB instances \u00b6 It is recommended that you use an IAM user account to access Amazon RDS DB instances instead of using your AWS account. This measure improves security as the permissions of an IAM user account can be limited so that this account only grants access to your Amazon RDS DB instances. On the other hand, you use your AWS account to access all AWS services. The procedure for creating IAM user accounts is well described in the Amazon RDS documentation. This section only goes through the essential steps and points out the steps required for using Amazon RDS with Percona Monitoring and Management. The first step is to define a policy which will hold all the necessary permissions. Then, you need to associate this policy with the IAM user or group. In this section, we will create a new user for this purpose. Creating a policy \u00b6 A policy defines how AWS services can be accessed. Once defined it can be associated with an existing user or group. To define a new policy use the IAM page at AWS. Select the Policies option on the navigation panel and click the Create policy button. On the Create policy page, select the JSON tab and replace the existing contents with the following JSON document. { \"Version\" : \"2012-10-17\" , \"Statement\" : [{ \"Sid\" : \"Stmt1508404837000\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"rds:DescribeDBInstances\" , \"cloudwatch:GetMetricStatistics\" , \"cloudwatch:ListMetrics\" ], \"Resource\" : [ \"*\" ] }, { \"Sid\" : \"Stmt1508410723001\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"logs:DescribeLogStreams\" , \"logs:GetLogEvents\" , \"logs:FilterLogEvents\" ], \"Resource\" : [ \"arn:aws:logs:*:*:log-group:RDSOSMetrics:*\" ]} ] } Click Review policy and set a name to your policy, such as AmazonRDSforPMMPolicy . Then, click the Create policy button. Creating an IAM user \u00b6 Policies are attached to existing IAM users or groups. To create a new IAM user, select Users on the Identity and Access Management page at AWS. Then click Add user and complete the following steps: On the Add user page, set the user name and select the Programmatic access option under Select AWS access type . Set a custom password and then proceed to permissions by clicking the Permissions button. On the Set permissions page, add the new user to one or more groups if necessary. Then, click Review . On the Add user page, click Create user . Creating an access key for an IAM user \u00b6 To discover an Amazon RDS DB instance in PMM, you either need to use the access key and secret access key of an existing IAM user or an IAM role. To create an access key for use with PMM, open the IAM console and click Users on the navigation pane. Then, select your IAM user. To create the access key, open the Security credentials tab and click the Create access key button. The system automatically generates a new access key ID and a secret access key that you can provide on the PMM Add Instance dashboard to have your Amazon RDS DB instances discovered. In case, the PMM Server and Amazon RDS DB instance were created by using the same AWS account, you do not need create the access key ID and secret access key manually. PMM retrieves this information automatically and attempts to discover your Amazon RDS DB instances. Attaching a policy to an IAM user \u00b6 The last step before you are ready to create an Amazon RDS DB instance is to attach the policy with the required permissions to the IAM user. First, make sure that the Identity and Access Management page is open and open Users . Then, locate and open the IAM user that you plan to use with Amazon RDS DB instances. Complete the following steps, to apply the policy: On the Permissions tab, click the Add permissions button. On the Add permissions page, click Attach existing policies directly . Using the Filter , locate the policy with the required permissions (such as AmazonRDSforPMMPolicy ). Select a check-box next to the name of the policy and click Review . The selected policy appears on the Permissions summary page. Click Add permissions . The AmazonRDSforPMMPolicy is now added to your IAM user. Creating an IAM role \u00b6 Instead of creating an IAM user you can create an IAM role for a service, to discover Amazon RDS DB instances automatically without the need for access and secret keys. (But this only works if you are running PMM through AWS.) To create an IAM role open the IAM console and click Roles on the navigation pane. Click the Create role button. Select AWS service and select EC2 for the use case. Click the Next: Permissions button. Find the policy created previously and select it. Click the Next: Tags button. (Optional) Add a metadata tag to the role. Click the Next: Review button. Fill the role name and description. Click the Create role button After the role is created EC2 instances running PMM will have permissions to discover RDS DB instances. It\u2019s also possible to create an IAM role to delegate permissions to an IAM user or to add permissions to a user belonging to another AWS account. See the official AWS documentation on creating IAM roles . Setting up the Amazon RDS DB Instance \u00b6 Query Analytics requires Configuring Performance Schema as the query source, because the slow query log is stored on the AWS (Amazon Web Services) side, and QAN agent is not able to read it. Enable the performance_schema option under Parameter Groups in Amazon RDS. Important Enabling Performance Schema on T2 instances is not recommended because it can easily run the T2 instance out of memory. When adding a monitoring instance for Amazon RDS, specify a unique name to distinguish it from the local instance. If you do not specify a name, it will use the client\u2019s host name. Create the pmm user with the following privileges on the Amazon RDS instance that you want to monitor: CREATE USER 'pmm' @ '%' IDENTIFIED BY 'pass' ; GRANT SELECT , PROCESS , REPLICATION CLIENT ON * . * TO 'pmm' @ '%' ; ALTER USER 'pmm' @ '%' WITH MAX_USER_CONNECTIONS 10 ; GRANT SELECT , UPDATE , DELETE , DROP ON performance_schema . * TO 'pmm' @ '%' ; Adding an Amazon RDS, Aurora or Remote Instance \u00b6 The preferred method of adding an Amazon RDS database instance to PMM is via the Configuration \u2192 PMM Inventory \u2192 Add Instance menu option. This method supports Amazon RDS database instances that use Amazon Aurora, MySQL, or MariaDB engines, as well as any remote PostgreSQL, ProxySQL, MySQL and MongoDB instances. The following steps are needed to add an Amazon RDS database instance to PMM: In the PMM web interface, go to Configuration \u2192 PMM Inventory \u2192 Add Instance . Select Amazon RDS \u2013 Add a remote instance . Enter the access key ID and the secret access key of your IAM user or leave these fields empty if an IAM role was created. Click the Discover button for PMM to retrieve the available Amazon RDS instances. For the instance that you would like to monitor, select the Start monitoring button. You will see a new page with the number of fields. The list is divided into the following groups: Main details , RDS database , Labels , and Additional options . Some already known data, such as already entered AWS access key , are filled in automatically, and some fields are optional. The Main details section allows you to specify the DNS hostname of your instance, the service name to use within PMM, the port your service is listening on, and the database user name and password. The Labels section allows you to specify labels for the environment, the AWS region and availability zone to be used, the Replication set and Cluster names and also it allows you to set the list of custom labels in a key:value format. The Additional options section contains specific flags which allow you to tune the RDS monitoring. They can allow you to skip connection check, to use TLS for the database connection, not to validate the TLS certificate and the hostname, as well as to disable basic and/or enhanced metrics collection for the RDS instance to reduce costs. Also this section contains a database-specific flag, which would allow Query Analytics for the selected remote database: when adding some remote MySQL, AWS RDS MySQL or Aurora MySQL instance, you will be able to choose using performance schema for the database monitoring; when adding a PostgreSQL instance, you will be able to activate using pg_stat_statements extension; when adding a MongoDB instance, you will be able to choose using Query Analytics MongoDB profiler. Finally press the Add service button to start monitoring your instance. Adding an Amazon RDS PostgreSQL instance \u00b6 For PostgreSQL, use the same method described above. In the PMM web interface, go to Configuration \u2192 PMM Inventory \u2192 Add Instance .. Select Amazon RDS \u2013 Add a remote instance . At the moment of writing this guide, the Add button doesn\u2019t mention PostgreSQL but the discovery function already supports it. Follow steps 4 to 6 as in the previous section. Fill the form and remember to select PG Stat Statement to enable Query Analytics. To get queries for Query Analytics, you need to enable pg_stat_statements in postgres database of your instance by running: CREATE EXTENSION pg_stat_statements SCHEMA public ;","title":"Amazon RDS"},{"location":"setting-up/client/aws.html#amazon-rds","text":"","title":"Amazon RDS"},{"location":"setting-up/client/aws.html#required-settings","text":"It is possible to use PMM for monitoring Amazon RDS . In this case, the PMM Client is not installed on the host where the database server is deployed. By using the PMM web interface, you connect to the Amazon RDS DB instance. You only need to provide the IAM user access key (or assign an IAM role) and PMM discovers the Amazon RDS DB instances available for monitoring. First of all, ensure that there is the minimal latency between PMM Server and the Amazon RDS instance. Network connectivity can become an issue for VictoriaMetrics to scrape metrics with 1 second resolution. We strongly suggest that you run PMM Server on AWS (Amazon Web Services) in the same availability zone as Amazon RDS instances. It is crucial that enhanced monitoring be enabled for the Amazon RDS DB instances you intend to monitor. Set the Enable Enhanced Monitoring option in the settings of your Amazon RDS DB instance.","title":"Required settings"},{"location":"setting-up/client/aws.html#creating-an-iam-user-with-permission-to-access-amazon-rds-db-instances","text":"It is recommended that you use an IAM user account to access Amazon RDS DB instances instead of using your AWS account. This measure improves security as the permissions of an IAM user account can be limited so that this account only grants access to your Amazon RDS DB instances. On the other hand, you use your AWS account to access all AWS services. The procedure for creating IAM user accounts is well described in the Amazon RDS documentation. This section only goes through the essential steps and points out the steps required for using Amazon RDS with Percona Monitoring and Management. The first step is to define a policy which will hold all the necessary permissions. Then, you need to associate this policy with the IAM user or group. In this section, we will create a new user for this purpose.","title":"Creating an IAM user with permission to access Amazon RDS DB instances"},{"location":"setting-up/client/aws.html#creating-a-policy","text":"A policy defines how AWS services can be accessed. Once defined it can be associated with an existing user or group. To define a new policy use the IAM page at AWS. Select the Policies option on the navigation panel and click the Create policy button. On the Create policy page, select the JSON tab and replace the existing contents with the following JSON document. { \"Version\" : \"2012-10-17\" , \"Statement\" : [{ \"Sid\" : \"Stmt1508404837000\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"rds:DescribeDBInstances\" , \"cloudwatch:GetMetricStatistics\" , \"cloudwatch:ListMetrics\" ], \"Resource\" : [ \"*\" ] }, { \"Sid\" : \"Stmt1508410723001\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"logs:DescribeLogStreams\" , \"logs:GetLogEvents\" , \"logs:FilterLogEvents\" ], \"Resource\" : [ \"arn:aws:logs:*:*:log-group:RDSOSMetrics:*\" ]} ] } Click Review policy and set a name to your policy, such as AmazonRDSforPMMPolicy . Then, click the Create policy button.","title":"Creating a policy"},{"location":"setting-up/client/aws.html#creating-an-iam-user","text":"Policies are attached to existing IAM users or groups. To create a new IAM user, select Users on the Identity and Access Management page at AWS. Then click Add user and complete the following steps: On the Add user page, set the user name and select the Programmatic access option under Select AWS access type . Set a custom password and then proceed to permissions by clicking the Permissions button. On the Set permissions page, add the new user to one or more groups if necessary. Then, click Review . On the Add user page, click Create user .","title":"Creating an IAM user"},{"location":"setting-up/client/aws.html#creating-an-access-key-for-an-iam-user","text":"To discover an Amazon RDS DB instance in PMM, you either need to use the access key and secret access key of an existing IAM user or an IAM role. To create an access key for use with PMM, open the IAM console and click Users on the navigation pane. Then, select your IAM user. To create the access key, open the Security credentials tab and click the Create access key button. The system automatically generates a new access key ID and a secret access key that you can provide on the PMM Add Instance dashboard to have your Amazon RDS DB instances discovered. In case, the PMM Server and Amazon RDS DB instance were created by using the same AWS account, you do not need create the access key ID and secret access key manually. PMM retrieves this information automatically and attempts to discover your Amazon RDS DB instances.","title":"Creating an access key for an IAM user"},{"location":"setting-up/client/aws.html#attaching-a-policy-to-an-iam-user","text":"The last step before you are ready to create an Amazon RDS DB instance is to attach the policy with the required permissions to the IAM user. First, make sure that the Identity and Access Management page is open and open Users . Then, locate and open the IAM user that you plan to use with Amazon RDS DB instances. Complete the following steps, to apply the policy: On the Permissions tab, click the Add permissions button. On the Add permissions page, click Attach existing policies directly . Using the Filter , locate the policy with the required permissions (such as AmazonRDSforPMMPolicy ). Select a check-box next to the name of the policy and click Review . The selected policy appears on the Permissions summary page. Click Add permissions . The AmazonRDSforPMMPolicy is now added to your IAM user.","title":"Attaching a policy to an IAM user"},{"location":"setting-up/client/aws.html#creating-an-iam-role","text":"Instead of creating an IAM user you can create an IAM role for a service, to discover Amazon RDS DB instances automatically without the need for access and secret keys. (But this only works if you are running PMM through AWS.) To create an IAM role open the IAM console and click Roles on the navigation pane. Click the Create role button. Select AWS service and select EC2 for the use case. Click the Next: Permissions button. Find the policy created previously and select it. Click the Next: Tags button. (Optional) Add a metadata tag to the role. Click the Next: Review button. Fill the role name and description. Click the Create role button After the role is created EC2 instances running PMM will have permissions to discover RDS DB instances. It\u2019s also possible to create an IAM role to delegate permissions to an IAM user or to add permissions to a user belonging to another AWS account. See the official AWS documentation on creating IAM roles .","title":"Creating an IAM role"},{"location":"setting-up/client/aws.html#setting-up-the-amazon-rds-db-instance","text":"Query Analytics requires Configuring Performance Schema as the query source, because the slow query log is stored on the AWS (Amazon Web Services) side, and QAN agent is not able to read it. Enable the performance_schema option under Parameter Groups in Amazon RDS. Important Enabling Performance Schema on T2 instances is not recommended because it can easily run the T2 instance out of memory. When adding a monitoring instance for Amazon RDS, specify a unique name to distinguish it from the local instance. If you do not specify a name, it will use the client\u2019s host name. Create the pmm user with the following privileges on the Amazon RDS instance that you want to monitor: CREATE USER 'pmm' @ '%' IDENTIFIED BY 'pass' ; GRANT SELECT , PROCESS , REPLICATION CLIENT ON * . * TO 'pmm' @ '%' ; ALTER USER 'pmm' @ '%' WITH MAX_USER_CONNECTIONS 10 ; GRANT SELECT , UPDATE , DELETE , DROP ON performance_schema . * TO 'pmm' @ '%' ;","title":"Setting up the Amazon RDS DB Instance"},{"location":"setting-up/client/aws.html#adding-an-amazon-rds-aurora-or-remote-instance","text":"The preferred method of adding an Amazon RDS database instance to PMM is via the Configuration \u2192 PMM Inventory \u2192 Add Instance menu option. This method supports Amazon RDS database instances that use Amazon Aurora, MySQL, or MariaDB engines, as well as any remote PostgreSQL, ProxySQL, MySQL and MongoDB instances. The following steps are needed to add an Amazon RDS database instance to PMM: In the PMM web interface, go to Configuration \u2192 PMM Inventory \u2192 Add Instance . Select Amazon RDS \u2013 Add a remote instance . Enter the access key ID and the secret access key of your IAM user or leave these fields empty if an IAM role was created. Click the Discover button for PMM to retrieve the available Amazon RDS instances. For the instance that you would like to monitor, select the Start monitoring button. You will see a new page with the number of fields. The list is divided into the following groups: Main details , RDS database , Labels , and Additional options . Some already known data, such as already entered AWS access key , are filled in automatically, and some fields are optional. The Main details section allows you to specify the DNS hostname of your instance, the service name to use within PMM, the port your service is listening on, and the database user name and password. The Labels section allows you to specify labels for the environment, the AWS region and availability zone to be used, the Replication set and Cluster names and also it allows you to set the list of custom labels in a key:value format. The Additional options section contains specific flags which allow you to tune the RDS monitoring. They can allow you to skip connection check, to use TLS for the database connection, not to validate the TLS certificate and the hostname, as well as to disable basic and/or enhanced metrics collection for the RDS instance to reduce costs. Also this section contains a database-specific flag, which would allow Query Analytics for the selected remote database: when adding some remote MySQL, AWS RDS MySQL or Aurora MySQL instance, you will be able to choose using performance schema for the database monitoring; when adding a PostgreSQL instance, you will be able to activate using pg_stat_statements extension; when adding a MongoDB instance, you will be able to choose using Query Analytics MongoDB profiler. Finally press the Add service button to start monitoring your instance.","title":"Adding an Amazon RDS, Aurora or Remote Instance"},{"location":"setting-up/client/aws.html#adding-an-amazon-rds-postgresql-instance","text":"For PostgreSQL, use the same method described above. In the PMM web interface, go to Configuration \u2192 PMM Inventory \u2192 Add Instance .. Select Amazon RDS \u2013 Add a remote instance . At the moment of writing this guide, the Add button doesn\u2019t mention PostgreSQL but the discovery function already supports it. Follow steps 4 to 6 as in the previous section. Fill the form and remember to select PG Stat Statement to enable Query Analytics. To get queries for Query Analytics, you need to enable pg_stat_statements in postgres database of your instance by running: CREATE EXTENSION pg_stat_statements SCHEMA public ;","title":"Adding an Amazon RDS PostgreSQL instance"},{"location":"setting-up/client/azure.html","text":"Microsoft Azure \u00b6 Caution Microsoft Azure functionality is currently in technical preview and is subject to change. Activate Microsoft Azure \u00b6 The Microsoft Azure feature is turned off by default. To turn it on: Go to Configuration \u2192 Settings \u2192 Advanced Settings . Click the toggle in the Technical preview features section of the page. Required settings \u00b6 It is possible to use PMM for monitoring Azure database instances like other remote instances. In this case, the PMM Client is not installed on the host where the database server is deployed. By using the PMM web interface, you connect to the Azure DB instance. Discovery is not yet implemented in PMM but it is possible to add known instances by providing the connection parameters. First of all, ensure that there is the minimal latency between PMM Server and the Azure instance. Second, add a firewall rule to enable access from PMM Client like this: Setting up a MySQL instance \u00b6 Query Analytics requires you to configure Performance Schema as the query source, because the slow query log is stored on the Azure side, and QAN agent is not able to read it. Enable the performance_schema option under Parameter Groups in Azure MySQL databases. When adding a monitoring instance for Azure, specify a unique name to distinguish it from the local MySQL instance. If you do not specify a name, it will use the client\u2019s host name. Create the pmm user with the following privileges on the Azure MySQL database instance that you want to monitor: CREATE USER 'pmm' @ '%' IDENTIFIED BY 'pass' ; GRANT SELECT , PROCESS , REPLICATION CLIENT ON * . * TO 'pmm' @ '%' ; ALTER USER 'pmm' @ '%' WITH MAX_USER_CONNECTIONS 10 ; Adding an Azure Instance \u00b6 Follow the instructions for remotes instances explained here , Azure MySQL databases are similar to AWS RDS databases. Example: and be sure to set Performance Schema as the query collection method for Query Analytics. MariaDB \u00b6 MariaDB up to version 10.2 works out of the box but starting with MariaDB 10.3 instrumentation is disabled by default and cannot be enabled since there is no SUPER role in Azure-MariaDB. So, it is not possible to run the required queries to enable instrumentation. Monitoring will work but Query Analytics won\u2019t receive any query data. PostgreSQL \u00b6 For PostgreSQL follow the same methods used for MySQL and MariaDB and enable track_io_timing in the instance configuration to enable Query Analytics. For Query Analytics, set the server parameter: pg_stat_statements.track = all To discover databases on Azure \u00b6 You need to get the Client ID, Client Secret, Tenant ID and Subscription ID. Get the subscription ID \u00b6 Search Subscriptions , click on your susbcription name Copy the susbcription ID Create a new application to get the tenant ID, client ID and the client secret. \u00b6 Search for Azure Active Directory Register a new application At this point you can copy the client and tenant IDs. Create an application secret. Copy the value of the application secret. Once you leave this page you won\u2019t be able to see the secret again and you will have to generate a new one. Give API access permissions to your application. 6.1. Search for Subscriptions like in step 1. 6.2. Select your aplication and grant Monitor Reader permissions. This might require you have admin permissions in your Azure account. When you fill in all fields press the Discover button and you will see a list of available databases for monitoring. You can monitor 6 types of databases: Microsoft.DBforMySQL/servers Microsoft.DBforMySQL/flexibleServers Microsoft.DBforMariaDB/servers Microsoft.DBforPostgreSQL/servers Microsoft.DBforPostgreSQL/flexibleServers Microsoft.DBforPostgreSQL/serversv2 You can find more details on how to create DB on Azure at: https://docs.microsoft.com/en-us/azure/postgresql/ https://docs.microsoft.com/en-us/azure/mysql/ Tip You must set pg_stat_statements.track = all in your PostgreSQL Server settings to use PMM Query Analytics. ( Read more. ) In the list of databases on the Discovery page click Start Monitoring to add the selected Azure Database to PMM. Fill in all required fields and click Add service . PMM can use 3 exporters to collect metrics: Azure Metrics Exporter \u2013 collect \u201csystem\u201d metrics related to DB. node_cpu_average azure_resource_info node_filesystem_size_bytes azure_memory_percent_average azure_storage_percent_average azure_storage_used_bytes_average node_network_receive_bytes_total node_network_transmit_bytes_total mysql_exporter or postgres_exporter \u2013 to collect database related metrics. PMM Agent to collect queries related metrics using pg_stat_statements for PostgreSQL or Performance Schema for MySQL (MariaDB) Adding an Azure Instance on pmm-client side \u00b6 TLS/SSL is enforced on the server by default. So please download the certificate needed to communicate over SSL with your Azure Database. It can be done on Networking tab for your Azure Database instance. Also enforced TLS/SSL connection option can be disabled on server side. Command for adding an azure database service for monitoring without TLS/SSL. pmm-admin add mysql --username = azureuser --password = secure --host = azuremysql.mysql.database.azure.com --service-name = azure1 --query-source = perfschema Downloaded certificate is named DigiCertGlobalRootCA.crt.pem . An example of the command for adding an Azure database service for monitoring with TLS/SSL would be: pmm-admin add mysql --username = azureuser --password = secure --host = azuremysql.mysql.database.azure.com --service-name = azure1 --query-source = perfschema --tls --tls-ca = DigiCertGlobalRootCA.crt.pem --tls-cert = client-cert.pem --tls-key = client-key.pem --tls-skip-verify","title":"Microsoft Azure"},{"location":"setting-up/client/azure.html#microsoft-azure","text":"Caution Microsoft Azure functionality is currently in technical preview and is subject to change.","title":"Microsoft Azure"},{"location":"setting-up/client/azure.html#activate-microsoft-azure","text":"The Microsoft Azure feature is turned off by default. To turn it on: Go to Configuration \u2192 Settings \u2192 Advanced Settings . Click the toggle in the Technical preview features section of the page.","title":"Activate Microsoft Azure"},{"location":"setting-up/client/azure.html#required-settings","text":"It is possible to use PMM for monitoring Azure database instances like other remote instances. In this case, the PMM Client is not installed on the host where the database server is deployed. By using the PMM web interface, you connect to the Azure DB instance. Discovery is not yet implemented in PMM but it is possible to add known instances by providing the connection parameters. First of all, ensure that there is the minimal latency between PMM Server and the Azure instance. Second, add a firewall rule to enable access from PMM Client like this:","title":"Required settings"},{"location":"setting-up/client/azure.html#setting-up-a-mysql-instance","text":"Query Analytics requires you to configure Performance Schema as the query source, because the slow query log is stored on the Azure side, and QAN agent is not able to read it. Enable the performance_schema option under Parameter Groups in Azure MySQL databases. When adding a monitoring instance for Azure, specify a unique name to distinguish it from the local MySQL instance. If you do not specify a name, it will use the client\u2019s host name. Create the pmm user with the following privileges on the Azure MySQL database instance that you want to monitor: CREATE USER 'pmm' @ '%' IDENTIFIED BY 'pass' ; GRANT SELECT , PROCESS , REPLICATION CLIENT ON * . * TO 'pmm' @ '%' ; ALTER USER 'pmm' @ '%' WITH MAX_USER_CONNECTIONS 10 ;","title":"Setting up a MySQL instance"},{"location":"setting-up/client/azure.html#adding-an-azure-instance","text":"Follow the instructions for remotes instances explained here , Azure MySQL databases are similar to AWS RDS databases. Example: and be sure to set Performance Schema as the query collection method for Query Analytics.","title":"Adding an Azure Instance"},{"location":"setting-up/client/azure.html#mariadb","text":"MariaDB up to version 10.2 works out of the box but starting with MariaDB 10.3 instrumentation is disabled by default and cannot be enabled since there is no SUPER role in Azure-MariaDB. So, it is not possible to run the required queries to enable instrumentation. Monitoring will work but Query Analytics won\u2019t receive any query data.","title":"MariaDB"},{"location":"setting-up/client/azure.html#postgresql","text":"For PostgreSQL follow the same methods used for MySQL and MariaDB and enable track_io_timing in the instance configuration to enable Query Analytics. For Query Analytics, set the server parameter: pg_stat_statements.track = all","title":"PostgreSQL"},{"location":"setting-up/client/azure.html#to-discover-databases-on-azure","text":"You need to get the Client ID, Client Secret, Tenant ID and Subscription ID.","title":"To discover databases on Azure"},{"location":"setting-up/client/azure.html#get-the-subscription-id","text":"Search Subscriptions , click on your susbcription name Copy the susbcription ID","title":"Get the subscription ID"},{"location":"setting-up/client/azure.html#create-a-new-application-to-get-the-tenant-id-client-id-and-the-client-secret","text":"Search for Azure Active Directory Register a new application At this point you can copy the client and tenant IDs. Create an application secret. Copy the value of the application secret. Once you leave this page you won\u2019t be able to see the secret again and you will have to generate a new one. Give API access permissions to your application. 6.1. Search for Subscriptions like in step 1. 6.2. Select your aplication and grant Monitor Reader permissions. This might require you have admin permissions in your Azure account. When you fill in all fields press the Discover button and you will see a list of available databases for monitoring. You can monitor 6 types of databases: Microsoft.DBforMySQL/servers Microsoft.DBforMySQL/flexibleServers Microsoft.DBforMariaDB/servers Microsoft.DBforPostgreSQL/servers Microsoft.DBforPostgreSQL/flexibleServers Microsoft.DBforPostgreSQL/serversv2 You can find more details on how to create DB on Azure at: https://docs.microsoft.com/en-us/azure/postgresql/ https://docs.microsoft.com/en-us/azure/mysql/ Tip You must set pg_stat_statements.track = all in your PostgreSQL Server settings to use PMM Query Analytics. ( Read more. ) In the list of databases on the Discovery page click Start Monitoring to add the selected Azure Database to PMM. Fill in all required fields and click Add service . PMM can use 3 exporters to collect metrics: Azure Metrics Exporter \u2013 collect \u201csystem\u201d metrics related to DB. node_cpu_average azure_resource_info node_filesystem_size_bytes azure_memory_percent_average azure_storage_percent_average azure_storage_used_bytes_average node_network_receive_bytes_total node_network_transmit_bytes_total mysql_exporter or postgres_exporter \u2013 to collect database related metrics. PMM Agent to collect queries related metrics using pg_stat_statements for PostgreSQL or Performance Schema for MySQL (MariaDB)","title":"Create a new application to get the tenant ID, client ID and the client secret."},{"location":"setting-up/client/azure.html#adding-an-azure-instance-on-pmm-client-side","text":"TLS/SSL is enforced on the server by default. So please download the certificate needed to communicate over SSL with your Azure Database. It can be done on Networking tab for your Azure Database instance. Also enforced TLS/SSL connection option can be disabled on server side. Command for adding an azure database service for monitoring without TLS/SSL. pmm-admin add mysql --username = azureuser --password = secure --host = azuremysql.mysql.database.azure.com --service-name = azure1 --query-source = perfschema Downloaded certificate is named DigiCertGlobalRootCA.crt.pem . An example of the command for adding an Azure database service for monitoring with TLS/SSL would be: pmm-admin add mysql --username = azureuser --password = secure --host = azuremysql.mysql.database.azure.com --service-name = azure1 --query-source = perfschema --tls --tls-ca = DigiCertGlobalRootCA.crt.pem --tls-cert = client-cert.pem --tls-key = client-key.pem --tls-skip-verify","title":"Adding an Azure Instance on pmm-client side"},{"location":"setting-up/client/external.html","text":"External Services \u00b6 Adding general external services \u00b6 You can collect metrics from an external (custom) exporter on a node when: there is already a PMM Agent instance running and, this node has been configured using the pmm-admin config command. Usage \u00b6 pmm-admin add external --service-name = <service-name> --listen-port = <listen-port> --metrics-path = <metrics-path> --scheme = <scheme> pmm-admin add external-serverless --external-name = <external-service-name> --host = <hostname> --listen-port = <listen-port> --metrics-path = <metrics-path> --scheme = <scheme> Getting data from external exporters \u00b6 There two ways to get metrics from other exporters: external will collect metrics from the exporter that is run on the same host as PMM Client\u2019s connection to it by a port. (See more details with pmm-admin add external --help .) external-serverless is useful for collecting metrics from cloud services. You need a host and port number to add it to PMM Server. (See more details with pmm-admin add external-serverless --help .) Here are the differences between external and external-serverless types. Connection schema of external exporter: Connection schema of external serverless exporter: How I can add something not supported by PMM \u00b6 PMM can collect any metrics in Open metrics or Prometheus exposition format. You must specify the host and port of these metrics using the pmm-admin add external or pmm-admin add external-serverless commands. From this point, PMM will collect and store available metrics. To browse and visualize collected metrics as a first step, we can look at the Advanced Data Exploration dashboard and select informative services and metrics. Another way is to create a new Grafana Dashboard to PMM as needed . One more way is to search for an already created dashboard at https://grafana.com/grafana/dashboards for the added exporter and import it into PMM. Third-party exporters \u00b6 You can find more exporters on the official Prometheus page . Custom exporter \u00b6 You can write a custom external exporter or extend your application to expose metrics in Prometheus format. For more details see https://prometheus.io/docs/instrumenting/writing_exporters/ . Examples \u00b6 root@mysql1:~# pmm-admin add external --group = processes --listen-port = 9256 External Service added. Service ID : /service_id/6485f4fd-745b-4dfb-8b72-328e300f8b50 Service name: mysql1-processes Group : processes Add an exporter running on local port 9256 to the group called processes . Use the group and host names to automatically generate a service name. Use the default scheme and metrics path. Adding an External service via UI \u00b6 In the PMM web interface, go to Configuration \u2192 PMM Inventory \u2192 Add Instance . Select External Service \u2013 Add a remote instance . Fill the form and set the external service endpoint. The endpoint can be set manually: or by parsing required data from a URL string, in which case you only need to pass a valid URL.","title":"External Services"},{"location":"setting-up/client/external.html#external-services","text":"","title":"External Services"},{"location":"setting-up/client/external.html#adding-general-external-services","text":"You can collect metrics from an external (custom) exporter on a node when: there is already a PMM Agent instance running and, this node has been configured using the pmm-admin config command.","title":"Adding general external services"},{"location":"setting-up/client/external.html#usage","text":"pmm-admin add external --service-name = <service-name> --listen-port = <listen-port> --metrics-path = <metrics-path> --scheme = <scheme> pmm-admin add external-serverless --external-name = <external-service-name> --host = <hostname> --listen-port = <listen-port> --metrics-path = <metrics-path> --scheme = <scheme>","title":"Usage"},{"location":"setting-up/client/external.html#getting-data-from-external-exporters","text":"There two ways to get metrics from other exporters: external will collect metrics from the exporter that is run on the same host as PMM Client\u2019s connection to it by a port. (See more details with pmm-admin add external --help .) external-serverless is useful for collecting metrics from cloud services. You need a host and port number to add it to PMM Server. (See more details with pmm-admin add external-serverless --help .) Here are the differences between external and external-serverless types. Connection schema of external exporter: Connection schema of external serverless exporter:","title":"Getting data from external exporters"},{"location":"setting-up/client/external.html#how-i-can-add-something-not-supported-by-pmm","text":"PMM can collect any metrics in Open metrics or Prometheus exposition format. You must specify the host and port of these metrics using the pmm-admin add external or pmm-admin add external-serverless commands. From this point, PMM will collect and store available metrics. To browse and visualize collected metrics as a first step, we can look at the Advanced Data Exploration dashboard and select informative services and metrics. Another way is to create a new Grafana Dashboard to PMM as needed . One more way is to search for an already created dashboard at https://grafana.com/grafana/dashboards for the added exporter and import it into PMM.","title":"How I can add something not supported by PMM"},{"location":"setting-up/client/external.html#third-party-exporters","text":"You can find more exporters on the official Prometheus page .","title":"Third-party exporters"},{"location":"setting-up/client/external.html#custom-exporter","text":"You can write a custom external exporter or extend your application to expose metrics in Prometheus format. For more details see https://prometheus.io/docs/instrumenting/writing_exporters/ .","title":"Custom exporter"},{"location":"setting-up/client/external.html#examples","text":"root@mysql1:~# pmm-admin add external --group = processes --listen-port = 9256 External Service added. Service ID : /service_id/6485f4fd-745b-4dfb-8b72-328e300f8b50 Service name: mysql1-processes Group : processes Add an exporter running on local port 9256 to the group called processes . Use the group and host names to automatically generate a service name. Use the default scheme and metrics path.","title":"Examples"},{"location":"setting-up/client/external.html#adding-an-external-service-via-ui","text":"In the PMM web interface, go to Configuration \u2192 PMM Inventory \u2192 Add Instance . Select External Service \u2013 Add a remote instance . Fill the form and set the external service endpoint. The endpoint can be set manually: or by parsing required data from a URL string, in which case you only need to pass a valid URL.","title":"Adding an External service via UI"},{"location":"setting-up/client/google.html","text":"Google Cloud Platform \u00b6 PMM can monitor MySQL or PostgreSQL instances hosted on the Google Cloud Platform . The connection can be direct, or indirect using Cloud SQL Proxy . MySQL \u00b6 Set up a MySQL instance on Google Cloud . The database server must be accessible by PMM Client. If PMM Client is not also hosted on GCP, you will need to add a network interface with a public interface. Configure Performance Schema on the MySQL server. Using the GCP console\u2019s Cloud Shell or your own gcloud installation, run: gcloud sql instances patch <instance_name> --database-flags performance_schema = on Log into the PMM user interface. Select Configuration \u2192 PMM Inventory \u2192 Add Instance . Click MySQL Add a remote instance . Fill in the details for the remote MySQL instance. Ensure Use performance schema is selected. Click Add service . Check for values in the MySQL Instance Overview dashboard and in Query Analytics . PostgreSQL \u00b6 Set up a PostgreSQL instance on Google Cloud . The database server must be accessible by PMM Client. If PMM Client is not also hosted on GCP, you will need to add a network interface with a public interface. Configure pg_stat_statements . Open an interactive SQL session with your GCP PostgreSQL server and run: CREATE EXTENSION pg_stat_statements ; Log into the PMM user interface. Select Configuration \u2192 PMM Inventory \u2192 Add Instance . Select PostgreSQL Add a remote instance . Fill in the details for the remote PostgreSQL instance: In Stat tracking options , select PG Stat Statements . Click Add service . Check for values in the PostgreSQL Instance Overview dashboard and Query Analytics . Cloud SQL Proxy \u00b6 MySQL \u00b6 Create instance on GCP. Note connection as <project_id>:<zone>:<db_instance_name> . Enable Admin API and download the JSON credential file. Enable Performance Schema . Run Cloud SQL Proxy (runs on PMM Client node). As a Docker container: docker run -d \\ -v ~/path/to/admin-api-file.json:/config \\ -p 127 .0.0.1:3306:3306 \\ gcr.io/cloudsql-docker/gce-proxy:1.19.1 \\ /cloud_sql_proxy \\ -instances = example-project-NNNN:us-central1:mysql-for-pmm = tcp:0.0.0.0:3306 \\ -credential_file = /config On Linux: wget https://dl.google.com/cloudsql/cloud_sql_proxy.linux.amd64 -O cloud_sql_proxy chmod +x cloud_sql_proxy ./cloud_sql_proxy -instances = example-project-NNNN:us-central1:mysql-for-pmm = tcp:3306 \\ -credential_file = /path/to/credential-file.json Add instance. pmm-admin add mysql --host = 127 .0.0.1 --port = 3306 \\ --username = root --password = secret \\ --service-name = MySQLGCP --query-source = perfschema PostgreSQL \u00b6 Create instance on GCP. Note connection as <project_id>:<zone>:<db_instance_name> . Enable Admin API and download the JSON credential file. Run Cloud SQL Proxy. ./cloud_sql_proxy -instances = example-project-NNNN:us-central1:pg-for-pmm = tcp:5432 \\ -credential_file = /path/to/credential-file.json Log into PostgreSQL. Load extension: CREATE EXTENSION pg_stat_statements ; Add service: pmm-admin add postgresql --host = 127 .0.0.1 --port = 5432 \\ --username = \"postgres\" --password = secret --service-name = PGGCP","title":"Google Cloud Platform"},{"location":"setting-up/client/google.html#google-cloud-platform","text":"PMM can monitor MySQL or PostgreSQL instances hosted on the Google Cloud Platform . The connection can be direct, or indirect using Cloud SQL Proxy .","title":"Google Cloud Platform"},{"location":"setting-up/client/google.html#mysql","text":"Set up a MySQL instance on Google Cloud . The database server must be accessible by PMM Client. If PMM Client is not also hosted on GCP, you will need to add a network interface with a public interface. Configure Performance Schema on the MySQL server. Using the GCP console\u2019s Cloud Shell or your own gcloud installation, run: gcloud sql instances patch <instance_name> --database-flags performance_schema = on Log into the PMM user interface. Select Configuration \u2192 PMM Inventory \u2192 Add Instance . Click MySQL Add a remote instance . Fill in the details for the remote MySQL instance. Ensure Use performance schema is selected. Click Add service . Check for values in the MySQL Instance Overview dashboard and in Query Analytics .","title":"MySQL"},{"location":"setting-up/client/google.html#postgresql","text":"Set up a PostgreSQL instance on Google Cloud . The database server must be accessible by PMM Client. If PMM Client is not also hosted on GCP, you will need to add a network interface with a public interface. Configure pg_stat_statements . Open an interactive SQL session with your GCP PostgreSQL server and run: CREATE EXTENSION pg_stat_statements ; Log into the PMM user interface. Select Configuration \u2192 PMM Inventory \u2192 Add Instance . Select PostgreSQL Add a remote instance . Fill in the details for the remote PostgreSQL instance: In Stat tracking options , select PG Stat Statements . Click Add service . Check for values in the PostgreSQL Instance Overview dashboard and Query Analytics .","title":"PostgreSQL"},{"location":"setting-up/client/google.html#cloud-sql-proxy","text":"","title":"Cloud SQL Proxy"},{"location":"setting-up/client/google.html#mysql_1","text":"Create instance on GCP. Note connection as <project_id>:<zone>:<db_instance_name> . Enable Admin API and download the JSON credential file. Enable Performance Schema . Run Cloud SQL Proxy (runs on PMM Client node). As a Docker container: docker run -d \\ -v ~/path/to/admin-api-file.json:/config \\ -p 127 .0.0.1:3306:3306 \\ gcr.io/cloudsql-docker/gce-proxy:1.19.1 \\ /cloud_sql_proxy \\ -instances = example-project-NNNN:us-central1:mysql-for-pmm = tcp:0.0.0.0:3306 \\ -credential_file = /config On Linux: wget https://dl.google.com/cloudsql/cloud_sql_proxy.linux.amd64 -O cloud_sql_proxy chmod +x cloud_sql_proxy ./cloud_sql_proxy -instances = example-project-NNNN:us-central1:mysql-for-pmm = tcp:3306 \\ -credential_file = /path/to/credential-file.json Add instance. pmm-admin add mysql --host = 127 .0.0.1 --port = 3306 \\ --username = root --password = secret \\ --service-name = MySQLGCP --query-source = perfschema","title":"MySQL"},{"location":"setting-up/client/google.html#postgresql_1","text":"Create instance on GCP. Note connection as <project_id>:<zone>:<db_instance_name> . Enable Admin API and download the JSON credential file. Run Cloud SQL Proxy. ./cloud_sql_proxy -instances = example-project-NNNN:us-central1:pg-for-pmm = tcp:5432 \\ -credential_file = /path/to/credential-file.json Log into PostgreSQL. Load extension: CREATE EXTENSION pg_stat_statements ; Add service: pmm-admin add postgresql --host = 127 .0.0.1 --port = 5432 \\ --username = \"postgres\" --password = secret --service-name = PGGCP","title":"PostgreSQL"},{"location":"setting-up/client/haproxy.html","text":"HAProxy \u00b6 Adding HAProxy services \u00b6 You can collect metrics from HAProxy on a node when: There is already a configured haproxy instance. See How to configure HAProxy . After HAProxy is running (default address http://localhost:8404/metrics ) you can add it to PMM. Use the haproxy alias to enable HAProxy metrics monitoring. There is already a PMM Agent instance running. This node has been configured using the pmm-admin config command. USAGE \u00b6 pmm-admin add haproxy --listen-port = 8404 where listen-port is the port number where HAProxy running. (This is the only required flag.) The output of this command should look as follows: HAProxy Service added. Service ID : /service_id/c481183f-70a2-443f-91e5-cae5cecd06a2 Service name: Ubuntu-haproxy Additionally, one positional argument can be appended to the command line flags: a service name to be used by PMM. If not specified, they are substituted automatically as <node>-haproxy . During adding here is connection check (can be skipped by flag --skip-connection-check ). If HAProxy doesn\u2019t run properly on the given port then you will see an error message: Connection check failed: Get \"http://127.0.0.1:8404/metrics\": dial tcp 127.0.0.1:8404: connect: connection refused. Beside positional argument shown above you can specify service name with the following flags: --username , --password , --metrics-path (path for scraping metrics, default: /metrics) and --scheme (http or https). Here are some examples: pmm-admin add haproxy --listen-port = 8404 --username = pmm --password = pmm new-haproxy pmm-admin add haproxy --listen-port = 8404 --metrics-path = /prom-metrics --scheme = https Here you can check list of all available flags: pmm-admin . You can also add HAProxy by UI in Grafana: Select Configuration \u2192 PMM Inventory \u2192 Add Instance . HAProxy data is visible in the Advanced Data Exploration dashboard:","title":"HAProxy"},{"location":"setting-up/client/haproxy.html#haproxy","text":"","title":"HAProxy"},{"location":"setting-up/client/haproxy.html#adding-haproxy-services","text":"You can collect metrics from HAProxy on a node when: There is already a configured haproxy instance. See How to configure HAProxy . After HAProxy is running (default address http://localhost:8404/metrics ) you can add it to PMM. Use the haproxy alias to enable HAProxy metrics monitoring. There is already a PMM Agent instance running. This node has been configured using the pmm-admin config command.","title":"Adding HAProxy services"},{"location":"setting-up/client/haproxy.html#usage","text":"pmm-admin add haproxy --listen-port = 8404 where listen-port is the port number where HAProxy running. (This is the only required flag.) The output of this command should look as follows: HAProxy Service added. Service ID : /service_id/c481183f-70a2-443f-91e5-cae5cecd06a2 Service name: Ubuntu-haproxy Additionally, one positional argument can be appended to the command line flags: a service name to be used by PMM. If not specified, they are substituted automatically as <node>-haproxy . During adding here is connection check (can be skipped by flag --skip-connection-check ). If HAProxy doesn\u2019t run properly on the given port then you will see an error message: Connection check failed: Get \"http://127.0.0.1:8404/metrics\": dial tcp 127.0.0.1:8404: connect: connection refused. Beside positional argument shown above you can specify service name with the following flags: --username , --password , --metrics-path (path for scraping metrics, default: /metrics) and --scheme (http or https). Here are some examples: pmm-admin add haproxy --listen-port = 8404 --username = pmm --password = pmm new-haproxy pmm-admin add haproxy --listen-port = 8404 --metrics-path = /prom-metrics --scheme = https Here you can check list of all available flags: pmm-admin . You can also add HAProxy by UI in Grafana: Select Configuration \u2192 PMM Inventory \u2192 Add Instance . HAProxy data is visible in the Advanced Data Exploration dashboard:","title":"USAGE"},{"location":"setting-up/client/linux.html","text":"Linux \u00b6 Adding general system metrics service \u00b6 PMM collects Linux metrics automatically starting from the moment when you have configured your node for monitoring with pmm-admin config .","title":"Linux"},{"location":"setting-up/client/linux.html#linux","text":"","title":"Linux"},{"location":"setting-up/client/linux.html#adding-general-system-metrics-service","text":"PMM collects Linux metrics automatically starting from the moment when you have configured your node for monitoring with pmm-admin config .","title":"Adding general system metrics service"},{"location":"setting-up/client/mongodb.html","text":"MongoDB \u00b6 How to set up PMM to monitor a MongoDB or Percona Server for MongoDB database instance. Summary Create PMM account and set permissions. Configure profiling. Add service. Check service. Before you start \u00b6 Check that: PMM Server is installed and running with a known IP address or hostname accessible from the client node. PMM Client is installed and the node is registered with PMM Server . You have superuser (root) access on the client host. You have adminUserAnyDatabase or superuser role privilege to any database servers that you want to monitor. Your MongoDB server is version 4.0 or higher. Create PMM account and set permissions \u00b6 We recommend using a dedicated account to connect PMM Client to the monitored database instance. This example creates a new custom role with the privileges needed by the Query Analyzer, and adds a database user with that role plus the built-in clusterMonitor role. Values for username ( user ) and password ( pwd ) are examples. Replace them before using this code. Run this in a mongo session. db.ge t Sibli n gDB( \"admin\" ).crea te Role( { role : \"explainRole\" , privileges : [{ resource : { db : \"\" , collec t io n : \"\" }, ac t io ns : [ \"listIndexes\" , \"listCollections\" , \"dbStats\" , \"dbHash\" , \"collStats\" , \"find\" ] }], roles :[] } ) db.ge t Sibli n gDB( \"admin\" ).crea te User( { user : \"pmm_mongodb\" , pwd : \"password\" , roles : [ { role : \"explainRole\" , db : \"admin\" }, { role : \"clusterMonitor\" , db : \"admin\" }, { role : \"read\" , db : \"local\" } ] } ) Profiling \u00b6 To use PMM Query Analytics, you must turn on MongoDB\u2019s profiling feature . You can set profiling: permanently, by editing the MongoDB configuration file and restarting the database instance (recommended); when starting MongoDB, by passing arguments to mongod on the command line; until the next database instance restart, by running a command in a mongo session. Profiling is turned off by default as it can adversely affect the performance of the database server. Set profiling in the configuration file \u00b6 Edit the configuration file (usually /etc/mongod.conf ). Create or add this to the operationProfiling section. ( Read more .) operationProfiling: mode: all slowOpThresholdMs: 200 rateLimit: 100 # (Only available with Percona Server for MongoDB.) Important This is a YAML file. Indentation matters. Restart the mongod service. (Example for systemd .) systemctl restart mongod Set profiling on the command Line \u00b6 mongod --dbpath = DATABASEDIR --profile 2 --slowms 200 --rateLimit 100 --dbpath : The path to database files (usually /var/lib/mongo ). --profile : The MongoDB profiling level. A value of 2 tells the server to collect profiling data for all operations. To lower the load on the server, use a value of 1 to only record slow operations. --slowms : An operation is classified as slow if it runs for longer than this number of milliseconds. --rateLimit : (Only available with Percona Server for MongoDB.) The sample rate of profiled queries. A value of 100 means sample every 100 th fast query. ( Read more .) Caution Smaller values improve accuracy but can adversly affect the performance of your server. Set profiling in a mongo session \u00b6 In a mongo session, the profiler should be enabled per database. For example, to enable the profiler in the testdb , run this: use test db db.se t Pro f ili n gLevel( 2 , { slowms : 0 } ) If you have already added a service , you should remove it and re-add it after changing the profiling level. Add service \u00b6 When you have configured your database server, you can add a MongoDB service with the user interface or on the command line. With the user interface \u00b6 Select Configuration \u2192 PMM Inventory \u2192 Add Instance . Select MongoDB \u2013 Add a remote instance . Enter or select values for the fields. Click Add service . On the command line \u00b6 Use pmm-admin to add the database server as a service using one of these example commands. When successful, PMM Client will print MongoDB Service added with the service\u2019s ID and name. Use the --environment and -custom-labels options to set tags for the service to help identify them. Tips When adding nodes of a sharded cluster, add each node separately using the --cluster mycluster option for the MongoDB Cluster Summary dashboard to populate correctly. Atlas doesn\u2019t support direct connections. When connecting to an Atlas instance, use the pmm-admin option --direct-connection=false . (Doing so will prevent replicaset status from working and the MongoDB Overview dashboard widget will show invalid values.) Examples \u00b6 pmm-admin add mongodb \\ --username = pmm_mongodb --password = password \\ --query-source = profiler --cluster = mycluster pmm-admin add mongodb \\ --username = pmm_mongodb --password = password \\ mongo 127 .0.0.1:27017 pmm-admin add mongodb \\ --username = pmm_mongodb --password = password \\ --service-name = mymongosvc --host = 127 .0.0.1 --port = 27017 Connect via UNIX socket \u00b6 pmm-admin add mongodb --socket = /tmp/mongodb-27017.sock Connecting via SSL/TLS \u00b6 pmm-admin add mongodb --tls \\ --tls-certificate-key-file = PATHTOCER \\ --tls-certificate-key-file-password = IFPASSWORDTOCERTISSET \\ --tls-ca-file = PATHTOCACERT --authentication-mechanism = AUTHENTICATION-MECHANISM --authentication-database = AUTHENTICATION-DATABASE where: PATHTOCERT : Path to TLS certificate file. IFPASSWORDTOCERTISSET : Password for TLS certificate file. PATHTOCACERT : Path to certificate authority file. AUTHENTICATION-MECHANISM : Authentication mechanism. Default is empty. Use MONGODB-X509 for SSL certificates. AUTHENTICATION-DATABASE : Authentication database. Default is empty. Use $external for SSL certificates. Check the service \u00b6 With the user interface \u00b6 Select Configuration \u2192 PMM Inventory \u2192 Inventory list . Look in the Services tab for a matching Service Type (MongoDB), Service name , Addresses , and any other values used when adding the service. Look in the Agents tab to check the desired data source is being used. If your MongoDB instance is configured to use TLS, click on the Use TLS for database connection check box and fill in TLS certificates and keys. If you use TLS, the authentication mechanism is automatically set to MONGODB-X509 . On the command line \u00b6 Look for your service in the output of this command. pmm-admin inventory list services --service-type = mongodb Check data \u00b6 Open the MongoDB Instances Overview dashboard. Set the Service Name to the newly-added service. Query Analytics \u00b6 Open PMM Query Analytics . In the Filters panel: Under Service Name , select your service. Under Service Type select mongodb . Remove service \u00b6 With the user interface \u00b6 Select Configuration \u2192 PMM Inventory \u2192 Inventory List . In the first column, click the tick box for the service you want to remove. Click Delete . On the Confirm action dialog window: (Optional) Select Force mode to also delete associated agents. Click Proceed . On the command line \u00b6 pmm-admin remove mongodb SERVICE_NAME SERVICE_NAME : The name the service was added as. (Find it with pmm-admin list .) See also pmm-admin add mongodb Troubleshooting connection difficulties","title":"MongoDB"},{"location":"setting-up/client/mongodb.html#mongodb","text":"How to set up PMM to monitor a MongoDB or Percona Server for MongoDB database instance. Summary Create PMM account and set permissions. Configure profiling. Add service. Check service.","title":"MongoDB"},{"location":"setting-up/client/mongodb.html#before-you-start","text":"Check that: PMM Server is installed and running with a known IP address or hostname accessible from the client node. PMM Client is installed and the node is registered with PMM Server . You have superuser (root) access on the client host. You have adminUserAnyDatabase or superuser role privilege to any database servers that you want to monitor. Your MongoDB server is version 4.0 or higher.","title":"Before you start"},{"location":"setting-up/client/mongodb.html#create-pmm-account-and-set-permissions","text":"We recommend using a dedicated account to connect PMM Client to the monitored database instance. This example creates a new custom role with the privileges needed by the Query Analyzer, and adds a database user with that role plus the built-in clusterMonitor role. Values for username ( user ) and password ( pwd ) are examples. Replace them before using this code. Run this in a mongo session. db.ge t Sibli n gDB( \"admin\" ).crea te Role( { role : \"explainRole\" , privileges : [{ resource : { db : \"\" , collec t io n : \"\" }, ac t io ns : [ \"listIndexes\" , \"listCollections\" , \"dbStats\" , \"dbHash\" , \"collStats\" , \"find\" ] }], roles :[] } ) db.ge t Sibli n gDB( \"admin\" ).crea te User( { user : \"pmm_mongodb\" , pwd : \"password\" , roles : [ { role : \"explainRole\" , db : \"admin\" }, { role : \"clusterMonitor\" , db : \"admin\" }, { role : \"read\" , db : \"local\" } ] } )","title":"Create PMM account and set permissions"},{"location":"setting-up/client/mongodb.html#profiling","text":"To use PMM Query Analytics, you must turn on MongoDB\u2019s profiling feature . You can set profiling: permanently, by editing the MongoDB configuration file and restarting the database instance (recommended); when starting MongoDB, by passing arguments to mongod on the command line; until the next database instance restart, by running a command in a mongo session. Profiling is turned off by default as it can adversely affect the performance of the database server.","title":"Profiling"},{"location":"setting-up/client/mongodb.html#set-profiling-in-the-configuration-file","text":"Edit the configuration file (usually /etc/mongod.conf ). Create or add this to the operationProfiling section. ( Read more .) operationProfiling: mode: all slowOpThresholdMs: 200 rateLimit: 100 # (Only available with Percona Server for MongoDB.) Important This is a YAML file. Indentation matters. Restart the mongod service. (Example for systemd .) systemctl restart mongod","title":"Set profiling in the configuration file"},{"location":"setting-up/client/mongodb.html#set-profiling-on-the-command-line","text":"mongod --dbpath = DATABASEDIR --profile 2 --slowms 200 --rateLimit 100 --dbpath : The path to database files (usually /var/lib/mongo ). --profile : The MongoDB profiling level. A value of 2 tells the server to collect profiling data for all operations. To lower the load on the server, use a value of 1 to only record slow operations. --slowms : An operation is classified as slow if it runs for longer than this number of milliseconds. --rateLimit : (Only available with Percona Server for MongoDB.) The sample rate of profiled queries. A value of 100 means sample every 100 th fast query. ( Read more .) Caution Smaller values improve accuracy but can adversly affect the performance of your server.","title":"Set profiling on the command Line"},{"location":"setting-up/client/mongodb.html#set-profiling-in-a-mongo-session","text":"In a mongo session, the profiler should be enabled per database. For example, to enable the profiler in the testdb , run this: use test db db.se t Pro f ili n gLevel( 2 , { slowms : 0 } ) If you have already added a service , you should remove it and re-add it after changing the profiling level.","title":"Set profiling in a mongo session"},{"location":"setting-up/client/mongodb.html#add-service","text":"When you have configured your database server, you can add a MongoDB service with the user interface or on the command line.","title":"Add service"},{"location":"setting-up/client/mongodb.html#with-the-user-interface","text":"Select Configuration \u2192 PMM Inventory \u2192 Add Instance . Select MongoDB \u2013 Add a remote instance . Enter or select values for the fields. Click Add service .","title":"With the user interface"},{"location":"setting-up/client/mongodb.html#on-the-command-line","text":"Use pmm-admin to add the database server as a service using one of these example commands. When successful, PMM Client will print MongoDB Service added with the service\u2019s ID and name. Use the --environment and -custom-labels options to set tags for the service to help identify them. Tips When adding nodes of a sharded cluster, add each node separately using the --cluster mycluster option for the MongoDB Cluster Summary dashboard to populate correctly. Atlas doesn\u2019t support direct connections. When connecting to an Atlas instance, use the pmm-admin option --direct-connection=false . (Doing so will prevent replicaset status from working and the MongoDB Overview dashboard widget will show invalid values.)","title":"On the command line"},{"location":"setting-up/client/mongodb.html#examples","text":"pmm-admin add mongodb \\ --username = pmm_mongodb --password = password \\ --query-source = profiler --cluster = mycluster pmm-admin add mongodb \\ --username = pmm_mongodb --password = password \\ mongo 127 .0.0.1:27017 pmm-admin add mongodb \\ --username = pmm_mongodb --password = password \\ --service-name = mymongosvc --host = 127 .0.0.1 --port = 27017","title":"Examples"},{"location":"setting-up/client/mongodb.html#connect-via-unix-socket","text":"pmm-admin add mongodb --socket = /tmp/mongodb-27017.sock","title":"Connect via UNIX socket"},{"location":"setting-up/client/mongodb.html#connecting-via-ssltls","text":"pmm-admin add mongodb --tls \\ --tls-certificate-key-file = PATHTOCER \\ --tls-certificate-key-file-password = IFPASSWORDTOCERTISSET \\ --tls-ca-file = PATHTOCACERT --authentication-mechanism = AUTHENTICATION-MECHANISM --authentication-database = AUTHENTICATION-DATABASE where: PATHTOCERT : Path to TLS certificate file. IFPASSWORDTOCERTISSET : Password for TLS certificate file. PATHTOCACERT : Path to certificate authority file. AUTHENTICATION-MECHANISM : Authentication mechanism. Default is empty. Use MONGODB-X509 for SSL certificates. AUTHENTICATION-DATABASE : Authentication database. Default is empty. Use $external for SSL certificates.","title":"Connecting via SSL/TLS"},{"location":"setting-up/client/mongodb.html#check-the-service","text":"","title":"Check the service"},{"location":"setting-up/client/mongodb.html#with-the-user-interface_1","text":"Select Configuration \u2192 PMM Inventory \u2192 Inventory list . Look in the Services tab for a matching Service Type (MongoDB), Service name , Addresses , and any other values used when adding the service. Look in the Agents tab to check the desired data source is being used. If your MongoDB instance is configured to use TLS, click on the Use TLS for database connection check box and fill in TLS certificates and keys. If you use TLS, the authentication mechanism is automatically set to MONGODB-X509 .","title":"With the user interface"},{"location":"setting-up/client/mongodb.html#on-the-command-line_1","text":"Look for your service in the output of this command. pmm-admin inventory list services --service-type = mongodb","title":"On the command line"},{"location":"setting-up/client/mongodb.html#check-data","text":"Open the MongoDB Instances Overview dashboard. Set the Service Name to the newly-added service.","title":"Check data"},{"location":"setting-up/client/mongodb.html#query-analytics","text":"Open PMM Query Analytics . In the Filters panel: Under Service Name , select your service. Under Service Type select mongodb .","title":"Query Analytics"},{"location":"setting-up/client/mongodb.html#remove-service","text":"","title":"Remove service"},{"location":"setting-up/client/mongodb.html#with-the-user-interface_2","text":"Select Configuration \u2192 PMM Inventory \u2192 Inventory List . In the first column, click the tick box for the service you want to remove. Click Delete . On the Confirm action dialog window: (Optional) Select Force mode to also delete associated agents. Click Proceed .","title":"With the user interface"},{"location":"setting-up/client/mongodb.html#on-the-command-line_2","text":"pmm-admin remove mongodb SERVICE_NAME SERVICE_NAME : The name the service was added as. (Find it with pmm-admin list .) See also pmm-admin add mongodb Troubleshooting connection difficulties","title":"On the command line"},{"location":"setting-up/client/mysql.html","text":"MySQL \u00b6 How to set up PMM to monitor a MySQL or MySQL-based database instance. PMM Client collects metrics from MySQL , Percona Server for MySQL , Percona XtraDB Cluster , and MariaDB . (Amazon RDS is also supported and explained in a separate section .) Summary Create PMM account and set permissions. Choose a data source: Slow query log, or, Performance Schema. Configure: Query response time, Tablestats, User statistics. Add service. Check service. Before you start \u00b6 Check that: PMM Server is installed and running with a known IP address accessible from the client node. PMM Client is installed and the node is registered with PMM Server . You have superuser (root) access on the client host. Create a database account for PMM \u00b6 It is good practice to use a non-superuser account to connect PMM Client to the monitored database instance. This example creates a database user with name pmm , password pass , and the necessary permissions. On MySQL 8.0 CREATE USER 'pmm' @ 'localhost' IDENTIFIED BY 'pass' WITH MAX_USER_CONNECTIONS 10 ; GRANT SELECT , PROCESS , REPLICATION CLIENT , RELOAD , BACKUP_ADMIN ON * . * TO 'pmm' @ 'localhost' ; On MySQL 5.7 CREATE USER 'pmm' @ 'localhost' IDENTIFIED BY 'pass' WITH MAX_USER_CONNECTIONS 10 ; GRANT SELECT , PROCESS , REPLICATION CLIENT , RELOAD ON * . * TO 'pmm' @ 'localhost' ; Choose and configure a source \u00b6 Decide which source of metrics to use, and configure your database server for it. The choices are Slow query log and Performance Schema . While you can use both at the same time we recommend using only one\u2013there is some overlap in the data reported, and each incurs a small performance penalty. The choice depends on the version and variant of your MySQL instance, and how much detail you want to see. Here are the benefits and drawbacks of Slow query log and Performance Schema metrics sources. Benefits Drawbacks Slow query log 1. More detail. 2. Lower resource impact (with query sampling feature in Percona Server for MySQL). 1. PMM Client must be on same host as database server or have access to slow query log. 2. Log files grow and must be actively managed. Performance Schema 1. Faster parsing. 2. Enabled by default on newer versions of MySQL. 1. Less detail. Data source recommendations \u00b6 Database server Versions Recommended source MySQL 5.1-5.5 Slow query log MySQL 5.6+ Performance Schema MariaDB 10.0+ Performance Schema Percona Server for MySQL 5.7, 8.0 Slow query log Percona XtraDB Cluster 5.6, 5.7, 8.0 Slow query log Slow query log \u00b6 This section covers how to configure a MySQL-based database server to use the slow query log as a source of metrics. Applicable versions \u00b6 Server Versions MySQL 5.1-5.5 MariaDB 10.1.2+ Percona Server for MySQL 5.7.10+, 8.0.12+ Percona XtraDB Cluster 5.6, 5.7, 8.0 The slow query log records the details of queries that take more than a certain amount of time to complete. With the database server configured to write this information to a file rather than a table, PMM Client parses the file and sends aggregated data to PMM Server via the Query Analytics part of PMM Agent. Settings \u00b6 Variable Value Description slow_query_log ON Enables the slow query log. log_output 'FILE' Ensures the log is sent to a file. (This is the default on MariaDB.) long_query_time 0 The slow query threshold in seconds. In heavily-loaded applications, many quick queries can affect performance more than a few slow ones. Setting this value to 0 ensures all queries are captured. log_slow_admin_statements ON Includes the logging of slow administrative statements. log_slow_slave_statements ON Enables logging for queries that have taken more than long_query_time seconds to execute on the replica. Examples \u00b6 Configuration file. slow_query_log = ON log_output = FILE long_query_time = 0 log_slow_admin_statements = ON log_slow_slave_statements = ON Session. SET GLOBAL slow_query_log = 1 ; SET GLOBAL log_output = 'FILE' ; SET GLOBAL long_query_time = 0 ; SET GLOBAL log_slow_admin_statements = 1 ; SET GLOBAL log_slow_slave_statements = 1 ; Slow query log \u2013 extended \u00b6 Some MySQL-based database servers support extended slow query log variables. Applicable versions \u00b6 Server Versions Percona Server for MySQL 5.7.10+, 8.0.12+ Percona XtraDB Cluster 5.6, 5.7, 8.0 MariaDB 10.0 Settings \u00b6 Variable Value Description log_slow_rate_limit 100 Defines the rate of queries captured by the slow query log . A good rule of thumb is 100 queries logged per second. For example, if your Percona Server instance processes 10,000 queries per second, you should set log_slow_rate_limit to 100 and capture every 100 th query for the slow query log . Depending on the amount of traffic, logging could become aggressive and resource consuming. This variable throttles the level of intensity of the data capture without compromising information. log_slow_rate_type \u2018query\u2019 Set so that it applies to queries, rather than sessions. slow_query_log_always_write_time 1 Specifies which queries should ignore sampling. With query sampling this ensures that queries with longer execution time will always be captured by the slow query log, avoiding the possibility that infrequent slow queries might not get captured at all. log_slow_verbosity \u2018full\u2019 Ensures that all information about each captured query is stored in the slow query log. slow_query_log_use_global_control \u2018all\u2019 Configure the slow query log during runtime and apply these settings to existing connections. (By default, slow query log settings apply only to new sessions.) Examples \u00b6 Configuration file (Percona Server for MySQL, Percona XtraDB Cluster). log_slow_rate_limit = 100 log_slow_rate_type = 'query' slow_query_log_always_write_time = 1 log_slow_verbosity = 'full' slow_query_log_use_global_control = 'all' Configuration file (MariaDB). log_slow_rate_limit = 100 Session (Percona Server for MySQL, Percona XtraDB Cluster). SET GLOBAL log_slow_rate_limit = 100 ; SET GLOBAL log_slow_rate_type = 'query' ; SET GLOBAL slow_query_log_always_write_time = 1 ; SET GLOBAL log_slow_verbosity = 'full' ; SET GLOBAL slow_query_log_use_global_control = 'all' ; Slow query log rotation \u00b6 Slow query log files can grow quickly and must be managed. When adding a service with the command line use the pmm-admin option --size-slow-logs to set at what size the slow query log file is rotated. (The size is specified as a number with a suffix. See pmm-admin add mysql .) When the limit is reached, PMM Client will: remove the previous .old slow log file, rename the current file by adding the suffix .old , execute the MySQL command FLUSH LOGS . Only one .old file is kept. Older ones are deleted. You can manage log rotation yourself, for example, with logrotate . If you do, you can disable PMM Client\u2019s log rotation by providing a negative value to --size-slow-logs option when adding a service with pmm-admin add . Performance Schema \u00b6 This section covers how to configure a MySQL-based database server to use Performance Schema as a source of metrics. Applicable versions \u00b6 Server Versions Percona Server for MySQL 5.6, 5.7, 8.0 Percona XtraDB Cluster 5.6, 5.7, 8.0 MariaDB 10.3+ PMM\u2019s MySQL Performance Schema Details dashboard charts the various performance_schema metrics. To use Performance Schema , set these variables. Variable Value Description performance_schema ON Enables Performance Schema metrics. This is the default in MySQL 5.6.6 and higher. performance-schema-instrument 'statement/%=ON' Configures Performance Schema instruments. performance-schema-consumer-statements-digest ON Configures the statements-digest consumer. innodb_monitor_enable all Enables InnoDB metrics counters. Examples \u00b6 Configuration file. performance_schema = ON performance-schema-instrument = 'statement/%=ON' performance-schema-consumer-statements-digest = ON innodb_monitor_enable = all Session. ( performance_schema cannot be set in a session and must be set at server start-up.) UPDATE performance_schema . setup_consumers SET ENABLED = 'YES' WHERE NAME LIKE '%statements%' ; SET GLOBAL innodb_monitor_enable = all ; MariaDB 10.5.7 or lower \u00b6 There is no Explain or Example data shown by default in Query Analytics when monitoring MariaDB instances version 10.5.7 or lower. A workaround is to set this variable. Variable Value Description performance_schema.setup_instruments 'statement/%' List of instrumented object classes. Session. UPDATE performance_schema . setup_instruments SET ENABLED = 'YES' , TIMED = 'YES' WHERE NAME LIKE 'statement/%' ; UPDATE performance_schema . setup_consumers SET ENABLED = 'YES' WHERE NAME LIKE '%statements%' ; Transactions MariaDB doesn\u2019t implement queries history for transactions. All queries executed within a transaction won\u2019t have query examples since PMM relies on the performance_schema.events_statements_history to grab the query example but that table won\u2019t have any query executed as part of a transaction. This behavior is because MariaDB doesn\u2019t implement these consumers: events_transactions_current events_transactions_history events_transactions_history_long Query response time \u00b6 Query time distribution is a chart in the Details tab of Query Analytics showing the proportion of query time spent on various activities. It is enabled with the query_response_time_stats variable and associated plugins. Applicable versions \u00b6 Server Versions Percona Server for MySQL 5.7 ( not Percona Server for MySQL 8.0 .) MariaDB 10.0.4 Set this variable to see query time distribution charts. Variable Value Description query_response_time_stats ON Report query response time distributions . (Requires plugin installation. See below.) Configuration file. query_response_time_stats = ON You must also install the plugins. Session. Check that /usr/lib/mysql/plugin/query_response_time.so exists. Install the plugins and activate. For MariaDB 10.3 : INSTALL PLUGIN QUERY_RESPONSE_TIME_AUDIT SONAME 'query_response_time.so' ; INSTALL PLUGIN QUERY_RESPONSE_TIME SONAME 'query_response_time.so' ; SET GLOBAL query_response_time_stats = ON ; For Percona Server for MySQL 5.7 : INSTALL PLUGIN QUERY_RESPONSE_TIME_AUDIT SONAME 'query_response_time.so' ; INSTALL PLUGIN QUERY_RESPONSE_TIME SONAME 'query_response_time.so' ; INSTALL PLUGIN QUERY_RESPONSE_TIME_READ SONAME 'query_response_time.so' ; INSTALL PLUGIN QUERY_RESPONSE_TIME_WRITE SONAME 'query_response_time.so' ; SET GLOBAL query_response_time_stats = ON ; Tablestats \u00b6 Some table metrics are automatically disabled when the number of tables exceeds a default limit of 1000 tables. This prevents PMM Client from affecting the performance of your database server. The limit can be changed when adding a service on the command line with the two pmm-admin options: pmm-admin option Description --disable-tablestats Disables tablestats collection when the default limit is reached. --disable-tablestats-limit=N Sets the number of tables ( N ) for which tablestats collection is disabled. 0 means no limit. A negative number means tablestats is completely disabled (for any number of tables). User statistics \u00b6 Applicable versions \u00b6 User activity, individual table and index access details are shown on the MySQL User Details dashboard when the userstat variable is set. Server Versions Percona Server for MySQL 5.6, 5.7, 8.0 Percona XtraDB Cluster 5.6, 5.7, 8.0 MariaDB 5.2.0+ Examples \u00b6 Configuration file. userstat = ON Session. SET GLOBAL userstat = ON ; Add service \u00b6 When you have configured your database server, you can add a MySQL service with the user interface or on the command line. When adding a service with the command line, you must use the pmm-admin --query-source=SOURCE option to match the source you\u2019ve chosen and configured the database server for. With the PMM user interface, you select Use performance schema , or deselect it to use slow query log . With the user interface \u00b6 Select Configuration \u2192 PMM Inventory \u2192 Add Instance . Select MySQL \u2013 Add a remote instance . Enter or select values for the fields. Click Add service . If your MySQL instance is configured to use TLS, click on the Use TLS for database connections check box and fill in your TLS certificates and key. On the command line \u00b6 Add the database server as a service using one of these example commands. If successful, PMM Client will print MySQL Service added with the service\u2019s ID and name. Use the --environment and -custom-labels options to set tags for the service to help identify them. Examples \u00b6 TLS connection \u00b6 pmm-admin add mysql --username = user --password = pass --tls --tls-skip-verify --tls-ca = pathtoca.pem --tls-cert = pathtocert.pem --tls-key = pathtocertkey.pem --server-url = http://admin:admin@127.0.0.1 --query-source = perfschema name localhost:3306 Slow query log \u00b6 Default query source ( slowlog ), service name ( {node name}-mysql ), and service address/port ( 127.0.0.1:3306 ), with database server account pmm and password pass . pmm-admin add mysql --username = pmm --password = pass Slow query log source and log size limit (1 gigabyte), service name ( MYSQL_NODE ) and service address/port ( 191.168.1.123:3306 ). pmm-admin add mysql --query-source = slowlog --size-slow-logs = 1GiB --username = pmm --password = pass MYSQL_NODE 192 .168.1.123:3306 Slow query log source, disabled log management (use logrotate or some other log management tool), service name ( MYSQL_NODE ) and service address/port ( 191.168.1.123:3306 ). pmm-admin add mysql --query-source = slowlog --size-slow-logs = -1GiB --username = pmm --password = pass MYSQL_NODE 192 .168.1.123:3306 Default query source ( slowlog ), service name ( {node}-mysql ), connect via socket. pmm-admin add mysql --username = pmm --password = pass --socket = /var/run/mysqld/mysqld.sock Performance Schema \u00b6 Performance schema query source, service name ( MYSQL_NODE ) and default service address/port ( 127.0.0.1:3306 ). pmm-admin add mysql --query-source = perfschema --username = pmm --password = pass MYSQL_NODE Performance schema query source, service name ( MYSQL_NODE ) and default service address/port ( 127.0.0.1:3306 ) specified with flags. pmm-admin add mysql --query-source = perfschema --username = pmm --password = pass --service-name = MYSQL_NODE --host = 127 .0.0.1 --port = 3306 Identifying services \u00b6 Default query source ( slowlog ), environment labeled test , custom labels setting source to slowlog . (This example uses positional parameters for service name and service address.) pmm-admin add mysql --environment = test --custom-labels = 'source=slowlog' --username = root --password = password --query-source = slowlog MySQLSlowLog localhost:3306 Check the service \u00b6 PMM user interface \u00b6 Select Configuration \u2192 PMM Inventory \u2192 Inventory list . Look in the Services tab for a matching Service Type (MySQL), Service name , Addresses , and any other details entered in the form. Look in the Agents tab to check the desired data source is being used. Command line \u00b6 Look for your service in the output of this command. pmm-admin inventory list services --service-type = mysql Check data \u00b6 Open the MySQL Instance Summary dashboard. Set the Service Name to the newly-added service. Percona Server for MySQL, MariaDB \u00b6 If query response time plugin was installed, check for data in the MySQL Query Response Time Details dashboard or select a query in PMM Query Analytics to see the Query time distribution bar. Percona XtraDB Cluster \u00b6 Open the PXC/Galera Cluster Summary dashboard . See also Percona Server for MySQL \u2013 Slow Query Log Extended Percona Server for MySQL \u2013 User Statistics MariaDB \u2013 Slow Query Log Overview MariaDB \u2013 Slow Query Log Extended Statistics MariaDB \u2013 User Statistics Percona Blog \u2013 PERFORMANCE_SCHEMA vs Slow Query Log Percona Blog \u2013 MySQL\u2019s INNODB_METRICS table Percona Blog \u2013 Rotating MySQL Slow Logs Safely Percona Blog \u2013 Impact of logging on MySQL\u2019s performance Percona Blog \u2013 Running Custom MySQL Queries in Percona Monitoring and Management","title":"MySQL"},{"location":"setting-up/client/mysql.html#mysql","text":"How to set up PMM to monitor a MySQL or MySQL-based database instance. PMM Client collects metrics from MySQL , Percona Server for MySQL , Percona XtraDB Cluster , and MariaDB . (Amazon RDS is also supported and explained in a separate section .) Summary Create PMM account and set permissions. Choose a data source: Slow query log, or, Performance Schema. Configure: Query response time, Tablestats, User statistics. Add service. Check service.","title":"MySQL"},{"location":"setting-up/client/mysql.html#before-you-start","text":"Check that: PMM Server is installed and running with a known IP address accessible from the client node. PMM Client is installed and the node is registered with PMM Server . You have superuser (root) access on the client host.","title":"Before you start"},{"location":"setting-up/client/mysql.html#create-a-database-account-for-pmm","text":"It is good practice to use a non-superuser account to connect PMM Client to the monitored database instance. This example creates a database user with name pmm , password pass , and the necessary permissions. On MySQL 8.0 CREATE USER 'pmm' @ 'localhost' IDENTIFIED BY 'pass' WITH MAX_USER_CONNECTIONS 10 ; GRANT SELECT , PROCESS , REPLICATION CLIENT , RELOAD , BACKUP_ADMIN ON * . * TO 'pmm' @ 'localhost' ; On MySQL 5.7 CREATE USER 'pmm' @ 'localhost' IDENTIFIED BY 'pass' WITH MAX_USER_CONNECTIONS 10 ; GRANT SELECT , PROCESS , REPLICATION CLIENT , RELOAD ON * . * TO 'pmm' @ 'localhost' ;","title":"Create a database account for PMM"},{"location":"setting-up/client/mysql.html#choose-and-configure-a-source","text":"Decide which source of metrics to use, and configure your database server for it. The choices are Slow query log and Performance Schema . While you can use both at the same time we recommend using only one\u2013there is some overlap in the data reported, and each incurs a small performance penalty. The choice depends on the version and variant of your MySQL instance, and how much detail you want to see. Here are the benefits and drawbacks of Slow query log and Performance Schema metrics sources. Benefits Drawbacks Slow query log 1. More detail. 2. Lower resource impact (with query sampling feature in Percona Server for MySQL). 1. PMM Client must be on same host as database server or have access to slow query log. 2. Log files grow and must be actively managed. Performance Schema 1. Faster parsing. 2. Enabled by default on newer versions of MySQL. 1. Less detail.","title":"Choose and configure a source"},{"location":"setting-up/client/mysql.html#data-source-recommendations","text":"Database server Versions Recommended source MySQL 5.1-5.5 Slow query log MySQL 5.6+ Performance Schema MariaDB 10.0+ Performance Schema Percona Server for MySQL 5.7, 8.0 Slow query log Percona XtraDB Cluster 5.6, 5.7, 8.0 Slow query log","title":"Data source recommendations"},{"location":"setting-up/client/mysql.html#slow-query-log","text":"This section covers how to configure a MySQL-based database server to use the slow query log as a source of metrics.","title":"Slow query log"},{"location":"setting-up/client/mysql.html#applicable-versions","text":"Server Versions MySQL 5.1-5.5 MariaDB 10.1.2+ Percona Server for MySQL 5.7.10+, 8.0.12+ Percona XtraDB Cluster 5.6, 5.7, 8.0 The slow query log records the details of queries that take more than a certain amount of time to complete. With the database server configured to write this information to a file rather than a table, PMM Client parses the file and sends aggregated data to PMM Server via the Query Analytics part of PMM Agent.","title":"Applicable versions"},{"location":"setting-up/client/mysql.html#settings","text":"Variable Value Description slow_query_log ON Enables the slow query log. log_output 'FILE' Ensures the log is sent to a file. (This is the default on MariaDB.) long_query_time 0 The slow query threshold in seconds. In heavily-loaded applications, many quick queries can affect performance more than a few slow ones. Setting this value to 0 ensures all queries are captured. log_slow_admin_statements ON Includes the logging of slow administrative statements. log_slow_slave_statements ON Enables logging for queries that have taken more than long_query_time seconds to execute on the replica.","title":"Settings"},{"location":"setting-up/client/mysql.html#examples","text":"Configuration file. slow_query_log = ON log_output = FILE long_query_time = 0 log_slow_admin_statements = ON log_slow_slave_statements = ON Session. SET GLOBAL slow_query_log = 1 ; SET GLOBAL log_output = 'FILE' ; SET GLOBAL long_query_time = 0 ; SET GLOBAL log_slow_admin_statements = 1 ; SET GLOBAL log_slow_slave_statements = 1 ;","title":"Examples"},{"location":"setting-up/client/mysql.html#slow-query-log-extended","text":"Some MySQL-based database servers support extended slow query log variables.","title":"Slow query log -- extended"},{"location":"setting-up/client/mysql.html#applicable-versions_1","text":"Server Versions Percona Server for MySQL 5.7.10+, 8.0.12+ Percona XtraDB Cluster 5.6, 5.7, 8.0 MariaDB 10.0","title":"Applicable versions"},{"location":"setting-up/client/mysql.html#settings_1","text":"Variable Value Description log_slow_rate_limit 100 Defines the rate of queries captured by the slow query log . A good rule of thumb is 100 queries logged per second. For example, if your Percona Server instance processes 10,000 queries per second, you should set log_slow_rate_limit to 100 and capture every 100 th query for the slow query log . Depending on the amount of traffic, logging could become aggressive and resource consuming. This variable throttles the level of intensity of the data capture without compromising information. log_slow_rate_type \u2018query\u2019 Set so that it applies to queries, rather than sessions. slow_query_log_always_write_time 1 Specifies which queries should ignore sampling. With query sampling this ensures that queries with longer execution time will always be captured by the slow query log, avoiding the possibility that infrequent slow queries might not get captured at all. log_slow_verbosity \u2018full\u2019 Ensures that all information about each captured query is stored in the slow query log. slow_query_log_use_global_control \u2018all\u2019 Configure the slow query log during runtime and apply these settings to existing connections. (By default, slow query log settings apply only to new sessions.)","title":"Settings"},{"location":"setting-up/client/mysql.html#examples_1","text":"Configuration file (Percona Server for MySQL, Percona XtraDB Cluster). log_slow_rate_limit = 100 log_slow_rate_type = 'query' slow_query_log_always_write_time = 1 log_slow_verbosity = 'full' slow_query_log_use_global_control = 'all' Configuration file (MariaDB). log_slow_rate_limit = 100 Session (Percona Server for MySQL, Percona XtraDB Cluster). SET GLOBAL log_slow_rate_limit = 100 ; SET GLOBAL log_slow_rate_type = 'query' ; SET GLOBAL slow_query_log_always_write_time = 1 ; SET GLOBAL log_slow_verbosity = 'full' ; SET GLOBAL slow_query_log_use_global_control = 'all' ;","title":"Examples"},{"location":"setting-up/client/mysql.html#slow-query-log-rotation","text":"Slow query log files can grow quickly and must be managed. When adding a service with the command line use the pmm-admin option --size-slow-logs to set at what size the slow query log file is rotated. (The size is specified as a number with a suffix. See pmm-admin add mysql .) When the limit is reached, PMM Client will: remove the previous .old slow log file, rename the current file by adding the suffix .old , execute the MySQL command FLUSH LOGS . Only one .old file is kept. Older ones are deleted. You can manage log rotation yourself, for example, with logrotate . If you do, you can disable PMM Client\u2019s log rotation by providing a negative value to --size-slow-logs option when adding a service with pmm-admin add .","title":"Slow query log rotation"},{"location":"setting-up/client/mysql.html#performance-schema","text":"This section covers how to configure a MySQL-based database server to use Performance Schema as a source of metrics.","title":"Performance Schema"},{"location":"setting-up/client/mysql.html#applicable-versions_2","text":"Server Versions Percona Server for MySQL 5.6, 5.7, 8.0 Percona XtraDB Cluster 5.6, 5.7, 8.0 MariaDB 10.3+ PMM\u2019s MySQL Performance Schema Details dashboard charts the various performance_schema metrics. To use Performance Schema , set these variables. Variable Value Description performance_schema ON Enables Performance Schema metrics. This is the default in MySQL 5.6.6 and higher. performance-schema-instrument 'statement/%=ON' Configures Performance Schema instruments. performance-schema-consumer-statements-digest ON Configures the statements-digest consumer. innodb_monitor_enable all Enables InnoDB metrics counters.","title":"Applicable versions"},{"location":"setting-up/client/mysql.html#examples_2","text":"Configuration file. performance_schema = ON performance-schema-instrument = 'statement/%=ON' performance-schema-consumer-statements-digest = ON innodb_monitor_enable = all Session. ( performance_schema cannot be set in a session and must be set at server start-up.) UPDATE performance_schema . setup_consumers SET ENABLED = 'YES' WHERE NAME LIKE '%statements%' ; SET GLOBAL innodb_monitor_enable = all ;","title":"Examples"},{"location":"setting-up/client/mysql.html#mariadb-1057-or-lower","text":"There is no Explain or Example data shown by default in Query Analytics when monitoring MariaDB instances version 10.5.7 or lower. A workaround is to set this variable. Variable Value Description performance_schema.setup_instruments 'statement/%' List of instrumented object classes. Session. UPDATE performance_schema . setup_instruments SET ENABLED = 'YES' , TIMED = 'YES' WHERE NAME LIKE 'statement/%' ; UPDATE performance_schema . setup_consumers SET ENABLED = 'YES' WHERE NAME LIKE '%statements%' ; Transactions MariaDB doesn\u2019t implement queries history for transactions. All queries executed within a transaction won\u2019t have query examples since PMM relies on the performance_schema.events_statements_history to grab the query example but that table won\u2019t have any query executed as part of a transaction. This behavior is because MariaDB doesn\u2019t implement these consumers: events_transactions_current events_transactions_history events_transactions_history_long","title":"MariaDB 10.5.7 or lower"},{"location":"setting-up/client/mysql.html#query-response-time","text":"Query time distribution is a chart in the Details tab of Query Analytics showing the proportion of query time spent on various activities. It is enabled with the query_response_time_stats variable and associated plugins.","title":"Query response time"},{"location":"setting-up/client/mysql.html#applicable-versions_3","text":"Server Versions Percona Server for MySQL 5.7 ( not Percona Server for MySQL 8.0 .) MariaDB 10.0.4 Set this variable to see query time distribution charts. Variable Value Description query_response_time_stats ON Report query response time distributions . (Requires plugin installation. See below.) Configuration file. query_response_time_stats = ON You must also install the plugins. Session. Check that /usr/lib/mysql/plugin/query_response_time.so exists. Install the plugins and activate. For MariaDB 10.3 : INSTALL PLUGIN QUERY_RESPONSE_TIME_AUDIT SONAME 'query_response_time.so' ; INSTALL PLUGIN QUERY_RESPONSE_TIME SONAME 'query_response_time.so' ; SET GLOBAL query_response_time_stats = ON ; For Percona Server for MySQL 5.7 : INSTALL PLUGIN QUERY_RESPONSE_TIME_AUDIT SONAME 'query_response_time.so' ; INSTALL PLUGIN QUERY_RESPONSE_TIME SONAME 'query_response_time.so' ; INSTALL PLUGIN QUERY_RESPONSE_TIME_READ SONAME 'query_response_time.so' ; INSTALL PLUGIN QUERY_RESPONSE_TIME_WRITE SONAME 'query_response_time.so' ; SET GLOBAL query_response_time_stats = ON ;","title":"Applicable versions"},{"location":"setting-up/client/mysql.html#tablestats","text":"Some table metrics are automatically disabled when the number of tables exceeds a default limit of 1000 tables. This prevents PMM Client from affecting the performance of your database server. The limit can be changed when adding a service on the command line with the two pmm-admin options: pmm-admin option Description --disable-tablestats Disables tablestats collection when the default limit is reached. --disable-tablestats-limit=N Sets the number of tables ( N ) for which tablestats collection is disabled. 0 means no limit. A negative number means tablestats is completely disabled (for any number of tables).","title":"Tablestats"},{"location":"setting-up/client/mysql.html#user-statistics","text":"","title":"User statistics"},{"location":"setting-up/client/mysql.html#applicable-versions_4","text":"User activity, individual table and index access details are shown on the MySQL User Details dashboard when the userstat variable is set. Server Versions Percona Server for MySQL 5.6, 5.7, 8.0 Percona XtraDB Cluster 5.6, 5.7, 8.0 MariaDB 5.2.0+","title":"Applicable versions"},{"location":"setting-up/client/mysql.html#examples_3","text":"Configuration file. userstat = ON Session. SET GLOBAL userstat = ON ;","title":"Examples"},{"location":"setting-up/client/mysql.html#add-service","text":"When you have configured your database server, you can add a MySQL service with the user interface or on the command line. When adding a service with the command line, you must use the pmm-admin --query-source=SOURCE option to match the source you\u2019ve chosen and configured the database server for. With the PMM user interface, you select Use performance schema , or deselect it to use slow query log .","title":"Add service"},{"location":"setting-up/client/mysql.html#with-the-user-interface","text":"Select Configuration \u2192 PMM Inventory \u2192 Add Instance . Select MySQL \u2013 Add a remote instance . Enter or select values for the fields. Click Add service . If your MySQL instance is configured to use TLS, click on the Use TLS for database connections check box and fill in your TLS certificates and key.","title":"With the user interface"},{"location":"setting-up/client/mysql.html#on-the-command-line","text":"Add the database server as a service using one of these example commands. If successful, PMM Client will print MySQL Service added with the service\u2019s ID and name. Use the --environment and -custom-labels options to set tags for the service to help identify them.","title":"On the command line"},{"location":"setting-up/client/mysql.html#examples_4","text":"","title":"Examples"},{"location":"setting-up/client/mysql.html#tls-connection","text":"pmm-admin add mysql --username = user --password = pass --tls --tls-skip-verify --tls-ca = pathtoca.pem --tls-cert = pathtocert.pem --tls-key = pathtocertkey.pem --server-url = http://admin:admin@127.0.0.1 --query-source = perfschema name localhost:3306","title":"TLS connection"},{"location":"setting-up/client/mysql.html#slow-query-log_1","text":"Default query source ( slowlog ), service name ( {node name}-mysql ), and service address/port ( 127.0.0.1:3306 ), with database server account pmm and password pass . pmm-admin add mysql --username = pmm --password = pass Slow query log source and log size limit (1 gigabyte), service name ( MYSQL_NODE ) and service address/port ( 191.168.1.123:3306 ). pmm-admin add mysql --query-source = slowlog --size-slow-logs = 1GiB --username = pmm --password = pass MYSQL_NODE 192 .168.1.123:3306 Slow query log source, disabled log management (use logrotate or some other log management tool), service name ( MYSQL_NODE ) and service address/port ( 191.168.1.123:3306 ). pmm-admin add mysql --query-source = slowlog --size-slow-logs = -1GiB --username = pmm --password = pass MYSQL_NODE 192 .168.1.123:3306 Default query source ( slowlog ), service name ( {node}-mysql ), connect via socket. pmm-admin add mysql --username = pmm --password = pass --socket = /var/run/mysqld/mysqld.sock","title":"Slow query log"},{"location":"setting-up/client/mysql.html#performance-schema_1","text":"Performance schema query source, service name ( MYSQL_NODE ) and default service address/port ( 127.0.0.1:3306 ). pmm-admin add mysql --query-source = perfschema --username = pmm --password = pass MYSQL_NODE Performance schema query source, service name ( MYSQL_NODE ) and default service address/port ( 127.0.0.1:3306 ) specified with flags. pmm-admin add mysql --query-source = perfschema --username = pmm --password = pass --service-name = MYSQL_NODE --host = 127 .0.0.1 --port = 3306","title":"Performance Schema"},{"location":"setting-up/client/mysql.html#identifying-services","text":"Default query source ( slowlog ), environment labeled test , custom labels setting source to slowlog . (This example uses positional parameters for service name and service address.) pmm-admin add mysql --environment = test --custom-labels = 'source=slowlog' --username = root --password = password --query-source = slowlog MySQLSlowLog localhost:3306","title":"Identifying services"},{"location":"setting-up/client/mysql.html#check-the-service","text":"","title":"Check the service"},{"location":"setting-up/client/mysql.html#pmm-user-interface","text":"Select Configuration \u2192 PMM Inventory \u2192 Inventory list . Look in the Services tab for a matching Service Type (MySQL), Service name , Addresses , and any other details entered in the form. Look in the Agents tab to check the desired data source is being used.","title":"PMM user interface"},{"location":"setting-up/client/mysql.html#command-line","text":"Look for your service in the output of this command. pmm-admin inventory list services --service-type = mysql","title":"Command line"},{"location":"setting-up/client/mysql.html#check-data","text":"Open the MySQL Instance Summary dashboard. Set the Service Name to the newly-added service.","title":"Check data"},{"location":"setting-up/client/mysql.html#percona-server-for-mysql-mariadb","text":"If query response time plugin was installed, check for data in the MySQL Query Response Time Details dashboard or select a query in PMM Query Analytics to see the Query time distribution bar.","title":"Percona Server for MySQL, MariaDB"},{"location":"setting-up/client/mysql.html#percona-xtradb-cluster","text":"Open the PXC/Galera Cluster Summary dashboard . See also Percona Server for MySQL \u2013 Slow Query Log Extended Percona Server for MySQL \u2013 User Statistics MariaDB \u2013 Slow Query Log Overview MariaDB \u2013 Slow Query Log Extended Statistics MariaDB \u2013 User Statistics Percona Blog \u2013 PERFORMANCE_SCHEMA vs Slow Query Log Percona Blog \u2013 MySQL\u2019s INNODB_METRICS table Percona Blog \u2013 Rotating MySQL Slow Logs Safely Percona Blog \u2013 Impact of logging on MySQL\u2019s performance Percona Blog \u2013 Running Custom MySQL Queries in Percona Monitoring and Management","title":"Percona XtraDB Cluster"},{"location":"setting-up/client/postgresql.html","text":"PostgreSQL \u00b6 How to set up PMM to monitor a PostgreSQL or Percona Distribution for PostgreSQL database instance. Summary Create PMM account and set permissions. Choose, install and configure an extension: pg_stat_statements , or, pg_stat_monitor . Add service. Check service. Before you start \u00b6 Check that: PMM Server is installed and running with a known IP address accessible from the client node. PMM Client is installed and the node is registered with PMM Server . You have superuser (root) access on the client host. You have superuser access to any database servers that you want to monitor. (PMM follows PostgreSQL\u2019s end-of-life policy . For specific details on supported platforms and versions, see Percona\u2019s Software Platform Lifecycle page .) Create a database account for PMM \u00b6 We recommend creating a PMM database account that can connect to the postgres database with the SUPERUSER role. Create a user. This example uses pmm . (Replace ****** with a strong password of your choice.) CREATE USER pmm WITH SUPERUSER ENCRYPTED PASSWORD '******' ; If your database runs on Amazon RDS: CREATE USER pmm WITH rds_superuser ENCRYPTED PASSWORD '******' ; Optionally, you can also set up a connection limit (only if the user is not a SUPERUSER): ALTER USER pmm CONNECTION LIMIT 10 ; PMM must be able to log in locally as this user to the PostgreSQL instance. To enable this, edit the pg_hba.conf file. If not already enabled by an existing rule, add: local all pmm md5 # TYPE DATABASE USER ADDRESS METHOD (Ignore the second line. It is a comment to show field alignment.) Reload the configuration: su - postgres psql -c \"select pg_reload_conf()\" Check local login. psql postgres pmm -c \"\\conninfo\" Enter the password for the pmm user when prompted. Choose and configure an extension \u00b6 Decide which database extension to use, and configure your database server for it. The choices are: pg_stat_statements , the original extension created by PostgreSQL, part of the postgresql-contrib package available on Linux. pg_stat_monitor is a new extension created by Percona. It is based on and compatible with pg_stat_statements . pg_stat_monitor has all the features of pg_stat_statements , but adds bucket-based data aggregation . We recommend choosing only one of these. If you use both, you will get duplicate metrics. Caution While we recommend use of the newer pg_stat_monitor extension, be aware it is currently in beta phase and unsupported. Here are the benefits and drawbacks of each. Benefits Drawbacks pg_stat_statements 1. Part of official postgresql-contrib package. 1. No aggregated statistics or histograms. 2. No Query Examples. pg_stat_monitor 1. Builds on pg_stat_monitor features. 2. Bucket-based aggregation. 1. Beta software. Bucket-based data aggregation pg_stat_monitor collects statistics and aggregates data in a data collection unit called a bucket . These are linked together to form a bucket chain . You can specify: the number of buckets (the length of the chain); how much space is available for all buckets; a time limit for each bucket\u2019s data collection (the bucket expiry ). When a bucket\u2019s expiration time is reached, accumulated statistics are reset and data is stored in the next available bucket in the chain. When all buckets in the chain have been used, the first bucket is reused and its contents are overwritten. If a bucket fills before its expiration time is reached, data is discarded. pg_stat_statements \u00b6 Install \u00b6 Debian/Ubuntu Root permissions apt install -y postgresql-contrib Red Hat/CentOS Root permissions yum install -y postgresql-contrib Configure \u00b6 Add these lines to your postgresql.conf file: shared_preload_libraries = 'pg_stat_statements' track_activity_query_size = 2048 # Increase tracked query string size pg_stat_statements.track = all # Track all statements including nested track_io_timing = on # Capture read/write stats Restart the database server. After the restart, the extension starts capturing statistics from every database. Install the extension. psql postgres postgres -c \"CREATE EXTENSION pg_stat_statements SCHEMA public\" This command creates the view where you can access the collected statistics. We recommend that you create the extension for the postgres database. In this case, you receive access to the statistics collected from every database. You can now add the service . pg_stat_monitor \u00b6 Caution pg_stat_monitor is currently in beta phase and is unsupported. pg_stat_monitor has been tested with: PostgreSQL versions 11, 12, 13. Percona Distribution for PostgreSQL versions 11, 12, 13. Install \u00b6 If you use Percona Distribution for PostgreSQL , you can install the extension with your Linux package manager. See Installing Percona Distribution for PostgreSQL . If you use PostgreSQL you can install by downloading and compiling the source code. See Installing pg_stat_monitor . Configure \u00b6 Set or change the value for shared_preload_library . In your postgresql.conf file: shared_preload_libraries = 'pg_stat_monitor' Caution If you use both pg_stat_statements and pg_stat_monitor , set pg_stat_monitor after pg_stat_statements : shared_preload_libraries = 'pg_stat_statements, pg_stat_monitor' Set configuration values. In your postgresql.conf file: pg_stat_monitor.pgsm_query_max_len = 2048 Caution It is important to set maximal length of query to 2048 characters or more for PMM to work properly. You can get a list of other available settings with SELECT * FROM pg_stat_monitor_settings; . Another important parameter is: pg_stat_monitor.pgsm_normalized_query If the value is set to 1, the actual query values are replaced by placeholders. If the value is 0, the examples are given in QAN. Examples can be found in QAN details tab example. See pg_stat_monitor GitHub repository for details about available parameters. Start or restart your PostgreSQL instance. The extension starts capturing statistics from every database. In a psql session: CREATE EXTENSION pg_stat_monitor ; This command creates the view where you can access the collected statistics. We recommend that you create the extension for the postgres database. In this case, you receive the access to the statistics, collected from every database. Check the version. SELECT pg_stat_monitor_version (); Add service \u00b6 When you have configured your database server, you can add a PostgreSQL service with the user interface or on the command line. With the user interface \u00b6 Select Configuration \u2192 PMM Inventory \u2192 Add Instance . Select PostgreSQL \u2013 Add a remote instance . Enter or select values for the fields. Click Add service . If your PostgreSQL instance is configured to use TLS, click on the Use TLS for database connections check box and fill in your TLS certificates and key. Note For TLS connection to work SSL needs to be configured in your PostgreSQL instance. Make sure SSL is enabled in the server configuration file postgresql.conf , and that hosts are allowed to connect in the client authentication configuration file pg_hba.conf . (See PostgreSQL documentation on Secure TCP/IP Connections with SSL .) On the command line \u00b6 Add the database server as a service using one of these example commands. If successful, PMM Client will print PostgreSQL Service added with the service\u2019s ID and name. Use the --environment and -custom-labels options to set tags for the service to help identify them. Examples \u00b6 Add instance with default node ( <node>-postgresql ). pmm-admin add postgresql \\ --username = pmm \\ --password = password \\ --server-url = https://admin:admin@X.X.X.X:443 \\ --server-insecure-tls <user name> : The PostgreSQL PMM user <password> : The PostgreSQL user credentials. The service name will be automatically chosen. Add instance with specified service name. pmm-admin add postgresql \\ --username = pmm \\ --password = password \\ --server-url = https://admin:admin@X.X.X.X:443 \\ --server-insecure-tls \\ --service-name = SERVICE-NAME Add instance to connect with a UNIX socket. pmm-admin add postgresql --socket = /var/run/postgresql Connecting via SSL/TLS \u00b6 pmm-admin add postgresql --tls \\ --tls-cert-file = PATHTOCERT \\ --tls-ca-file = PATHTOCACERT \\ --tls-key-file = PATHTOKEY \\ --host = HOST \\ --port = PORT \\ --username = USER \\ --service-name = SERVICE-NAME where: PATHTOCERT : Path to client certificate file. PATHTOCACERT : Path to certificate authority file. PATHTOKEY : Path to client key file. HOST : Instance hostname or IP. PORT : PostgreSQL service port number. USER : Database user allowed to connect via TLS. Should match the common name (CN) used in the client certificate. SERVICE : Name to give to the service within PMM. Check the service \u00b6 Check service - PMM user interface \u00b6 Select Configuration \u2192 PMM Inventory \u2192 Inventory list . Look in the Services tab for a matching Service Type (PostgreSQL), Service name , Addresses , and any other details entered in the form. Look in the Agents tab to check the desired data source is being used. Check service - Command line \u00b6 Look for your service in the output of this command. pmm-admin inventory list services If using Docker, use docker exec pmm-client pmm-admin inventory list services Check data \u00b6 Open the PostgreSQL Instance Summary dashboard. Set the Service Name to the newly-added service. Running custom queries \u00b6 The Postgres exporter can run custom queries to add new metrics not provided by default. Those custom queries must be defined in the /usr/local/percona/pmm2/collectors/custom-queries/postgresql in the same host where the exporter is running. There are 3 directories inside it: - high-resolution/ - every 5 seconds - medium-resolution/ - every 10 seconds - low-resolution/ - every 60 seconds Depending on the desired resolution for your custom queries, you can place a file with the queries definition. The file is a yaml where each query can have these fields: query_name: query: the query definition master: boolean to specify if the query should be executed only in the master metrics: - metric name: usage: GAUGE, LABEL, COUNTER, MAPPEDMETRIC or DURATION description: a human readable description Example \u00b6 pg_postmaster_uptime: query: \u201cselect extract(epoch from current_timestamp - pg_postmaster_start_time()) as seconds\u201d master: true metrics: - seconds: usage: \u201cGAUGE\u201d description: \u201cService uptime\u201d Check the see also section for a more detailed description on MySQL custom queries with more examples about how to use custom queries in dashboards. See also pmm-admin man page for pmm-admin add postgresql Configuring Percona Repositories with percona-release Percona Blog \u2013 Running Custom MySQL Queries in Percona Monitoring and Management","title":"PostgreSQL"},{"location":"setting-up/client/postgresql.html#postgresql","text":"How to set up PMM to monitor a PostgreSQL or Percona Distribution for PostgreSQL database instance. Summary Create PMM account and set permissions. Choose, install and configure an extension: pg_stat_statements , or, pg_stat_monitor . Add service. Check service.","title":"PostgreSQL"},{"location":"setting-up/client/postgresql.html#before-you-start","text":"Check that: PMM Server is installed and running with a known IP address accessible from the client node. PMM Client is installed and the node is registered with PMM Server . You have superuser (root) access on the client host. You have superuser access to any database servers that you want to monitor. (PMM follows PostgreSQL\u2019s end-of-life policy . For specific details on supported platforms and versions, see Percona\u2019s Software Platform Lifecycle page .)","title":"Before you start"},{"location":"setting-up/client/postgresql.html#create-a-database-account-for-pmm","text":"We recommend creating a PMM database account that can connect to the postgres database with the SUPERUSER role. Create a user. This example uses pmm . (Replace ****** with a strong password of your choice.) CREATE USER pmm WITH SUPERUSER ENCRYPTED PASSWORD '******' ; If your database runs on Amazon RDS: CREATE USER pmm WITH rds_superuser ENCRYPTED PASSWORD '******' ; Optionally, you can also set up a connection limit (only if the user is not a SUPERUSER): ALTER USER pmm CONNECTION LIMIT 10 ; PMM must be able to log in locally as this user to the PostgreSQL instance. To enable this, edit the pg_hba.conf file. If not already enabled by an existing rule, add: local all pmm md5 # TYPE DATABASE USER ADDRESS METHOD (Ignore the second line. It is a comment to show field alignment.) Reload the configuration: su - postgres psql -c \"select pg_reload_conf()\" Check local login. psql postgres pmm -c \"\\conninfo\" Enter the password for the pmm user when prompted.","title":"Create a database account for PMM"},{"location":"setting-up/client/postgresql.html#choose-and-configure-an-extension","text":"Decide which database extension to use, and configure your database server for it. The choices are: pg_stat_statements , the original extension created by PostgreSQL, part of the postgresql-contrib package available on Linux. pg_stat_monitor is a new extension created by Percona. It is based on and compatible with pg_stat_statements . pg_stat_monitor has all the features of pg_stat_statements , but adds bucket-based data aggregation . We recommend choosing only one of these. If you use both, you will get duplicate metrics. Caution While we recommend use of the newer pg_stat_monitor extension, be aware it is currently in beta phase and unsupported. Here are the benefits and drawbacks of each. Benefits Drawbacks pg_stat_statements 1. Part of official postgresql-contrib package. 1. No aggregated statistics or histograms. 2. No Query Examples. pg_stat_monitor 1. Builds on pg_stat_monitor features. 2. Bucket-based aggregation. 1. Beta software. Bucket-based data aggregation pg_stat_monitor collects statistics and aggregates data in a data collection unit called a bucket . These are linked together to form a bucket chain . You can specify: the number of buckets (the length of the chain); how much space is available for all buckets; a time limit for each bucket\u2019s data collection (the bucket expiry ). When a bucket\u2019s expiration time is reached, accumulated statistics are reset and data is stored in the next available bucket in the chain. When all buckets in the chain have been used, the first bucket is reused and its contents are overwritten. If a bucket fills before its expiration time is reached, data is discarded.","title":"Choose and configure an extension"},{"location":"setting-up/client/postgresql.html#pg_stat_statements","text":"","title":"pg_stat_statements"},{"location":"setting-up/client/postgresql.html#install","text":"Debian/Ubuntu Root permissions apt install -y postgresql-contrib Red Hat/CentOS Root permissions yum install -y postgresql-contrib","title":"Install"},{"location":"setting-up/client/postgresql.html#configure","text":"Add these lines to your postgresql.conf file: shared_preload_libraries = 'pg_stat_statements' track_activity_query_size = 2048 # Increase tracked query string size pg_stat_statements.track = all # Track all statements including nested track_io_timing = on # Capture read/write stats Restart the database server. After the restart, the extension starts capturing statistics from every database. Install the extension. psql postgres postgres -c \"CREATE EXTENSION pg_stat_statements SCHEMA public\" This command creates the view where you can access the collected statistics. We recommend that you create the extension for the postgres database. In this case, you receive access to the statistics collected from every database. You can now add the service .","title":"Configure"},{"location":"setting-up/client/postgresql.html#pg_stat_monitor","text":"Caution pg_stat_monitor is currently in beta phase and is unsupported. pg_stat_monitor has been tested with: PostgreSQL versions 11, 12, 13. Percona Distribution for PostgreSQL versions 11, 12, 13.","title":"pg_stat_monitor"},{"location":"setting-up/client/postgresql.html#install_1","text":"If you use Percona Distribution for PostgreSQL , you can install the extension with your Linux package manager. See Installing Percona Distribution for PostgreSQL . If you use PostgreSQL you can install by downloading and compiling the source code. See Installing pg_stat_monitor .","title":"Install"},{"location":"setting-up/client/postgresql.html#configure_1","text":"Set or change the value for shared_preload_library . In your postgresql.conf file: shared_preload_libraries = 'pg_stat_monitor' Caution If you use both pg_stat_statements and pg_stat_monitor , set pg_stat_monitor after pg_stat_statements : shared_preload_libraries = 'pg_stat_statements, pg_stat_monitor' Set configuration values. In your postgresql.conf file: pg_stat_monitor.pgsm_query_max_len = 2048 Caution It is important to set maximal length of query to 2048 characters or more for PMM to work properly. You can get a list of other available settings with SELECT * FROM pg_stat_monitor_settings; . Another important parameter is: pg_stat_monitor.pgsm_normalized_query If the value is set to 1, the actual query values are replaced by placeholders. If the value is 0, the examples are given in QAN. Examples can be found in QAN details tab example. See pg_stat_monitor GitHub repository for details about available parameters. Start or restart your PostgreSQL instance. The extension starts capturing statistics from every database. In a psql session: CREATE EXTENSION pg_stat_monitor ; This command creates the view where you can access the collected statistics. We recommend that you create the extension for the postgres database. In this case, you receive the access to the statistics, collected from every database. Check the version. SELECT pg_stat_monitor_version ();","title":"Configure"},{"location":"setting-up/client/postgresql.html#add-service","text":"When you have configured your database server, you can add a PostgreSQL service with the user interface or on the command line.","title":"Add service"},{"location":"setting-up/client/postgresql.html#with-the-user-interface","text":"Select Configuration \u2192 PMM Inventory \u2192 Add Instance . Select PostgreSQL \u2013 Add a remote instance . Enter or select values for the fields. Click Add service . If your PostgreSQL instance is configured to use TLS, click on the Use TLS for database connections check box and fill in your TLS certificates and key. Note For TLS connection to work SSL needs to be configured in your PostgreSQL instance. Make sure SSL is enabled in the server configuration file postgresql.conf , and that hosts are allowed to connect in the client authentication configuration file pg_hba.conf . (See PostgreSQL documentation on Secure TCP/IP Connections with SSL .)","title":"With the user interface"},{"location":"setting-up/client/postgresql.html#on-the-command-line","text":"Add the database server as a service using one of these example commands. If successful, PMM Client will print PostgreSQL Service added with the service\u2019s ID and name. Use the --environment and -custom-labels options to set tags for the service to help identify them.","title":"On the command line"},{"location":"setting-up/client/postgresql.html#examples","text":"Add instance with default node ( <node>-postgresql ). pmm-admin add postgresql \\ --username = pmm \\ --password = password \\ --server-url = https://admin:admin@X.X.X.X:443 \\ --server-insecure-tls <user name> : The PostgreSQL PMM user <password> : The PostgreSQL user credentials. The service name will be automatically chosen. Add instance with specified service name. pmm-admin add postgresql \\ --username = pmm \\ --password = password \\ --server-url = https://admin:admin@X.X.X.X:443 \\ --server-insecure-tls \\ --service-name = SERVICE-NAME Add instance to connect with a UNIX socket. pmm-admin add postgresql --socket = /var/run/postgresql","title":"Examples"},{"location":"setting-up/client/postgresql.html#connecting-via-ssltls","text":"pmm-admin add postgresql --tls \\ --tls-cert-file = PATHTOCERT \\ --tls-ca-file = PATHTOCACERT \\ --tls-key-file = PATHTOKEY \\ --host = HOST \\ --port = PORT \\ --username = USER \\ --service-name = SERVICE-NAME where: PATHTOCERT : Path to client certificate file. PATHTOCACERT : Path to certificate authority file. PATHTOKEY : Path to client key file. HOST : Instance hostname or IP. PORT : PostgreSQL service port number. USER : Database user allowed to connect via TLS. Should match the common name (CN) used in the client certificate. SERVICE : Name to give to the service within PMM.","title":"Connecting via SSL/TLS"},{"location":"setting-up/client/postgresql.html#check-the-service","text":"","title":"Check the service"},{"location":"setting-up/client/postgresql.html#check-service-pmm-user-interface","text":"Select Configuration \u2192 PMM Inventory \u2192 Inventory list . Look in the Services tab for a matching Service Type (PostgreSQL), Service name , Addresses , and any other details entered in the form. Look in the Agents tab to check the desired data source is being used.","title":"Check service - PMM user interface"},{"location":"setting-up/client/postgresql.html#check-service-command-line","text":"Look for your service in the output of this command. pmm-admin inventory list services If using Docker, use docker exec pmm-client pmm-admin inventory list services","title":"Check service - Command line"},{"location":"setting-up/client/postgresql.html#check-data","text":"Open the PostgreSQL Instance Summary dashboard. Set the Service Name to the newly-added service.","title":"Check data"},{"location":"setting-up/client/postgresql.html#running-custom-queries","text":"The Postgres exporter can run custom queries to add new metrics not provided by default. Those custom queries must be defined in the /usr/local/percona/pmm2/collectors/custom-queries/postgresql in the same host where the exporter is running. There are 3 directories inside it: - high-resolution/ - every 5 seconds - medium-resolution/ - every 10 seconds - low-resolution/ - every 60 seconds Depending on the desired resolution for your custom queries, you can place a file with the queries definition. The file is a yaml where each query can have these fields: query_name: query: the query definition master: boolean to specify if the query should be executed only in the master metrics: - metric name: usage: GAUGE, LABEL, COUNTER, MAPPEDMETRIC or DURATION description: a human readable description","title":"Running custom queries"},{"location":"setting-up/client/postgresql.html#example","text":"pg_postmaster_uptime: query: \u201cselect extract(epoch from current_timestamp - pg_postmaster_start_time()) as seconds\u201d master: true metrics: - seconds: usage: \u201cGAUGE\u201d description: \u201cService uptime\u201d Check the see also section for a more detailed description on MySQL custom queries with more examples about how to use custom queries in dashboards. See also pmm-admin man page for pmm-admin add postgresql Configuring Percona Repositories with percona-release Percona Blog \u2013 Running Custom MySQL Queries in Percona Monitoring and Management","title":"Example"},{"location":"setting-up/client/proxysql.html","text":"ProxySQL \u00b6 Use the proxysql alias to enable ProxySQL performance metrics monitoring. USAGE \u00b6 pmm-admin add proxysql --username = pmm --password = pmm where username and password are credentials for the administration interface of the monitored ProxySQL instance. You should configure a read-only account for monitoring using the admin-stats_credentials variable in ProxySQL Additionally, two positional arguments can be appended to the command line flags: a service name to be used by PMM, and a service address. If not specified, they are substituted automatically as <node>-proxysql and 127.0.0.1:6032 . The output of this command may look as follows: pmm-admin add proxysql --username = pmm --password = pmm ProxySQL Service added. Service ID : /service_id/f69df379-6584-4db5-a896-f35ae8c97573 Service name: ubuntu-proxysql Beside positional arguments shown above you can specify service name and service address with the following flags: --service-name , and --host (the hostname or IP address of the service) and --port (the port number of the service), or --socket (the UNIX socket path). If both flag and positional argument are present, flag gains higher priority. Here is the previous example modified to use these flags for both host/port or socket connections: pmm-admin add proxysql --username = pmm --password = pmm --service-name = my-new-proxysql --host = 127 .0.0.1 --port = 6032 pmm-admin add proxysql --username = pmm --password = pmm --service-name = my-new-proxysql --socket = /tmp/proxysql_admin.sock","title":"ProxySQL"},{"location":"setting-up/client/proxysql.html#proxysql","text":"Use the proxysql alias to enable ProxySQL performance metrics monitoring.","title":"ProxySQL"},{"location":"setting-up/client/proxysql.html#usage","text":"pmm-admin add proxysql --username = pmm --password = pmm where username and password are credentials for the administration interface of the monitored ProxySQL instance. You should configure a read-only account for monitoring using the admin-stats_credentials variable in ProxySQL Additionally, two positional arguments can be appended to the command line flags: a service name to be used by PMM, and a service address. If not specified, they are substituted automatically as <node>-proxysql and 127.0.0.1:6032 . The output of this command may look as follows: pmm-admin add proxysql --username = pmm --password = pmm ProxySQL Service added. Service ID : /service_id/f69df379-6584-4db5-a896-f35ae8c97573 Service name: ubuntu-proxysql Beside positional arguments shown above you can specify service name and service address with the following flags: --service-name , and --host (the hostname or IP address of the service) and --port (the port number of the service), or --socket (the UNIX socket path). If both flag and positional argument are present, flag gains higher priority. Here is the previous example modified to use these flags for both host/port or socket connections: pmm-admin add proxysql --username = pmm --password = pmm --service-name = my-new-proxysql --host = 127 .0.0.1 --port = 6032 pmm-admin add proxysql --username = pmm --password = pmm --service-name = my-new-proxysql --socket = /tmp/proxysql_admin.sock","title":"USAGE"},{"location":"setting-up/client/remote.html","text":"Remote instances \u00b6 Recommended settings \u00b6 When monitoring remote instances including RDS and Google instances, network latency might affect the scrape process and throw timeout errors. For this reason, it is recommended to lower the metrics resolution . Starting with PMM 2.18, the scrape timeout has been updated according to the following rules: For resolutions <= 2 seconds, scrape timeout is 1 second. For resolutions <= 10 seconds, timeout is set to resolution minus 1 second. For example, for 10 second resolution, timeout will be set at 9 seconds. For lower resolutions (values > 10 seconds), the scrape timeout is set to 90% of the resolution time. For example, for 60 second resolution, the scrape timeout will be set to 54 seconds. How to check for scrape timeouts \u00b6 Sometimes it is hard to check if you are using the correct values to scrape or if there some other reason why there is no data in a dashboard even when the instance has been added correctly and the agent is running. One additional step you can do is to check for scrape target statuses. Browse to http://<your-pmm-server-address>/prometheus/targets and then click on the Unhealthy button. The page will show only agents having issues while scrapping and the scrape result including the error messages. In the example here, there is a message that says: context deadline exceeded and the scrape duration column says the scrape took 10 seconds; this means that the exporter didn\u2019t respond in the 10 seconds the scrape process was allowed to run due to the configured metric resolutions and their timeouts. In this case, we can lower the metric resolutions increasing these values as shown in the image below.","title":"Remote instances"},{"location":"setting-up/client/remote.html#remote-instances","text":"","title":"Remote instances"},{"location":"setting-up/client/remote.html#recommended-settings","text":"When monitoring remote instances including RDS and Google instances, network latency might affect the scrape process and throw timeout errors. For this reason, it is recommended to lower the metrics resolution . Starting with PMM 2.18, the scrape timeout has been updated according to the following rules: For resolutions <= 2 seconds, scrape timeout is 1 second. For resolutions <= 10 seconds, timeout is set to resolution minus 1 second. For example, for 10 second resolution, timeout will be set at 9 seconds. For lower resolutions (values > 10 seconds), the scrape timeout is set to 90% of the resolution time. For example, for 60 second resolution, the scrape timeout will be set to 54 seconds.","title":"Recommended settings"},{"location":"setting-up/client/remote.html#how-to-check-for-scrape-timeouts","text":"Sometimes it is hard to check if you are using the correct values to scrape or if there some other reason why there is no data in a dashboard even when the instance has been added correctly and the agent is running. One additional step you can do is to check for scrape target statuses. Browse to http://<your-pmm-server-address>/prometheus/targets and then click on the Unhealthy button. The page will show only agents having issues while scrapping and the scrape result including the error messages. In the example here, there is a message that says: context deadline exceeded and the scrape duration column says the scrape took 10 seconds; this means that the exporter didn\u2019t respond in the 10 seconds the scrape process was allowed to run due to the configured metric resolutions and their timeouts. In this case, we can lower the metric resolutions increasing these values as shown in the image below.","title":"How to check for scrape timeouts"},{"location":"setting-up/server/index.html","text":"Set up PMM Server \u00b6 Check system requirements. Disk Approximately 1 GB of storage per monitored database node with data retention set to one week. By default, retention is 30 days. Tip Disable table statistics to decrease the VictoriaMetrics database size. Memory A minimum of 2 GB per monitored database node. The increase in memory usage is not proportional to the number of nodes. For example, data from 20 nodes should be easily handled with 16 GB. Architecture Your CPU must support the SSE4.2 instruction set, a requirement of ClickHouse, a third-party column-oriented database used by Query Analytics. If your CPU is lacking this instruction set you won\u2019t be able to use Query Analytics. Configure your network . Decide how you want to run PMM Server. Choose from: Docker ; Podman ; Helm ; Virtual appliance ; Amazon AWS ; Use the easy install script. Authenticating using API keys. While adding clients to the PMM server, you use the admin user. However, if you change the password for the admin user from the PMM UI, then the clients will not be able to access PMM. Also, due to multiple unsuccessful login attempts Grafana will lock out the admin user. The solution is to use API key for authentication. You can use API keys as a replacement for basic authentication.","title":"Server"},{"location":"setting-up/server/index.html#set-up-pmm-server","text":"Check system requirements. Disk Approximately 1 GB of storage per monitored database node with data retention set to one week. By default, retention is 30 days. Tip Disable table statistics to decrease the VictoriaMetrics database size. Memory A minimum of 2 GB per monitored database node. The increase in memory usage is not proportional to the number of nodes. For example, data from 20 nodes should be easily handled with 16 GB. Architecture Your CPU must support the SSE4.2 instruction set, a requirement of ClickHouse, a third-party column-oriented database used by Query Analytics. If your CPU is lacking this instruction set you won\u2019t be able to use Query Analytics. Configure your network . Decide how you want to run PMM Server. Choose from: Docker ; Podman ; Helm ; Virtual appliance ; Amazon AWS ; Use the easy install script. Authenticating using API keys. While adding clients to the PMM server, you use the admin user. However, if you change the password for the admin user from the PMM UI, then the clients will not be able to access PMM. Also, due to multiple unsuccessful login attempts Grafana will lock out the admin user. The solution is to use API key for authentication. You can use API keys as a replacement for basic authentication.","title":"Set up PMM Server"},{"location":"setting-up/server/aws.html","text":"AWS Marketplace \u00b6 You can run an instance of PMM Server hosted at AWS Marketplace. Assuming that you have an AWS (Amazon Web Services) account, locate Percona Monitoring and Management Server in AWS Marketplace or use this link . Selecting a region and instance type in the Pricing Information section will give you an estimate of the costs involved. This is only an indication of costs. You will choose regions and instance types in later steps. Percona Monitoring and Management Server is provided at no cost, but you may need to pay for infrastructure costs. Disk space consumed by PMM Server depends on the number of hosts being monitored. Although each environment will be unique, you can consider the data consumption figures for the PMM Demo web site which consumes approximately 230 MB per host per day, or approximately 6.9 GB per host at the default 30 day retention period. For more information, see our blog post How much disk space should I allocate for Percona Monitoring and Management? . Click Continue to Subscribe . Subscribe to this software : Check the terms and conditions and click Continue to Configuration . Configure this software : Select a value for Software Version . (The latest is 2.31.0.) Select a region. (You can change this in the next step.) Click Continue to Launch . Launch this software : Choose Action : Select a launch option. Launch from Website is a quick way to make your instance ready. For more control, choose Launch through EC2 . EC2 Instance Type : Select an instance type. VPC Settings : Choose or create a VPC (virtual private cloud). Subnet Settings : Choose or create a subnet. Security Group Settings : Choose a security group or click *Create New Based On Seller Settings Key Pair Settings : Choose or create a key pair. Click Launch . Limiting Access to the instance: security group and a key pair \u00b6 In the Security Group section, which acts like a firewall, you may use the preselected option Create new based on seller settings to create a security group with recommended settings. In the Key Pair select an already set up EC2 key pair to limit access to your instance. Important The security group should allow communication via the the following ports: 22 , 80 , and 443 . PMM should also be able to access port 3306 on the RDS that uses the instance. Applying settings \u00b6 Scroll up to the top of the page to view your settings. Then, click the Launch with 1 click button to continue and adjust your settings in the EC2 console. Your instance settings are summarized in a special area. Click the Launch with 1 click button to continue. The Launch with 1 click button may alternatively be titled as Accept Software Terms & Launch with 1-Click . Adjusting instance settings in the EC2 Console \u00b6 Your clicking the Launch with 1 click button, deploys your instance. To continue setting up your instance, run the EC2 console. It is available as a link at the top of the page that opens after you click the Launch with 1 click button. Your instance appears in the EC2 console in a table that lists all instances available to you. When a new instance is only created, it has no name. Make sure that you give it a name to distinguish from other instances managed via the EC2 console. Running the instance \u00b6 After you add your new instance it will take some time to initialize it. When the AWS console reports that the instance is now in a running state, you many continue with configuration of PMM Server. When started the next time after rebooting, your instance may acquire another IP address. You may choose to set up an elastic IP to avoid this problem. With your instance selected, open its IP address in a web browser. The IP address appears in the IPv4 Public IP column or as value of the Public IP field at the top of the Properties panel. To run the instance, copy and paste its public IP address to the location bar of your browser. In the Percona Monitoring and Management welcome page that opens, enter the instance ID. You can copy the instance ID from the Properties panel of your instance, select the Description tab back in the EC2 console. Click the Copy button next to the Instance ID field. This button appears as soon as you hover the cursor of your mouse over the ID. Hover the cursor over the instance ID for the Copy button to appear. Paste the instance in the Instance ID field of the Percona Monitoring and Management welcome page and click Submit . PMM Server provides user access control, and therefore you will need user credentials to access it: Default user name: admin Default password: admin You will be prompted to change the default password every time you log in. The PMM Server is now ready and the home page opens. You are creating a username and password that will be used for two purposes: authentication as a user to PMM - the credentials to log in to PMM. authentication between PMM Server and PMM Clients - you will re-use these credentials on another host when configuring PMM Client for the first time on a server, for example (DO NOT RUN ON THIS PMM SERVER YOU JUST CREATED): pmm-admin config --server-insecure-tls --server-url = https://admin:admin@<IP Address>:443 For instructions about how to access your instances by using an SSH client, see Connecting to Your Linux Instance Using SSH Make sure to replace the user name ec2-user used in this document with admin . Resizing the EBS Volume \u00b6 Your AWS instance comes with a predefined size which can become a limitation. To make more disk space available to your instance, you need to increase the size of the EBS volume as needed and then your instance will reconfigure itself to use the new size. The procedure of resizing EBS volumes is described in the Amazon documentation: Modifying the Size, IOPS, or Type of an EBS Volume on Linux . After the EBS volume is updated, PMM Server instance will auto-detect changes in approximately 5 minutes or less and will reconfigure itself for the updated conditions. Upgrading PMM Server on AWS \u00b6 Change Public IP address \u00b6 To assign a public IP address for an Amazon EC2 instance, follow these steps: Allocate Elastic IP address Associate Elastic IP address with a Network interface ID of your EC2 instance If you associate an Elastic IP address to an instance that already has an Elastic IP address associated, this previously associated Elastic IP address will be disassociated but still allocated to your account. Upgrading EC2 instance class \u00b6 Upgrading to a larger EC2 instance class is supported by PMM provided you follow the instructions from the AWS manual . The PMM AMI image uses a distinct EBS volume for the PMM data volume which permits independent resizing of the EC2 instance without impacting the EBS volume. Open the Amazon EC2 console. In the navigation pane, choose PMM Server Instances. Select the instance and choose Actions, Instance state, Stop instance. In the Change instance type dialog box, select the instance type that you want. Choose Apply to accept the new settings and start the stopped instance. Expanding the PMM Data EBS Volume \u00b6 The PMM data volume is mounted as an XFS formatted volume on top of an LVM volume. There are two ways to increase this volume size: Add a new disk via EC2 console or API, and expand the LVM volume to include the new disk volume. Expand existing EBS volume and grow the LVM volume. Expand existing EBS volume \u00b6 To expand the existing EBS volume for increased capacity, follow these steps. Expand the disk from AWS Console/CLI to the desired capacity. Login to the PMM EC2 instance and verify that the disk capacity has increased. For example, if you have expanded disk from 16G to 32G, dmesg output should look like below: [ 535.994494] xvdb: detected capacity change from 17179869184 to 34359738368 You can check information about volume groups and logical volumes with the vgs and lvs commands: vgs VG #PV #LV #SN Attr VSize VFree DataVG 1 2 0 wz--n- <16.00g 0 lvs LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert DataLV DataVG Vwi-aotz-- <12.80g ThinPool 1.74 ThinPool DataVG twi-aotz-- 15.96g 1.39 1.29 Now we can use the lsblk command to see that our disk size has been identified by the kernel correctly, but LVM2 is not yet aware of the new size. We can use pvresize to make sure the PV device reflects the new size. Once pvresize is executed, we can see that the VG has the new free space available. lsblk | grep xvdb xvdb 202:16 0 32G 0 disk pvscan PV /dev/xvdb VG DataVG lvm2 [<16.00 GiB / 0 free] Total: 1 [<16.00 GiB] / in use: 1 [<16.00 GiB] / in no VG: 0 [0 ] pvresize /dev/xvdb Physical volume \"/dev/xvdb\" changed 1 physical volume(s) resized / 0 physical volume(s) not resized pvs PV VG Fmt Attr PSize PFree /dev/xvdb DataVG lvm2 a-- <32.00g 16.00g We then extend our logical volume. Since the PMM image uses thin provisioning, we need to extend both the pool and the volume: lvs LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert DataLV DataVG Vwi-aotz-- <12.80g ThinPool 1.77 ThinPool DataVG twi-aotz-- 15.96g 1.42 1.32 lvextend /dev/mapper/DataVG-ThinPool -l 100 %VG Size of logical volume DataVG/ThinPool_tdata changed from 16.00 GiB (4096 extents) to 31.96 GiB (8183 extents). Logical volume DataVG/ThinPool_tdata successfully resized. lvs LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert DataLV DataVG Vwi-aotz-- <12.80g ThinPool 1.77 ThinPool DataVG twi-aotz-- 31.96g 0.71 1.71 Once the pool and volumes have been extended, we need to now extend the thin volume to consume the newly available space. In this example we\u2019ve grown available space to almost 32GB, and already consumed 12GB, so we\u2019re extending an additional 19GB: lvs LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert DataLV DataVG Vwi-aotz-- <12.80g ThinPool 1.77 ThinPool DataVG twi-aotz-- 31.96g 0.71 1.71 lvextend /dev/mapper/DataVG-DataLV -L +19G Size of logical volume DataVG/DataLV changed from <12.80 GiB (3276 extents) to <31.80 GiB (8140 extents). Logical volume DataVG/DataLV successfully resized. lvs LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert DataLV DataVG Vwi-aotz-- <31.80g ThinPool 0.71 ThinPool DataVG twi-aotz-- 31.96g 0.71 1.71 We then expand the XFS file system to reflect the new size using xfs_growfs , and confirm the file system is accurate using the df command. df -h /srv Filesystem Size Used Avail Use% Mounted on /dev/mapper/DataVG-DataLV 13G 249M 13G 2% /srv xfs_growfs /srv meta-data=/dev/mapper/DataVG-DataLV isize=512 agcount=103, agsize=32752 blks = sectsz=512 attr=2, projid32bit=1 = crc=1 finobt=0 spinodes=0 data = bsize=4096 blocks=3354624, imaxpct=25 = sunit=16 swidth=16 blks naming =version 2 bsize=4096 ascii-ci=0 ftype=1 log =internal bsize=4096 blocks=768, version=2 = sectsz=512 sunit=16 blks, lazy-count=1 realtime =none extsz=4096 blocks=0, rtextents=0 data blocks changed from 3354624 to 8335360 df -h /srv Filesystem Size Used Avail Use% Mounted on /dev/mapper/DataVG-DataLV 32G 254M 32G 1% /srv Expand the Amazon EBS root volume \u00b6 Expand the disk from AWS Console/CLI to the desired capacity. Login to the PMM EC2 instance and verify that the disk capacity has increased. For example, if you have expanded disk from 8G to 10G, dmesg output should look like below: # dmesg | grep \"capacity change\" [ 63175 .044762 ] nvme0n1: detected capacity change from 8589934592 to 10737418240 Use the lsblk command to see that our disk size has been identified by the kernel correctly, but LVM2 is not yet aware of the new size. # lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT nvme0n1 259 :1 0 10G 0 disk \u2514\u2500nvme0n1p1 259 :2 0 8G 0 part / ... For volumes that have a partition, such as the root volume shown in the previous step, use the growpart command to extend the partition. # growpart /dev/nvme0n1 1 CHANGED: partition = 1 start = 2048 old: size = 16775168 end = 16777216 new: size = 20969439 end = 20971487 To verify that the partition reflects the increased volume size, use the lsblk command again. # lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT nvme0n1 259:1 0 10G 0 disk \u2514\u2500nvme0n1p1 259:2 0 10G 0 part / ... Extend the XFS file system on the root volume by xfs_growfs command. I # xfs_growfs -d / meta-data = /dev/nvme0n1p1 isize = 512 agcount = 4 , agsize = 524224 blks = sectsz = 512 attr = 2 , projid32bit = 1 = crc = 1 finobt = 0 spinodes = 0 data = bsize = 4096 blocks = 2096896 , imaxpct = 25 = sunit = 0 swidth = 0 blks naming = version 2 bsize = 4096 ascii-ci = 0 ftype = 1 log = internal bsize = 4096 blocks = 2560 , version = 2 = sectsz = 512 sunit = 0 blks, lazy-count = 1 realtime = none extsz = 4096 blocks = 0 , rtextents = 0 data blocks changed from 2096896 to 2621120 Verify that file system reflects the increased volume size # df -hT / Filesystem Type Size Used Avail Use% Mounted on /dev/nvme0n1p1 xfs 10G 5 ,6G 4 ,5G 56 % / Backup PMM Server \u00b6 All data are stored in the /srv partition, so it\u2019s enough to back the PMM data volume. You can create a point-in-time snapshot of the volume and use it for data backup. The procedure of creating a snapshot is described in the Amazon documentation: Create Amazon EBS snapshots Restore PMM Server from a backup \u00b6 Create a new volume by using the latest snapshot of the PMM data volume. Stop the PMM Server instance. Detach the current PMM data volume. Attach the new volume. Start the PMM Server instance. Remove PMM Server \u00b6 Find the instance in the EC2 Console Select \u201cInstance state\u201d menu and \u201cTerminate instance\u201d Confirm termination operation See also Improving Percona Monitoring and Management EC2 Instance Resilience Using CloudWatch Alarm Actions","title":"AWS Marketplace"},{"location":"setting-up/server/aws.html#aws-marketplace","text":"You can run an instance of PMM Server hosted at AWS Marketplace. Assuming that you have an AWS (Amazon Web Services) account, locate Percona Monitoring and Management Server in AWS Marketplace or use this link . Selecting a region and instance type in the Pricing Information section will give you an estimate of the costs involved. This is only an indication of costs. You will choose regions and instance types in later steps. Percona Monitoring and Management Server is provided at no cost, but you may need to pay for infrastructure costs. Disk space consumed by PMM Server depends on the number of hosts being monitored. Although each environment will be unique, you can consider the data consumption figures for the PMM Demo web site which consumes approximately 230 MB per host per day, or approximately 6.9 GB per host at the default 30 day retention period. For more information, see our blog post How much disk space should I allocate for Percona Monitoring and Management? . Click Continue to Subscribe . Subscribe to this software : Check the terms and conditions and click Continue to Configuration . Configure this software : Select a value for Software Version . (The latest is 2.31.0.) Select a region. (You can change this in the next step.) Click Continue to Launch . Launch this software : Choose Action : Select a launch option. Launch from Website is a quick way to make your instance ready. For more control, choose Launch through EC2 . EC2 Instance Type : Select an instance type. VPC Settings : Choose or create a VPC (virtual private cloud). Subnet Settings : Choose or create a subnet. Security Group Settings : Choose a security group or click *Create New Based On Seller Settings Key Pair Settings : Choose or create a key pair. Click Launch .","title":"AWS Marketplace"},{"location":"setting-up/server/aws.html#limiting-access-to-the-instance-security-group-and-a-key-pair","text":"In the Security Group section, which acts like a firewall, you may use the preselected option Create new based on seller settings to create a security group with recommended settings. In the Key Pair select an already set up EC2 key pair to limit access to your instance. Important The security group should allow communication via the the following ports: 22 , 80 , and 443 . PMM should also be able to access port 3306 on the RDS that uses the instance.","title":"Limiting Access to the instance: security group and a key pair"},{"location":"setting-up/server/aws.html#applying-settings","text":"Scroll up to the top of the page to view your settings. Then, click the Launch with 1 click button to continue and adjust your settings in the EC2 console. Your instance settings are summarized in a special area. Click the Launch with 1 click button to continue. The Launch with 1 click button may alternatively be titled as Accept Software Terms & Launch with 1-Click .","title":"Applying settings"},{"location":"setting-up/server/aws.html#adjusting-instance-settings-in-the-ec2-console","text":"Your clicking the Launch with 1 click button, deploys your instance. To continue setting up your instance, run the EC2 console. It is available as a link at the top of the page that opens after you click the Launch with 1 click button. Your instance appears in the EC2 console in a table that lists all instances available to you. When a new instance is only created, it has no name. Make sure that you give it a name to distinguish from other instances managed via the EC2 console.","title":"Adjusting instance settings in the EC2 Console"},{"location":"setting-up/server/aws.html#running-the-instance","text":"After you add your new instance it will take some time to initialize it. When the AWS console reports that the instance is now in a running state, you many continue with configuration of PMM Server. When started the next time after rebooting, your instance may acquire another IP address. You may choose to set up an elastic IP to avoid this problem. With your instance selected, open its IP address in a web browser. The IP address appears in the IPv4 Public IP column or as value of the Public IP field at the top of the Properties panel. To run the instance, copy and paste its public IP address to the location bar of your browser. In the Percona Monitoring and Management welcome page that opens, enter the instance ID. You can copy the instance ID from the Properties panel of your instance, select the Description tab back in the EC2 console. Click the Copy button next to the Instance ID field. This button appears as soon as you hover the cursor of your mouse over the ID. Hover the cursor over the instance ID for the Copy button to appear. Paste the instance in the Instance ID field of the Percona Monitoring and Management welcome page and click Submit . PMM Server provides user access control, and therefore you will need user credentials to access it: Default user name: admin Default password: admin You will be prompted to change the default password every time you log in. The PMM Server is now ready and the home page opens. You are creating a username and password that will be used for two purposes: authentication as a user to PMM - the credentials to log in to PMM. authentication between PMM Server and PMM Clients - you will re-use these credentials on another host when configuring PMM Client for the first time on a server, for example (DO NOT RUN ON THIS PMM SERVER YOU JUST CREATED): pmm-admin config --server-insecure-tls --server-url = https://admin:admin@<IP Address>:443 For instructions about how to access your instances by using an SSH client, see Connecting to Your Linux Instance Using SSH Make sure to replace the user name ec2-user used in this document with admin .","title":"Running the instance"},{"location":"setting-up/server/aws.html#resizing-the-ebs-volume","text":"Your AWS instance comes with a predefined size which can become a limitation. To make more disk space available to your instance, you need to increase the size of the EBS volume as needed and then your instance will reconfigure itself to use the new size. The procedure of resizing EBS volumes is described in the Amazon documentation: Modifying the Size, IOPS, or Type of an EBS Volume on Linux . After the EBS volume is updated, PMM Server instance will auto-detect changes in approximately 5 minutes or less and will reconfigure itself for the updated conditions.","title":"Resizing the EBS Volume"},{"location":"setting-up/server/aws.html#upgrading-pmm-server-on-aws","text":"","title":"Upgrading PMM Server on AWS"},{"location":"setting-up/server/aws.html#change-public-ip-address","text":"To assign a public IP address for an Amazon EC2 instance, follow these steps: Allocate Elastic IP address Associate Elastic IP address with a Network interface ID of your EC2 instance If you associate an Elastic IP address to an instance that already has an Elastic IP address associated, this previously associated Elastic IP address will be disassociated but still allocated to your account.","title":"Change Public IP address"},{"location":"setting-up/server/aws.html#upgrading-ec2-instance-class","text":"Upgrading to a larger EC2 instance class is supported by PMM provided you follow the instructions from the AWS manual . The PMM AMI image uses a distinct EBS volume for the PMM data volume which permits independent resizing of the EC2 instance without impacting the EBS volume. Open the Amazon EC2 console. In the navigation pane, choose PMM Server Instances. Select the instance and choose Actions, Instance state, Stop instance. In the Change instance type dialog box, select the instance type that you want. Choose Apply to accept the new settings and start the stopped instance.","title":"Upgrading EC2 instance class"},{"location":"setting-up/server/aws.html#expanding-the-pmm-data-ebs-volume","text":"The PMM data volume is mounted as an XFS formatted volume on top of an LVM volume. There are two ways to increase this volume size: Add a new disk via EC2 console or API, and expand the LVM volume to include the new disk volume. Expand existing EBS volume and grow the LVM volume.","title":"Expanding the PMM Data EBS Volume"},{"location":"setting-up/server/aws.html#expand-existing-ebs-volume","text":"To expand the existing EBS volume for increased capacity, follow these steps. Expand the disk from AWS Console/CLI to the desired capacity. Login to the PMM EC2 instance and verify that the disk capacity has increased. For example, if you have expanded disk from 16G to 32G, dmesg output should look like below: [ 535.994494] xvdb: detected capacity change from 17179869184 to 34359738368 You can check information about volume groups and logical volumes with the vgs and lvs commands: vgs VG #PV #LV #SN Attr VSize VFree DataVG 1 2 0 wz--n- <16.00g 0 lvs LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert DataLV DataVG Vwi-aotz-- <12.80g ThinPool 1.74 ThinPool DataVG twi-aotz-- 15.96g 1.39 1.29 Now we can use the lsblk command to see that our disk size has been identified by the kernel correctly, but LVM2 is not yet aware of the new size. We can use pvresize to make sure the PV device reflects the new size. Once pvresize is executed, we can see that the VG has the new free space available. lsblk | grep xvdb xvdb 202:16 0 32G 0 disk pvscan PV /dev/xvdb VG DataVG lvm2 [<16.00 GiB / 0 free] Total: 1 [<16.00 GiB] / in use: 1 [<16.00 GiB] / in no VG: 0 [0 ] pvresize /dev/xvdb Physical volume \"/dev/xvdb\" changed 1 physical volume(s) resized / 0 physical volume(s) not resized pvs PV VG Fmt Attr PSize PFree /dev/xvdb DataVG lvm2 a-- <32.00g 16.00g We then extend our logical volume. Since the PMM image uses thin provisioning, we need to extend both the pool and the volume: lvs LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert DataLV DataVG Vwi-aotz-- <12.80g ThinPool 1.77 ThinPool DataVG twi-aotz-- 15.96g 1.42 1.32 lvextend /dev/mapper/DataVG-ThinPool -l 100 %VG Size of logical volume DataVG/ThinPool_tdata changed from 16.00 GiB (4096 extents) to 31.96 GiB (8183 extents). Logical volume DataVG/ThinPool_tdata successfully resized. lvs LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert DataLV DataVG Vwi-aotz-- <12.80g ThinPool 1.77 ThinPool DataVG twi-aotz-- 31.96g 0.71 1.71 Once the pool and volumes have been extended, we need to now extend the thin volume to consume the newly available space. In this example we\u2019ve grown available space to almost 32GB, and already consumed 12GB, so we\u2019re extending an additional 19GB: lvs LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert DataLV DataVG Vwi-aotz-- <12.80g ThinPool 1.77 ThinPool DataVG twi-aotz-- 31.96g 0.71 1.71 lvextend /dev/mapper/DataVG-DataLV -L +19G Size of logical volume DataVG/DataLV changed from <12.80 GiB (3276 extents) to <31.80 GiB (8140 extents). Logical volume DataVG/DataLV successfully resized. lvs LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert DataLV DataVG Vwi-aotz-- <31.80g ThinPool 0.71 ThinPool DataVG twi-aotz-- 31.96g 0.71 1.71 We then expand the XFS file system to reflect the new size using xfs_growfs , and confirm the file system is accurate using the df command. df -h /srv Filesystem Size Used Avail Use% Mounted on /dev/mapper/DataVG-DataLV 13G 249M 13G 2% /srv xfs_growfs /srv meta-data=/dev/mapper/DataVG-DataLV isize=512 agcount=103, agsize=32752 blks = sectsz=512 attr=2, projid32bit=1 = crc=1 finobt=0 spinodes=0 data = bsize=4096 blocks=3354624, imaxpct=25 = sunit=16 swidth=16 blks naming =version 2 bsize=4096 ascii-ci=0 ftype=1 log =internal bsize=4096 blocks=768, version=2 = sectsz=512 sunit=16 blks, lazy-count=1 realtime =none extsz=4096 blocks=0, rtextents=0 data blocks changed from 3354624 to 8335360 df -h /srv Filesystem Size Used Avail Use% Mounted on /dev/mapper/DataVG-DataLV 32G 254M 32G 1% /srv","title":"Expand existing EBS volume"},{"location":"setting-up/server/aws.html#expand-the-amazon-ebs-root-volume","text":"Expand the disk from AWS Console/CLI to the desired capacity. Login to the PMM EC2 instance and verify that the disk capacity has increased. For example, if you have expanded disk from 8G to 10G, dmesg output should look like below: # dmesg | grep \"capacity change\" [ 63175 .044762 ] nvme0n1: detected capacity change from 8589934592 to 10737418240 Use the lsblk command to see that our disk size has been identified by the kernel correctly, but LVM2 is not yet aware of the new size. # lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT nvme0n1 259 :1 0 10G 0 disk \u2514\u2500nvme0n1p1 259 :2 0 8G 0 part / ... For volumes that have a partition, such as the root volume shown in the previous step, use the growpart command to extend the partition. # growpart /dev/nvme0n1 1 CHANGED: partition = 1 start = 2048 old: size = 16775168 end = 16777216 new: size = 20969439 end = 20971487 To verify that the partition reflects the increased volume size, use the lsblk command again. # lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT nvme0n1 259:1 0 10G 0 disk \u2514\u2500nvme0n1p1 259:2 0 10G 0 part / ... Extend the XFS file system on the root volume by xfs_growfs command. I # xfs_growfs -d / meta-data = /dev/nvme0n1p1 isize = 512 agcount = 4 , agsize = 524224 blks = sectsz = 512 attr = 2 , projid32bit = 1 = crc = 1 finobt = 0 spinodes = 0 data = bsize = 4096 blocks = 2096896 , imaxpct = 25 = sunit = 0 swidth = 0 blks naming = version 2 bsize = 4096 ascii-ci = 0 ftype = 1 log = internal bsize = 4096 blocks = 2560 , version = 2 = sectsz = 512 sunit = 0 blks, lazy-count = 1 realtime = none extsz = 4096 blocks = 0 , rtextents = 0 data blocks changed from 2096896 to 2621120 Verify that file system reflects the increased volume size # df -hT / Filesystem Type Size Used Avail Use% Mounted on /dev/nvme0n1p1 xfs 10G 5 ,6G 4 ,5G 56 % /","title":"Expand the Amazon EBS root volume"},{"location":"setting-up/server/aws.html#backup-pmm-server","text":"All data are stored in the /srv partition, so it\u2019s enough to back the PMM data volume. You can create a point-in-time snapshot of the volume and use it for data backup. The procedure of creating a snapshot is described in the Amazon documentation: Create Amazon EBS snapshots","title":"Backup PMM Server"},{"location":"setting-up/server/aws.html#restore-pmm-server-from-a-backup","text":"Create a new volume by using the latest snapshot of the PMM data volume. Stop the PMM Server instance. Detach the current PMM data volume. Attach the new volume. Start the PMM Server instance.","title":"Restore PMM Server from a backup"},{"location":"setting-up/server/aws.html#remove-pmm-server","text":"Find the instance in the EC2 Console Select \u201cInstance state\u201d menu and \u201cTerminate instance\u201d Confirm termination operation See also Improving Percona Monitoring and Management EC2 Instance Resilience Using CloudWatch Alarm Actions","title":"Remove PMM Server"},{"location":"setting-up/server/dbaas.html","text":"DBaaS \u00b6 To enable and use the Database as a Service (DBaaS) feature in PMM, see DBaaS . You can use free K8s provided by Percona for evaluation. You can also create K8s on AWS using these instructions .","title":"DBaaS"},{"location":"setting-up/server/dbaas.html#dbaas","text":"To enable and use the Database as a Service (DBaaS) feature in PMM, see DBaaS . You can use free K8s provided by Percona for evaluation. You can also create K8s on AWS using these instructions .","title":"DBaaS"},{"location":"setting-up/server/docker.html","text":"Docker \u00b6 How to run PMM Server with Docker based on our Docker image . The tags used here are for the current release. Other tags are available. See also Easy-install script Before you start \u00b6 Install Docker 1.12.6 or higher. Run \u00b6 Summary Pull the Docker image. Copy it to create a persistent data container. Run the image. Open the PMM UI in a browser. You can store data from your PMM in: Docker volume (Preffered method) Data container Host directory Run Docker with volume \u00b6 Pull the image. docker pull percona/pmm-server:2 Create a volume: docker volume create pmm-data Run the image: docker run --detach --restart always \\ --publish 443 :443 \\ -v pmm-data:/srv \\ --name pmm-server \\ percona/pmm-server:2 4. Change the password for the default admin user. For PMM versions 2.27.0 and later: docker exec -t pmm-server change-admin-password <new_password> For PMM versions prior to 2.27.0: docker exec -t pmm-server bash -c 'grafana-cli --homepath /usr/share/grafana --configOverrides cfg:default.paths.data=/srv/grafana admin reset-admin-password newpass' Visit https://localhost:443 to see the PMM user interface in a web browser. (If you are accessing the docker host remotely, replace localhost with the IP or server name of the host.) Run Docker with data container \u00b6 Create a persistent data container. docker create --volume /srv \\ --name pmm-data \\ percona/pmm-server:2 /bin/true Important PMM Server expects the data volume to be /srv . Using any other value will result in data loss when upgrading. To check server and data container mount points: docker inspect pmm-data | grep Destination && \\ docker inspect pmm-server | grep Destination Run the image. docker run --detach --restart always \\ --publish 443 :443 \\ --volumes-from pmm-data \\ --name pmm-server \\ percona/pmm-server:2 Change the password for the default admin user. For PMM versions 2.27.0 and later: docker exec -t pmm-server change-admin-password <new_password> For PMM versions prior to 2.27.0: docker exec -t pmm-server bash -c 'grafana-cli --homepath /usr/share/grafana --configOverrides cfg:default.paths.data=/srv/grafana admin reset-admin-password newpass' Visit https://localhost:443 to see the PMM user interface in a web browser. (If you are accessing the docker host remotely, replace localhost with the IP or server name of the host.) Run Docker with the host directory \u00b6 Availability This feature is available starting with PMM 2.29.0. Pull the image. docker pull percona/pmm-server:2 Run the image. export DATA_DIR = $HOME /srv docker run -v $DATA_DIR /srv:/srv -d --restart always --publish 80 :80 --publish 443 :443 --name pmm-server percona/pmm-server:2 DATA_DIR is a directory where you want to store the state for PMM. Visit https://localhost:443 to see the PMM user interface in a web browser. (If you are accessing the docker host remotely, replace localhost with the IP or server name of the host.) Migrate from data container to host directory/volume \u00b6 To migrate your PMM from data container to host directory or volume run the following command: docker cp <containerId>:/srv /target/host/directory Backup \u00b6 Summary Stop and rename the pmm-server container. Take a local copy of the pmm-data container\u2019s /srv directory. Important Grafana plugins have been moved to the data volume /srv since the 2.23.0 version. So if you are upgrading PMM from any version before 2.23.0 and have installed additional plugins then plugins should be installed again after the upgrade. To check used grafana plugins: docker exec -it pmm-server ls /var/lib/grafana/plugins Stop the container. docker stop pmm-server Move the image. docker rename pmm-server pmm-server-backup Create a subdirectory (e.g., pmm-data-backup ) and move to it. mkdir pmm-data-backup && cd pmm-data-backup Backup the data. docker cp pmm-data:/srv . Upgrade \u00b6 Summary Stop the running container. Backup (rename) the container and copy data. Pull the latest Docker image. Run it. Important Downgrades are not possible. To go back to using a previous version you must have created a backup of it before upgrading. Tip To see what release you are running, use the PMM Upgrade panel on the Home Dashboard , or run: docker exec -it pmm-server \\ curl -ku admin:admin https://localhost/v1/version (If you are accessing the docker host remotely, replace localhost with the IP or server name of the host.) Stop the container. docker stop pmm-server Perform a backup . Pull the latest image. docker pull percona/pmm-server:2 Run it. docker run \\ --detach \\ --restart always \\ --publish 443 :443 \\ --volumes-from pmm-data \\ --name pmm-server \\ percona/pmm-server:2 Restore \u00b6 Summary Stop and remove the container. Restore (rename) the backup container. Restore saved data to the data container. Restore permissions to the data. Important You must have a backup to restore from. Stop the container. docker stop pmm-server Remove it. docker rm pmm-server Revert to the saved image. docker rename pmm-server-backup pmm-server Change directory to the backup directory (e.g. pmm-data-backup ). Remove Victoria Metrics data folder. docker run --rm --volumes-from pmm-data -it percona/pmm-server:2 rm -r /srv/victoriametrics/data Copy the data. docker cp srv pmm-data:/ Restore permissions. docker run --rm --volumes-from pmm-data -it percona/pmm-server:2 chown -R root:root /srv && \\ docker run --rm --volumes-from pmm-data -it percona/pmm-server:2 chown -R pmm:pmm /srv/alertmanager && \\ docker run --rm --volumes-from pmm-data -it percona/pmm-server:2 chown -R root:pmm /srv/clickhouse && \\ docker run --rm --volumes-from pmm-data -it percona/pmm-server:2 chown -R grafana:grafana /srv/grafana && \\ docker run --rm --volumes-from pmm-data -it percona/pmm-server:2 chown -R pmm:pmm /srv/logs && \\ docker run --rm --volumes-from pmm-data -it percona/pmm-server:2 chown -R postgres:postgres /srv/postgres && \\ docker run --rm --volumes-from pmm-data -it percona/pmm-server:2 chown -R pmm:pmm /srv/prometheus && \\ docker run --rm --volumes-from pmm-data -it percona/pmm-server:2 chown -R pmm:pmm /srv/victoriametrics && \\ docker run --rm --volumes-from pmm-data -it percona/pmm-server:2 chown -R postgres:postgres /srv/logs/postgresql.log Start the image. docker start pmm-server Remove \u00b6 Summary Stop the container. Remove (delete) both the server and data containers. Remove (delete) both images. Caution These steps delete the PMM Server Docker image and any accumulated PMM metrics data. Stop pmm-server container. docker stop pmm-server Remove containers. docker rm pmm-server pmm-data Remove the image. docker rmi $( docker images | grep \"percona/pmm-server\" | awk { 'print $3' } ) Environment variables \u00b6 Use the following Docker container environment variables (with -e var=value ) to set PMM Server parameters. Variable Description DISABLE_UPDATES Disables a periodic check for new PMM versions as well as ability to apply upgrades using the UI DISABLE_TELEMETRY Disable built-in telemetry and disable STT if telemetry is disabled. METRICS_RESOLUTION High metrics resolution in seconds. METRICS_RESOLUTION_HR High metrics resolution (same as above). METRICS_RESOLUTION_MR Medium metrics resolution in seconds. METRICS_RESOLUTION_LR Low metrics resolution in seconds. DATA_RETENTION How many days to keep time-series data in ClickHouse. ENABLE_VM_CACHE Enable cache in VM. ENABLE_ALERTING Enable integrated alerting. ENABLE_AZUREDISCOVER Enable support for discovery of Azure databases. ENABLE_BACKUP_MANAGEMENT Enable integrated backup tools. ENABLE_DBAAS Enable DBaaS features. PMM_DEBUG Enables a more verbose log level. PMM_TRACE Enables a more verbose log level including trace-back information. PMM_PUBLIC_ADDRESS External IP address or the DNS name on which PMM server is running. Preview environment variables \u00b6 Warning The PERCONA_TEST_* environment variables are experimental and subject to change. It is recommended that you use these variables for testing purposes only and not on production. Variable Description PERCONA_TEST_SAAS_HOST SaaS server hostname. PERCONA_TEST_PMM_CLICKHOUSE_ADDR Name of the host and port of the external ClickHouse database instance. PERCONA_TEST_PMM_CLICKHOUSE_DATABASE Database name of the external ClickHouse database instance. \u200b\u200bPERCONA_TEST_PMM_CLICKHOUSE_POOL_SIZE The maximum number of threads in the current connection thread pool. This value cannot be bigger than max_thread_pool_size. PERCONA_TEST_PMM_CLICKHOUSE_BLOCK_SIZE The number of rows to load from tables in one block for this connection. Ignored variables \u00b6 These variables will be ignored by pmm-managed when starting the server. If any other variable is found, it will be considered invalid and the server won\u2019t start. Variable Description _ , HOME , HOSTNAME , LANG , PATH , PWD , SHLVL , TERM Default environment variables. GF_* Grafana\u2019s environment variables. SUPERVISOR_ supervisord environment variables. KUBERNETES_ Kubernetes environment variables. MONITORING_ Kubernetes monitoring environment variables. PERCONA_TEST_ Unknown variable but won\u2019t prevent the server starting. PERCONA_TEST_DBAAS Deprecated. Use ENABLE_DBAAS . Tips \u00b6 To Disable the Home Dashboard PMM Upgrade panel you can either add -e DISABLE_UPDATES=true to the docker run command (for the life of the containter) or navigate to PMM \u2192 PMM Settings \u2192 Advanced Settings and disable \u201cCheck for Updates\u201d (can be turned back on by any admin in the UI). Eliminate browser certificate warnings by configuring a trusted certificate . You can optionally enable an (insecure) HTTP connection by adding --publish 80:80 to the docker run command. However, running PMM insecure is not recommended. You should also note that PMM Client requires TLS to communicate with the server, only working on a secure port. Isolated hosts \u00b6 If the host where you will run PMM Server has no internet connection, you can download the Docker image on a separate (internet-connected) host and securely copy it. On an internet-connected host, download the Docker image and its checksum file. wget https://downloads.percona.com/downloads/pmm2/2.31.0/docker/pmm-server-2.31.0.docker wget https://downloads.percona.com/downloads/pmm2/2.31.0/docker/pmm-server-2.31.0.sha256sum Copy both files to where you will run PMM Server. Open a terminal on the PMM Server host. (Optional) Check the Docker image file integrity. shasum -ca 256 pmm-server-2.31.0.sha256sum Load the image. docker load -i pmm-server-2.31.0.docker Create the pmm-data persistent data container. docker create --volume /srv \\ --name pmm-data percona/pmm-server:2.31.0 /bin/true Run the container. docker run \\ --detach \\ --restart always \\ --publish 443 :443 \\ --volumes-from pmm-data \\ --name pmm-server \\ percona/pmm-server:2.31.0","title":"Docker"},{"location":"setting-up/server/docker.html#docker","text":"How to run PMM Server with Docker based on our Docker image . The tags used here are for the current release. Other tags are available. See also Easy-install script","title":"Docker"},{"location":"setting-up/server/docker.html#before-you-start","text":"Install Docker 1.12.6 or higher.","title":"Before you start"},{"location":"setting-up/server/docker.html#run","text":"Summary Pull the Docker image. Copy it to create a persistent data container. Run the image. Open the PMM UI in a browser. You can store data from your PMM in: Docker volume (Preffered method) Data container Host directory","title":"Run"},{"location":"setting-up/server/docker.html#run-docker-with-volume","text":"Pull the image. docker pull percona/pmm-server:2 Create a volume: docker volume create pmm-data Run the image: docker run --detach --restart always \\ --publish 443 :443 \\ -v pmm-data:/srv \\ --name pmm-server \\ percona/pmm-server:2 4. Change the password for the default admin user. For PMM versions 2.27.0 and later: docker exec -t pmm-server change-admin-password <new_password> For PMM versions prior to 2.27.0: docker exec -t pmm-server bash -c 'grafana-cli --homepath /usr/share/grafana --configOverrides cfg:default.paths.data=/srv/grafana admin reset-admin-password newpass' Visit https://localhost:443 to see the PMM user interface in a web browser. (If you are accessing the docker host remotely, replace localhost with the IP or server name of the host.)","title":"Run Docker with volume"},{"location":"setting-up/server/docker.html#run-docker-with-data-container","text":"Create a persistent data container. docker create --volume /srv \\ --name pmm-data \\ percona/pmm-server:2 /bin/true Important PMM Server expects the data volume to be /srv . Using any other value will result in data loss when upgrading. To check server and data container mount points: docker inspect pmm-data | grep Destination && \\ docker inspect pmm-server | grep Destination Run the image. docker run --detach --restart always \\ --publish 443 :443 \\ --volumes-from pmm-data \\ --name pmm-server \\ percona/pmm-server:2 Change the password for the default admin user. For PMM versions 2.27.0 and later: docker exec -t pmm-server change-admin-password <new_password> For PMM versions prior to 2.27.0: docker exec -t pmm-server bash -c 'grafana-cli --homepath /usr/share/grafana --configOverrides cfg:default.paths.data=/srv/grafana admin reset-admin-password newpass' Visit https://localhost:443 to see the PMM user interface in a web browser. (If you are accessing the docker host remotely, replace localhost with the IP or server name of the host.)","title":"Run Docker with data container"},{"location":"setting-up/server/docker.html#run-docker-with-the-host-directory","text":"Availability This feature is available starting with PMM 2.29.0. Pull the image. docker pull percona/pmm-server:2 Run the image. export DATA_DIR = $HOME /srv docker run -v $DATA_DIR /srv:/srv -d --restart always --publish 80 :80 --publish 443 :443 --name pmm-server percona/pmm-server:2 DATA_DIR is a directory where you want to store the state for PMM. Visit https://localhost:443 to see the PMM user interface in a web browser. (If you are accessing the docker host remotely, replace localhost with the IP or server name of the host.)","title":"Run Docker with the host directory"},{"location":"setting-up/server/docker.html#migrate-from-data-container-to-host-directoryvolume","text":"To migrate your PMM from data container to host directory or volume run the following command: docker cp <containerId>:/srv /target/host/directory","title":"Migrate from data container to host directory/volume"},{"location":"setting-up/server/docker.html#backup","text":"Summary Stop and rename the pmm-server container. Take a local copy of the pmm-data container\u2019s /srv directory. Important Grafana plugins have been moved to the data volume /srv since the 2.23.0 version. So if you are upgrading PMM from any version before 2.23.0 and have installed additional plugins then plugins should be installed again after the upgrade. To check used grafana plugins: docker exec -it pmm-server ls /var/lib/grafana/plugins Stop the container. docker stop pmm-server Move the image. docker rename pmm-server pmm-server-backup Create a subdirectory (e.g., pmm-data-backup ) and move to it. mkdir pmm-data-backup && cd pmm-data-backup Backup the data. docker cp pmm-data:/srv .","title":"Backup"},{"location":"setting-up/server/docker.html#upgrade","text":"Summary Stop the running container. Backup (rename) the container and copy data. Pull the latest Docker image. Run it. Important Downgrades are not possible. To go back to using a previous version you must have created a backup of it before upgrading. Tip To see what release you are running, use the PMM Upgrade panel on the Home Dashboard , or run: docker exec -it pmm-server \\ curl -ku admin:admin https://localhost/v1/version (If you are accessing the docker host remotely, replace localhost with the IP or server name of the host.) Stop the container. docker stop pmm-server Perform a backup . Pull the latest image. docker pull percona/pmm-server:2 Run it. docker run \\ --detach \\ --restart always \\ --publish 443 :443 \\ --volumes-from pmm-data \\ --name pmm-server \\ percona/pmm-server:2","title":"Upgrade"},{"location":"setting-up/server/docker.html#restore","text":"Summary Stop and remove the container. Restore (rename) the backup container. Restore saved data to the data container. Restore permissions to the data. Important You must have a backup to restore from. Stop the container. docker stop pmm-server Remove it. docker rm pmm-server Revert to the saved image. docker rename pmm-server-backup pmm-server Change directory to the backup directory (e.g. pmm-data-backup ). Remove Victoria Metrics data folder. docker run --rm --volumes-from pmm-data -it percona/pmm-server:2 rm -r /srv/victoriametrics/data Copy the data. docker cp srv pmm-data:/ Restore permissions. docker run --rm --volumes-from pmm-data -it percona/pmm-server:2 chown -R root:root /srv && \\ docker run --rm --volumes-from pmm-data -it percona/pmm-server:2 chown -R pmm:pmm /srv/alertmanager && \\ docker run --rm --volumes-from pmm-data -it percona/pmm-server:2 chown -R root:pmm /srv/clickhouse && \\ docker run --rm --volumes-from pmm-data -it percona/pmm-server:2 chown -R grafana:grafana /srv/grafana && \\ docker run --rm --volumes-from pmm-data -it percona/pmm-server:2 chown -R pmm:pmm /srv/logs && \\ docker run --rm --volumes-from pmm-data -it percona/pmm-server:2 chown -R postgres:postgres /srv/postgres && \\ docker run --rm --volumes-from pmm-data -it percona/pmm-server:2 chown -R pmm:pmm /srv/prometheus && \\ docker run --rm --volumes-from pmm-data -it percona/pmm-server:2 chown -R pmm:pmm /srv/victoriametrics && \\ docker run --rm --volumes-from pmm-data -it percona/pmm-server:2 chown -R postgres:postgres /srv/logs/postgresql.log Start the image. docker start pmm-server","title":"Restore"},{"location":"setting-up/server/docker.html#remove","text":"Summary Stop the container. Remove (delete) both the server and data containers. Remove (delete) both images. Caution These steps delete the PMM Server Docker image and any accumulated PMM metrics data. Stop pmm-server container. docker stop pmm-server Remove containers. docker rm pmm-server pmm-data Remove the image. docker rmi $( docker images | grep \"percona/pmm-server\" | awk { 'print $3' } )","title":"Remove"},{"location":"setting-up/server/docker.html#environment-variables","text":"Use the following Docker container environment variables (with -e var=value ) to set PMM Server parameters. Variable Description DISABLE_UPDATES Disables a periodic check for new PMM versions as well as ability to apply upgrades using the UI DISABLE_TELEMETRY Disable built-in telemetry and disable STT if telemetry is disabled. METRICS_RESOLUTION High metrics resolution in seconds. METRICS_RESOLUTION_HR High metrics resolution (same as above). METRICS_RESOLUTION_MR Medium metrics resolution in seconds. METRICS_RESOLUTION_LR Low metrics resolution in seconds. DATA_RETENTION How many days to keep time-series data in ClickHouse. ENABLE_VM_CACHE Enable cache in VM. ENABLE_ALERTING Enable integrated alerting. ENABLE_AZUREDISCOVER Enable support for discovery of Azure databases. ENABLE_BACKUP_MANAGEMENT Enable integrated backup tools. ENABLE_DBAAS Enable DBaaS features. PMM_DEBUG Enables a more verbose log level. PMM_TRACE Enables a more verbose log level including trace-back information. PMM_PUBLIC_ADDRESS External IP address or the DNS name on which PMM server is running.","title":"Environment variables"},{"location":"setting-up/server/docker.html#preview-environment-variables","text":"Warning The PERCONA_TEST_* environment variables are experimental and subject to change. It is recommended that you use these variables for testing purposes only and not on production. Variable Description PERCONA_TEST_SAAS_HOST SaaS server hostname. PERCONA_TEST_PMM_CLICKHOUSE_ADDR Name of the host and port of the external ClickHouse database instance. PERCONA_TEST_PMM_CLICKHOUSE_DATABASE Database name of the external ClickHouse database instance. \u200b\u200bPERCONA_TEST_PMM_CLICKHOUSE_POOL_SIZE The maximum number of threads in the current connection thread pool. This value cannot be bigger than max_thread_pool_size. PERCONA_TEST_PMM_CLICKHOUSE_BLOCK_SIZE The number of rows to load from tables in one block for this connection.","title":"Preview environment variables"},{"location":"setting-up/server/docker.html#ignored-variables","text":"These variables will be ignored by pmm-managed when starting the server. If any other variable is found, it will be considered invalid and the server won\u2019t start. Variable Description _ , HOME , HOSTNAME , LANG , PATH , PWD , SHLVL , TERM Default environment variables. GF_* Grafana\u2019s environment variables. SUPERVISOR_ supervisord environment variables. KUBERNETES_ Kubernetes environment variables. MONITORING_ Kubernetes monitoring environment variables. PERCONA_TEST_ Unknown variable but won\u2019t prevent the server starting. PERCONA_TEST_DBAAS Deprecated. Use ENABLE_DBAAS .","title":"Ignored variables"},{"location":"setting-up/server/docker.html#tips","text":"To Disable the Home Dashboard PMM Upgrade panel you can either add -e DISABLE_UPDATES=true to the docker run command (for the life of the containter) or navigate to PMM \u2192 PMM Settings \u2192 Advanced Settings and disable \u201cCheck for Updates\u201d (can be turned back on by any admin in the UI). Eliminate browser certificate warnings by configuring a trusted certificate . You can optionally enable an (insecure) HTTP connection by adding --publish 80:80 to the docker run command. However, running PMM insecure is not recommended. You should also note that PMM Client requires TLS to communicate with the server, only working on a secure port.","title":"Tips"},{"location":"setting-up/server/docker.html#isolated-hosts","text":"If the host where you will run PMM Server has no internet connection, you can download the Docker image on a separate (internet-connected) host and securely copy it. On an internet-connected host, download the Docker image and its checksum file. wget https://downloads.percona.com/downloads/pmm2/2.31.0/docker/pmm-server-2.31.0.docker wget https://downloads.percona.com/downloads/pmm2/2.31.0/docker/pmm-server-2.31.0.sha256sum Copy both files to where you will run PMM Server. Open a terminal on the PMM Server host. (Optional) Check the Docker image file integrity. shasum -ca 256 pmm-server-2.31.0.sha256sum Load the image. docker load -i pmm-server-2.31.0.docker Create the pmm-data persistent data container. docker create --volume /srv \\ --name pmm-data percona/pmm-server:2.31.0 /bin/true Run the container. docker run \\ --detach \\ --restart always \\ --publish 443 :443 \\ --volumes-from pmm-data \\ --name pmm-server \\ percona/pmm-server:2.31.0","title":"Isolated hosts"},{"location":"setting-up/server/easy-install.html","text":"Easy-install script \u00b6 Caution You can download and check get-pmm.sh before running it from our github : Linux or macOS \u00b6 Using Curl: curl -fsSL https://www.percona.com/get/pmm | /bin/bash Using wget: wget -O - https://www.percona.com/get/pmm | /bin/bash This script: Installs Docker if it is not already installed on your system. Stops and backs up any PMM Server Docker containers that are currently running. Pulls and runs the latest PMM Server Docker image. Can run in Interactive mode to change the default settings: curl -fsSLO https://www.percona.com/get/pmm ( or wget https://www.percona.com/get/pmm ) chmod +x pmm ./pmm --interactive","title":"Easy-install script"},{"location":"setting-up/server/easy-install.html#easy-install-script","text":"Caution You can download and check get-pmm.sh before running it from our github :","title":"Easy-install script"},{"location":"setting-up/server/easy-install.html#linux-or-macos","text":"Using Curl: curl -fsSL https://www.percona.com/get/pmm | /bin/bash Using wget: wget -O - https://www.percona.com/get/pmm | /bin/bash This script: Installs Docker if it is not already installed on your system. Stops and backs up any PMM Server Docker containers that are currently running. Pulls and runs the latest PMM Server Docker image. Can run in Interactive mode to change the default settings: curl -fsSLO https://www.percona.com/get/pmm ( or wget https://www.percona.com/get/pmm ) chmod +x pmm ./pmm --interactive","title":"Linux or macOS"},{"location":"setting-up/server/helm.html","text":"Helm \u00b6 Caution PMM on Kubernetes with Helm is currently in technical preview and is subject to change. Helm is the package manager for Kubernetes. Percona Helm charts can be found in percona/percona-helm-charts repository on Github. Before you start \u00b6 Install Helm following its official installation instructions . Kubernetes cluster that Helm supports Helm v3 is needed to run the following steps. Use Helm to install PMM server on Kubernetes clusters \u00b6 Availability This feature is available starting with PMM 2.29.0. Summary Install Configuration parameters PMM admin password PMM environment variables PMM SSL certificates Upgrade Uninstall Install \u00b6 To install the chart with the release name pmm : helm repo add percona https://percona.github.io/percona-helm-charts/ helm install pmm percona/pmm The command deploys PMM on the Kubernetes cluster in the default configuration. The Parameters section lists the parameters that can be configured during installation. helm uninstall pmm Tip List all releases using helm list . Parameters \u00b6 The list of Parameters is subject to change from release to release. Check the Parameters section of the PMM Helm Chart. Tip You can list the default parameters values.yaml or get them from chart definition: helm show values percona/pmm Specify each parameter using the --set key=value[,key=value] or --set-string key=value[,key=value] arguments to helm install . For example, helm install pmm \\ --set-string pmmEnv.ENABLE_DBAAS = \"1\" \\ --set service.type = \"NodePort\" \\ --set storage.storageClassName = \"linode-block-storage-retain\" \\ percona/pmm The above command installs PMM with the enabled PMM DBaaS feature. Additionally, it sets the Service network type to NodePort and storage class to linode-block-storage-retain for persistence storage on LKE. helm uninstall pmm Important Once this chart is deployed, it is impossible to change the application\u2019s access credentials, such as password, using Helm. To change these application credentials after deployment, delete any persistent volumes (PVs) used by the chart and re-deploy it, or use the application\u2019s built-in administrative tools (if available) Alternatively, a YAML file that specifies the values for the above parameters can be provided while installing the chart. For example: helm show values percona/pmm > values.yaml #change needed parameters in values.yaml helm install pmm -f values.yaml percona/pmm PMM admin password \u00b6 PMM admin password would be set only on the first deployment. That setting is ignored if PMM was already provisioned and just restarted and/or updated. If PMM admin password is not set explicitly (default), it will be generated. To get admin password execute: kubectl get secret pmm-secret -o jsonpath = '{.data.PMM_ADMIN_PASSWORD}' | base64 --decode PMM environment variables \u00b6 In case you want to add extra environment variables (useful for advanced operations like custom init scripts), you can use the pmmEnv property. pmmEnv : DISABLE_UPDATES : \"1\" ENABLE_DBAAS : \"1\" PMM SSL certificates \u00b6 PMM ships with self signed SSL certificates to provide secure connection between client and server ( check here ). You could see the warning when connecting to PMM. To further increase security, you could provide your certificates and add values of credentials to the fields of the cert section: certs : name : pmm-certs files : certificate.crt : <content> certificate.key : <content> ca-certs.pem : <content> dhparam.pem : <content> Upgrades \u00b6 Percona will release a new chart updating its containers if a new version of the main container is available, there are any significant changes, or critical vulnerabilities exist. By default UI update feature is disabled and should not be enabled. Do not modify that parameter or add it while modifying the custom values.yaml file: pmmEnv : DISABLE_UPDATES : \"1\" Before updating the helm chart, it is recommended to pre-pull the image on the node where PMM is running, as the PMM images could be large and could take time to download. Update PMM as follows: helm repo update percona helm upgrade pmm -f values.yaml percona/pmm This will check updates in the repo and upgrade deployment if the updates are available. Uninstall \u00b6 To uninstall pmm deployment: helm uninstall pmm This command takes a release name and uninstalls the release. It removes all of the resources associated with the last release of the chart as well as the release history.","title":"Helm"},{"location":"setting-up/server/helm.html#helm","text":"Caution PMM on Kubernetes with Helm is currently in technical preview and is subject to change. Helm is the package manager for Kubernetes. Percona Helm charts can be found in percona/percona-helm-charts repository on Github.","title":"Helm"},{"location":"setting-up/server/helm.html#before-you-start","text":"Install Helm following its official installation instructions . Kubernetes cluster that Helm supports Helm v3 is needed to run the following steps.","title":"Before you start"},{"location":"setting-up/server/helm.html#use-helm-to-install-pmm-server-on-kubernetes-clusters","text":"Availability This feature is available starting with PMM 2.29.0. Summary Install Configuration parameters PMM admin password PMM environment variables PMM SSL certificates Upgrade Uninstall","title":"Use Helm to install PMM server on Kubernetes clusters"},{"location":"setting-up/server/helm.html#install","text":"To install the chart with the release name pmm : helm repo add percona https://percona.github.io/percona-helm-charts/ helm install pmm percona/pmm The command deploys PMM on the Kubernetes cluster in the default configuration. The Parameters section lists the parameters that can be configured during installation. helm uninstall pmm Tip List all releases using helm list .","title":"Install"},{"location":"setting-up/server/helm.html#parameters","text":"The list of Parameters is subject to change from release to release. Check the Parameters section of the PMM Helm Chart. Tip You can list the default parameters values.yaml or get them from chart definition: helm show values percona/pmm Specify each parameter using the --set key=value[,key=value] or --set-string key=value[,key=value] arguments to helm install . For example, helm install pmm \\ --set-string pmmEnv.ENABLE_DBAAS = \"1\" \\ --set service.type = \"NodePort\" \\ --set storage.storageClassName = \"linode-block-storage-retain\" \\ percona/pmm The above command installs PMM with the enabled PMM DBaaS feature. Additionally, it sets the Service network type to NodePort and storage class to linode-block-storage-retain for persistence storage on LKE. helm uninstall pmm Important Once this chart is deployed, it is impossible to change the application\u2019s access credentials, such as password, using Helm. To change these application credentials after deployment, delete any persistent volumes (PVs) used by the chart and re-deploy it, or use the application\u2019s built-in administrative tools (if available) Alternatively, a YAML file that specifies the values for the above parameters can be provided while installing the chart. For example: helm show values percona/pmm > values.yaml #change needed parameters in values.yaml helm install pmm -f values.yaml percona/pmm","title":"Parameters"},{"location":"setting-up/server/helm.html#pmm-admin-password","text":"PMM admin password would be set only on the first deployment. That setting is ignored if PMM was already provisioned and just restarted and/or updated. If PMM admin password is not set explicitly (default), it will be generated. To get admin password execute: kubectl get secret pmm-secret -o jsonpath = '{.data.PMM_ADMIN_PASSWORD}' | base64 --decode","title":"PMM admin password"},{"location":"setting-up/server/helm.html#pmm-environment-variables","text":"In case you want to add extra environment variables (useful for advanced operations like custom init scripts), you can use the pmmEnv property. pmmEnv : DISABLE_UPDATES : \"1\" ENABLE_DBAAS : \"1\"","title":"PMM environment variables"},{"location":"setting-up/server/helm.html#pmm-ssl-certificates","text":"PMM ships with self signed SSL certificates to provide secure connection between client and server ( check here ). You could see the warning when connecting to PMM. To further increase security, you could provide your certificates and add values of credentials to the fields of the cert section: certs : name : pmm-certs files : certificate.crt : <content> certificate.key : <content> ca-certs.pem : <content> dhparam.pem : <content>","title":"PMM SSL certificates"},{"location":"setting-up/server/helm.html#upgrades","text":"Percona will release a new chart updating its containers if a new version of the main container is available, there are any significant changes, or critical vulnerabilities exist. By default UI update feature is disabled and should not be enabled. Do not modify that parameter or add it while modifying the custom values.yaml file: pmmEnv : DISABLE_UPDATES : \"1\" Before updating the helm chart, it is recommended to pre-pull the image on the node where PMM is running, as the PMM images could be large and could take time to download. Update PMM as follows: helm repo update percona helm upgrade pmm -f values.yaml percona/pmm This will check updates in the repo and upgrade deployment if the updates are available.","title":"Upgrades"},{"location":"setting-up/server/helm.html#uninstall","text":"To uninstall pmm deployment: helm uninstall pmm This command takes a release name and uninstalls the release. It removes all of the resources associated with the last release of the chart as well as the release history.","title":"Uninstall"},{"location":"setting-up/server/network.html","text":"Network \u00b6 Ports \u00b6 This is a list of ports used by the various components of PMM. For PMM to work correctly, your system\u2019s firewall should allow TCP traffic on these ports (UDP is not needed). Ports to expose: PMM component TCP port Direction Description PMM Server 80 both HTTP server, used for gRPC over HTTP and web interface ( insecure , use with caution). PMM Server 443 both HTTPS server, used for gRPC over HTTPS and web interface (secure, use of SSL certificates is highly encouraged). Other ports: PMM component TCP port Direction Description PMM Server 7771 both gRPC, used for communication between pmm-agent , pmm-admin . PMM Server 7772 out HTTP1 server, used for older links like logs.zip . PMM Server 7773 out Debugging. pmm-agent 7777 out Default pmm-agent listen port. vm-agent 8428 both VictoriaMetrics port. pmm-agent 42000 - 51999 in Default range for pmm-agent connected agents. Important Depending on your architecture other ports may also need to be exposed. - For pmm-agent , the default listen port is 7777. - The default range for agents ports can be changed with the flag --ports-min and --ports-max , or in the configuration file.","title":"Network"},{"location":"setting-up/server/network.html#network","text":"","title":"Network"},{"location":"setting-up/server/network.html#ports","text":"This is a list of ports used by the various components of PMM. For PMM to work correctly, your system\u2019s firewall should allow TCP traffic on these ports (UDP is not needed). Ports to expose: PMM component TCP port Direction Description PMM Server 80 both HTTP server, used for gRPC over HTTP and web interface ( insecure , use with caution). PMM Server 443 both HTTPS server, used for gRPC over HTTPS and web interface (secure, use of SSL certificates is highly encouraged). Other ports: PMM component TCP port Direction Description PMM Server 7771 both gRPC, used for communication between pmm-agent , pmm-admin . PMM Server 7772 out HTTP1 server, used for older links like logs.zip . PMM Server 7773 out Debugging. pmm-agent 7777 out Default pmm-agent listen port. vm-agent 8428 both VictoriaMetrics port. pmm-agent 42000 - 51999 in Default range for pmm-agent connected agents. Important Depending on your architecture other ports may also need to be exposed. - For pmm-agent , the default listen port is 7777. - The default range for agents ports can be changed with the flag --ports-min and --ports-max , or in the configuration file.","title":"Ports"},{"location":"setting-up/server/podman.html","text":"Podman \u00b6 How to run PMM Server with Podman on our Docker image The tags used here are for the current release (PMM 2.31.0). Other tags are available. See also Docker Podman is an open-source project available on most Linux platforms and resides on GitHub . Podman is a daemonless container engine for developing, managing, and running Open Container Initiative (OCI) containers and container images on your Linux System. Non-privileged users could run containers under the control of Podman. It could be just aliased ( alias docker=podman ) with docker and work with the same way. All instructions from Docker section also apply here. Percona recommends running PMM as a non-privileged user and running it as part of the SystemD service provided. SystemD service ensures that the service is running and maintains logs and other management features (start, stop, etc.). Before you start \u00b6 Install Podman . Configure rootless Podman. Run as non-privileged user to start PMM \u00b6 Availability This feature is available starting with PMM 2.29.0. Summary Install. Configure. Enable and Start. Open the PMM UI in a browser. Install. Create ~/.config/systemd/user/pmm-server.service file: mkdir -p ~/.config/systemd/user/ cat << \"EOF\" > ~/.config/systemd/user/pmm-server.service [ Unit ] Description = pmm-server Wants = network-online.target After = network-online.target After = nss-user-lookup.target nss-lookup.target After = time-sync.target [ Service ] Type = simple # set environment for this unit Environment = PMM_PUBLIC_PORT = 8443 Environment = PMM_VOLUME_NAME = %N Environment = PMM_TAG = 2 .31.0 Environment = PMM_IMAGE = docker.io/percona/pmm-server Environment = PMM_ENV_FILE = %h/.config/pmm-server/pmm-server.env # optional env file that could override previous env settings for this unit EnvironmentFile = -%h/.config/pmm-server/env ExecStart = /usr/bin/podman run --rm --replace = true --name = %N -p ${ PMM_PUBLIC_PORT } :443/tcp --ulimit = host --volume = ${ PMM_VOLUME_NAME } :/srv --env-file = ${ PMM_ENV_FILE } --health-cmd = none --health-interval = disable ${ PMM_IMAGE } : ${ PMM_TAG } ExecStop = /usr/bin/podman stop -t 10 %N Restart = on-failure RestartSec = 20 [ Install ] Alias = %N WantedBy = default.target EOF Create ~/.config/pmm-server/pmm-server.env file: mkdir -p ~/.config/pmm-server/ cat << \"EOF\" > ~/.config/pmm-server/pmm-server.env # env file passed to the container # full list of environment variables: # https://www.percona.com/doc/percona-monitoring-and-management/2.x/setting-up/server/docker.html#environment-variables # keep updates disabled # do image replacement instead (update the tag and restart the service) DISABLE_UPDATES = 1 # Enable DBaaS feature #ENABLE_DBAAS=1 EOF Configure. There are 2 configuration files: 1. ~/.config/pmm-server/pmm-server.env defines environment variables for PMM Server (PMM parameters like DBaaS feature and etc) 2. ~/.config/pmm-server/env defines environment variables for SystemD service (image tags, repo and etc) SystemD service passes the environment parameters from the pmm-server.env file (in ~/.config/pmm-server/pmm-server.env ) to PMM. For more information about container environment variables, check Docker Environment . SystemD service uses some environment variables that could be customized if needed: Environment=PMM_PUBLIC_PORT=8443 Environment=PMM_VOLUME_NAME=%N Environment=PMM_TAG=2.31.0 Environment=PMM_IMAGE=docker.io/percona/pmm-server You can override the environment variables by defining them in the file ~/.config/pmm-server/env . For example, to override the path to a custom registry ~/.config/pmm-server/env : mkdir -p ~/.config/pmm-server/ cat << \"EOF\" > ~/.config/pmm-server/env PMM_TAG = 2 .30.0 PMM_IMAGE = docker.io/percona/pmm-server PMM_PUBLIC_PORT = 8443 EOF Important Ensure that you modify PMM_TAG in ~/.config/pmm-server/env and update it regularly as Percona cannot update it. It needs to be done by you. Enable and Start. systemctl --user enable --now pmm-server Visit https://localhost:8443 to see the PMM user interface in a web browser. (If you are accessing host remotely, replace localhost with the IP or server name of the host.) #first pull can take time sleep 80 timeout 60 podman wait --condition = running pmm-server Backup \u00b6 Summary Stop PMM server. Backup the data. Important Grafana plugins have been moved to the data volume /srv since the 2.23.0 version. So if you are upgrading PMM from any version before 2.23.0 and have installed additional plugins then plugins should be installed again after the upgrade. To check used grafana plugins: podman exec -it pmm-server ls /var/lib/grafana/plugins Stop PMM server. systemctl --user stop pmm-server Backup the data. podman wait --condition = stopped pmm-server || true sleep 30 podman volume export pmm-server --output pmm-server-backup.tar Important If you changed the default name to PMM_VOLUME_NAME environment variable, use that name after export instead of pmm-server (which is the default volume name). Upgrade \u00b6 Summary Perform a backup. Update PMM tag. Pre-pull image. Run it. Important You cannot downgrade. To go to a previous version, you must create a backup before upgrading. Tip To see the current release running on your system, use the PMM Upgrade panel on the Home Dashboard , or run: podman exec -it pmm-server \\ curl -ku admin:admin https://localhost/v1/version (If you are accessing the podman host remotely, replace localhost with the IP or server name of the host.) Perform a backup . Update PMM tag. Edit ~/.config/pmm-server/env and create/update with a new tag from latest release : sed -i \"s/PMM_TAG=.*/PMM_TAG=2.31.0/g\" ~/.config/pmm-server/env Pre-pull image for faster restart. sed -i \"s/PMM_TAG=.*/PMM_TAG=2.31.0-rc/g\" ~/.config/pmm-server/env sed -i \"s|PMM_IMAGE=.*|PMM_IMAGE=docker.io/perconalab/pmm-server|g\" ~/.config/pmm-server/env source ~/.config/pmm-server/env podman pull ${ PMM_IMAGE } : ${ PMM_TAG } Run PMM. systemctl --user restart pmm-server sleep 30 timeout 60 podman wait --condition = running pmm-server Restore \u00b6 Summary Stop PMM server. Run PMM on the previous image. Restore the volume. Start PMM Server. Important You must have a backup to restore from. You need to perform restore only if you have issues with upgrade or with the data. Stop PMM server. systemctl --user stop pmm-server Run PMM on the previous image. Edit ~/.config/pmm-server/env file: sed -i \"s/PMM_TAG=.*/PMM_TAG=2.29.1/g\" ~/.config/pmm-server/env Important X.Y.Z (2.29.1) is the version you used before upgrade and you made Backup with it Restore the volume. podman volume import pmm-server pmm-server-backup.tar Start PMM Server. systemctl --user start pmm-server sleep 30 timeout 60 podman wait \u2013condition=running pmm-server ``` Remove \u00b6 Summary Stop PMM server. Remove (delete) volume. Remove (delete) images. Caution These steps delete the PMM Server Docker image and the associated PMM metrics data. Stop PMM server. systemctl --user stop pmm-server Remove volume. #wait for container to stop podman wait --condition = stopped pmm-server || true sleep 10 podman volume rm --force pmm-server Remove the PMM images. podman rmi $( podman images | grep \"pmm-server\" | awk { 'print $3' } )","title":"Podman"},{"location":"setting-up/server/podman.html#podman","text":"How to run PMM Server with Podman on our Docker image The tags used here are for the current release (PMM 2.31.0). Other tags are available. See also Docker Podman is an open-source project available on most Linux platforms and resides on GitHub . Podman is a daemonless container engine for developing, managing, and running Open Container Initiative (OCI) containers and container images on your Linux System. Non-privileged users could run containers under the control of Podman. It could be just aliased ( alias docker=podman ) with docker and work with the same way. All instructions from Docker section also apply here. Percona recommends running PMM as a non-privileged user and running it as part of the SystemD service provided. SystemD service ensures that the service is running and maintains logs and other management features (start, stop, etc.).","title":"Podman"},{"location":"setting-up/server/podman.html#before-you-start","text":"Install Podman . Configure rootless Podman.","title":"Before you start"},{"location":"setting-up/server/podman.html#run-as-non-privileged-user-to-start-pmm","text":"Availability This feature is available starting with PMM 2.29.0. Summary Install. Configure. Enable and Start. Open the PMM UI in a browser. Install. Create ~/.config/systemd/user/pmm-server.service file: mkdir -p ~/.config/systemd/user/ cat << \"EOF\" > ~/.config/systemd/user/pmm-server.service [ Unit ] Description = pmm-server Wants = network-online.target After = network-online.target After = nss-user-lookup.target nss-lookup.target After = time-sync.target [ Service ] Type = simple # set environment for this unit Environment = PMM_PUBLIC_PORT = 8443 Environment = PMM_VOLUME_NAME = %N Environment = PMM_TAG = 2 .31.0 Environment = PMM_IMAGE = docker.io/percona/pmm-server Environment = PMM_ENV_FILE = %h/.config/pmm-server/pmm-server.env # optional env file that could override previous env settings for this unit EnvironmentFile = -%h/.config/pmm-server/env ExecStart = /usr/bin/podman run --rm --replace = true --name = %N -p ${ PMM_PUBLIC_PORT } :443/tcp --ulimit = host --volume = ${ PMM_VOLUME_NAME } :/srv --env-file = ${ PMM_ENV_FILE } --health-cmd = none --health-interval = disable ${ PMM_IMAGE } : ${ PMM_TAG } ExecStop = /usr/bin/podman stop -t 10 %N Restart = on-failure RestartSec = 20 [ Install ] Alias = %N WantedBy = default.target EOF Create ~/.config/pmm-server/pmm-server.env file: mkdir -p ~/.config/pmm-server/ cat << \"EOF\" > ~/.config/pmm-server/pmm-server.env # env file passed to the container # full list of environment variables: # https://www.percona.com/doc/percona-monitoring-and-management/2.x/setting-up/server/docker.html#environment-variables # keep updates disabled # do image replacement instead (update the tag and restart the service) DISABLE_UPDATES = 1 # Enable DBaaS feature #ENABLE_DBAAS=1 EOF Configure. There are 2 configuration files: 1. ~/.config/pmm-server/pmm-server.env defines environment variables for PMM Server (PMM parameters like DBaaS feature and etc) 2. ~/.config/pmm-server/env defines environment variables for SystemD service (image tags, repo and etc) SystemD service passes the environment parameters from the pmm-server.env file (in ~/.config/pmm-server/pmm-server.env ) to PMM. For more information about container environment variables, check Docker Environment . SystemD service uses some environment variables that could be customized if needed: Environment=PMM_PUBLIC_PORT=8443 Environment=PMM_VOLUME_NAME=%N Environment=PMM_TAG=2.31.0 Environment=PMM_IMAGE=docker.io/percona/pmm-server You can override the environment variables by defining them in the file ~/.config/pmm-server/env . For example, to override the path to a custom registry ~/.config/pmm-server/env : mkdir -p ~/.config/pmm-server/ cat << \"EOF\" > ~/.config/pmm-server/env PMM_TAG = 2 .30.0 PMM_IMAGE = docker.io/percona/pmm-server PMM_PUBLIC_PORT = 8443 EOF Important Ensure that you modify PMM_TAG in ~/.config/pmm-server/env and update it regularly as Percona cannot update it. It needs to be done by you. Enable and Start. systemctl --user enable --now pmm-server Visit https://localhost:8443 to see the PMM user interface in a web browser. (If you are accessing host remotely, replace localhost with the IP or server name of the host.) #first pull can take time sleep 80 timeout 60 podman wait --condition = running pmm-server","title":"Run as non-privileged user to start PMM"},{"location":"setting-up/server/podman.html#backup","text":"Summary Stop PMM server. Backup the data. Important Grafana plugins have been moved to the data volume /srv since the 2.23.0 version. So if you are upgrading PMM from any version before 2.23.0 and have installed additional plugins then plugins should be installed again after the upgrade. To check used grafana plugins: podman exec -it pmm-server ls /var/lib/grafana/plugins Stop PMM server. systemctl --user stop pmm-server Backup the data. podman wait --condition = stopped pmm-server || true sleep 30 podman volume export pmm-server --output pmm-server-backup.tar Important If you changed the default name to PMM_VOLUME_NAME environment variable, use that name after export instead of pmm-server (which is the default volume name).","title":"Backup"},{"location":"setting-up/server/podman.html#upgrade","text":"Summary Perform a backup. Update PMM tag. Pre-pull image. Run it. Important You cannot downgrade. To go to a previous version, you must create a backup before upgrading. Tip To see the current release running on your system, use the PMM Upgrade panel on the Home Dashboard , or run: podman exec -it pmm-server \\ curl -ku admin:admin https://localhost/v1/version (If you are accessing the podman host remotely, replace localhost with the IP or server name of the host.) Perform a backup . Update PMM tag. Edit ~/.config/pmm-server/env and create/update with a new tag from latest release : sed -i \"s/PMM_TAG=.*/PMM_TAG=2.31.0/g\" ~/.config/pmm-server/env Pre-pull image for faster restart. sed -i \"s/PMM_TAG=.*/PMM_TAG=2.31.0-rc/g\" ~/.config/pmm-server/env sed -i \"s|PMM_IMAGE=.*|PMM_IMAGE=docker.io/perconalab/pmm-server|g\" ~/.config/pmm-server/env source ~/.config/pmm-server/env podman pull ${ PMM_IMAGE } : ${ PMM_TAG } Run PMM. systemctl --user restart pmm-server sleep 30 timeout 60 podman wait --condition = running pmm-server","title":"Upgrade"},{"location":"setting-up/server/podman.html#restore","text":"Summary Stop PMM server. Run PMM on the previous image. Restore the volume. Start PMM Server. Important You must have a backup to restore from. You need to perform restore only if you have issues with upgrade or with the data. Stop PMM server. systemctl --user stop pmm-server Run PMM on the previous image. Edit ~/.config/pmm-server/env file: sed -i \"s/PMM_TAG=.*/PMM_TAG=2.29.1/g\" ~/.config/pmm-server/env Important X.Y.Z (2.29.1) is the version you used before upgrade and you made Backup with it Restore the volume. podman volume import pmm-server pmm-server-backup.tar Start PMM Server. systemctl --user start pmm-server sleep 30 timeout 60 podman wait \u2013condition=running pmm-server ```","title":"Restore"},{"location":"setting-up/server/podman.html#remove","text":"Summary Stop PMM server. Remove (delete) volume. Remove (delete) images. Caution These steps delete the PMM Server Docker image and the associated PMM metrics data. Stop PMM server. systemctl --user stop pmm-server Remove volume. #wait for container to stop podman wait --condition = stopped pmm-server || true sleep 10 podman volume rm --force pmm-server Remove the PMM images. podman rmi $( podman images | grep \"pmm-server\" | awk { 'print $3' } )","title":"Remove"},{"location":"setting-up/server/virtual-appliance.html","text":"Virtual Appliance \u00b6 How to run PMM Server as a virtual machine. Summary Download and verify the latest OVF file. Import it. Reconfigure network. Start the VM and get IP. Log into PMM UI. (Optional) Change VM root password. (Optional) Set up SSH. (Optional) Set up static IP. Most steps can be done with either a user interface or on the command line, but some steps can only be done in one or the other. Sections are labeled UI for user interface or CLI for command line instructions. Terminology \u00b6 Host is the desktop or server machine running the hypervisor. Hypervisor is software (e.g. VirtualBox , VMware ) that runs the guest OS as a virtual machine. Guest is the CentOS virtual machine that runs PMM Server. OVA file details \u00b6 Item Value Download page https://www.percona.com/downloads/pmm2/2.31.0/ova File name pmm-server-2.31.0.ova VM name PMM2-Server-2022-09-26-N ( N =build number) VM specifications \u00b6 Component Value OS CentOS 7.9 (64-bit) CPU 1 Base memory 4096 MB Disks LVM, 2 physical volumes Disk 1 ( sda ) VMDK (SCSI, 40 GB) Disk 2 ( sdb ) VMDK (SCSI, 400 GB) Users \u00b6 Default Username Default password root percona admin admin Download \u00b6 UI \u00b6 Open a web browser. Visit the PMM Server download page . Choose a Version or use the default (the latest). Click the link for pmm-server-2.31.0.ova to download it. Note where your browser saves it. Right click the link for pmm-server-2.31.0.sha256sum and save it in the same place as the .ova file. (Optional) Verify . CLI \u00b6 Download the latest PMM Server OVA and checksum files. wget https://www.percona.com/downloads/pmm2/2.31.0/ova/pmm-server-2.31.0.ova wget https://www.percona.com/downloads/pmm2/2.31.0/ova/pmm-server-2.31.0.sha256sum Verify \u00b6 CLI \u00b6 Verify the checksum of the downloaded .ova file. shasum -ca 256 pmm-server-2.31.0.sha256sum VMware \u00b6 Import \u00b6 UI \u00b6 Select File \u2192 Import . Click Choose file\u2026 . Navigate to the downloaded .ova file and select it. Click Open . Click Continue . In the Save as dialog: a. (Optional) Change the directory or file name. b. Click Save . Choose one of: (Optional) Click Finish . This starts the virtual machine. (Recommended) Click Customize Settings . This opens the VM\u2019s settings page without starting the machine. CLI \u00b6 Install ovftool . (You need to register.) Import and convert the OVA file. ( ovftool can\u2019t change CPU or memory settings during import but it can set the default interface.) Choose one of: Download and import the OVA file. ovftool --name = \"PMM Server\" --net:NAT = Wi-Fi \\ https://www.percona.com/downloads/pmm2/2.31.0/ova/pmm-server-2.31.0.ova \\ pmm-server-2.31.0.vmx Import an already-downloaded OVA file. ovftool --name = \"PMM Server\" --net:NAT = WiFi \\ pmm-server-2.31.0.ova \\ pmm-server.vmx Reconfigure interface \u00b6 When using the command line, the interface is remapped during import. UI \u00b6 If started, shut down the virtual machine. In the VMware main window, select the imported virtual machine. Click Virtual Machine \u2192 Settings\u2026 . Click Network Adapter . In the Bridged Networking section, select Autodetect . Close the settings window. Start guest and get IP address \u00b6 UI \u00b6 In the VMware main window, select the imported virtual machine. Click the play button or select Virtual Machine \u2192 Start Up . When the instance has booted, note the IP address in the guest console. CLI/UI \u00b6 Start the virtual machine in GUI mode. (There\u2019s no way to redirect a VMware VM\u2019s console to the host.) vmrun -gu root -gp percona start \\ pmm-server.vmx gui When the instance has booted, note the IP address in the guest console. (Optional) Stop and restart the instance in headless mode. vmrun stop pmm-server.vmx vmrun -gu root -gp percona start \\ pmm-server.vmx nogui VirtualBox \u00b6 Import \u00b6 UI \u00b6 Select File \u2192 Import appliance\u2026 . In the File field, type the path to the downloaded .ova file, or click the folder icon to navigate and open it. Click Continue . On the Appliance settings page, review the settings and click Import . Click Start . When the guest has booted, note the IP address in the guest console. CLI \u00b6 Open a terminal and change directory to where the downloaded .ova file is. (Optional) Do a \u2018dry run\u2019 import to see what values will be used. VBoxManage import pmm-server-2.31.0.ova --dry-run Import the image. Choose one of: With the default settings. VBoxManage import pmm-server-2.31.0.ova With custom settings (in this example, Name: \u201cPMM Server\u201d, CPUs: 2, RAM: 8192 MB). VBoxManage import --vsys 0 --vmname \"PMM Server\" \\ --cpus 2 --memory 8192 pmm-server-2.31.0.ova Interface \u00b6 UI \u00b6 Click Settings . Click Network . In the Adapter 1 field, click Attached to and change to Bridged Adapter . In the Name field, select your host\u2019s active network interface (e.g. en0: Wi-Fi (Wireless) ). Click OK . CLI \u00b6 Show the list of available bridge interfaces. VBoxManage list bridgedifs Find the name of the active interface you want to bridge to (one with Status: Up and a valid IP address). Example: en0: Wi-Fi (Wireless) Bridge the virtual machine\u2019s first interface ( nic1 ) to the host\u2019s en0 ethernet adapter. VBoxManage modifyvm 'PMM Server' \\ --nic1 bridged --bridgeadapter1 'en0: Wi-Fi (Wireless)' Redirect the console output into a host file. VBoxManage modifyvm 'PMM Server' \\ --uart1 0x3F8 4 --uartmode1 file /tmp/pmm-server-console.log Get IP \u00b6 UI \u00b6 Select the PMM Server virtual machine in the list. Click Start . When the guest has booted, note the IP address in the guest console. CLI \u00b6 Start the guest. VBoxManage startvm --type headless 'PMM Server' (Optional) Watch the log file. tail -f /tmp/pmm-server-console.log Wait for one minute for the server to boot up. Choose one of: Read the IP address from the tailed log file. Extract the IP address from the log file. grep -e \"^IP:\" /tmp/pmm-server-console.log | cut -f2 -d ' ' (Optional) Stop the guest: VBoxManage controlvm \"PMM Server\" poweroff Log into user interface \u00b6 UI \u00b6 Open a web browser and visit the guest IP address. The PMM login screen appears. Enter the default username and password in the relevant fields and click Log in . username: admin password: admin (Recommended) Follow the prompts to change the default password. You also can change the default password through SSH by using the change-admin-password command. The PMM Home Dashboard appears. (Optional) Change root password \u00b6 UI \u00b6 Start the virtual machine in GUI mode. Log in with the default superuser credentials: Username: root Password: percona Follow the prompts to change the password. (Optional) Set up SSH \u00b6 UI/CLI \u00b6 Create a key pair for the admin user. ssh-keygen -f admin Log into the PMM user interface. Select PMM \u2192 PMM Settings \u2192 SSH Key . Copy and paste the contents of the admin.pub file into the SSH Key field. Click Apply SSH Key . (This copies the public key to /home/admin/.ssh/authorized_keys in the guest). Log in via SSH ( N.N.N.N is the guest IP address). ssh -i admin admin@N.N.N.N (Optional) Set up static IP \u00b6 When the guest OS starts, it will get an IP address from the hypervisor\u2019s DHCP server. This IP can change each time the guest OS is restarted. Setting a static IP for the guest OS avoids having to check the IP address whenever the guest is restarted. CLI \u00b6 Start the virtual machine in non-headless (GUI) mode. Log in as root . Edit /etc/sysconfig/network-scripts/ifcfg-eth0 Change the value of BOOTPROTO : BOOTPROTO = none Add these values: IPADDR = 192.168.1.123 # replace with the desired static IP address NETMASK = 255.255.255.0 # replace with the netmask for your IP address GATEWAY = 192.168.1.1 # replace with the network gateway for your IP address PEERDNS = no DNS1 = 192.168.1.53 # replace with your DNS server IP Restart the interface. ifdown eth0 && ifup eth0 Check the IP. ip addr show eth0 8. Preserve the network configuration across reboots. echo \"network: {config: disabled}\" > /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg Remove \u00b6 UI \u00b6 Stop the virtual machine: select Close \u2192 Power Off . Remove the virtual machine: select Remove \u2192 Delete all files .","title":"Virtual Appliance"},{"location":"setting-up/server/virtual-appliance.html#virtual-appliance","text":"How to run PMM Server as a virtual machine. Summary Download and verify the latest OVF file. Import it. Reconfigure network. Start the VM and get IP. Log into PMM UI. (Optional) Change VM root password. (Optional) Set up SSH. (Optional) Set up static IP. Most steps can be done with either a user interface or on the command line, but some steps can only be done in one or the other. Sections are labeled UI for user interface or CLI for command line instructions.","title":"Virtual Appliance"},{"location":"setting-up/server/virtual-appliance.html#terminology","text":"Host is the desktop or server machine running the hypervisor. Hypervisor is software (e.g. VirtualBox , VMware ) that runs the guest OS as a virtual machine. Guest is the CentOS virtual machine that runs PMM Server.","title":"Terminology"},{"location":"setting-up/server/virtual-appliance.html#ova-file-details","text":"Item Value Download page https://www.percona.com/downloads/pmm2/2.31.0/ova File name pmm-server-2.31.0.ova VM name PMM2-Server-2022-09-26-N ( N =build number)","title":"OVA file details"},{"location":"setting-up/server/virtual-appliance.html#vm-specifications","text":"Component Value OS CentOS 7.9 (64-bit) CPU 1 Base memory 4096 MB Disks LVM, 2 physical volumes Disk 1 ( sda ) VMDK (SCSI, 40 GB) Disk 2 ( sdb ) VMDK (SCSI, 400 GB)","title":"VM specifications"},{"location":"setting-up/server/virtual-appliance.html#users","text":"Default Username Default password root percona admin admin","title":"Users"},{"location":"setting-up/server/virtual-appliance.html#download","text":"","title":"Download"},{"location":"setting-up/server/virtual-appliance.html#ui","text":"Open a web browser. Visit the PMM Server download page . Choose a Version or use the default (the latest). Click the link for pmm-server-2.31.0.ova to download it. Note where your browser saves it. Right click the link for pmm-server-2.31.0.sha256sum and save it in the same place as the .ova file. (Optional) Verify .","title":"UI"},{"location":"setting-up/server/virtual-appliance.html#cli","text":"Download the latest PMM Server OVA and checksum files. wget https://www.percona.com/downloads/pmm2/2.31.0/ova/pmm-server-2.31.0.ova wget https://www.percona.com/downloads/pmm2/2.31.0/ova/pmm-server-2.31.0.sha256sum","title":"CLI"},{"location":"setting-up/server/virtual-appliance.html#verify","text":"","title":"Verify"},{"location":"setting-up/server/virtual-appliance.html#cli_1","text":"Verify the checksum of the downloaded .ova file. shasum -ca 256 pmm-server-2.31.0.sha256sum","title":"CLI"},{"location":"setting-up/server/virtual-appliance.html#vmware","text":"","title":"VMware"},{"location":"setting-up/server/virtual-appliance.html#import","text":"","title":"Import"},{"location":"setting-up/server/virtual-appliance.html#ui_1","text":"Select File \u2192 Import . Click Choose file\u2026 . Navigate to the downloaded .ova file and select it. Click Open . Click Continue . In the Save as dialog: a. (Optional) Change the directory or file name. b. Click Save . Choose one of: (Optional) Click Finish . This starts the virtual machine. (Recommended) Click Customize Settings . This opens the VM\u2019s settings page without starting the machine.","title":"UI"},{"location":"setting-up/server/virtual-appliance.html#cli_2","text":"Install ovftool . (You need to register.) Import and convert the OVA file. ( ovftool can\u2019t change CPU or memory settings during import but it can set the default interface.) Choose one of: Download and import the OVA file. ovftool --name = \"PMM Server\" --net:NAT = Wi-Fi \\ https://www.percona.com/downloads/pmm2/2.31.0/ova/pmm-server-2.31.0.ova \\ pmm-server-2.31.0.vmx Import an already-downloaded OVA file. ovftool --name = \"PMM Server\" --net:NAT = WiFi \\ pmm-server-2.31.0.ova \\ pmm-server.vmx","title":"CLI"},{"location":"setting-up/server/virtual-appliance.html#reconfigure-interface","text":"When using the command line, the interface is remapped during import.","title":"Reconfigure interface"},{"location":"setting-up/server/virtual-appliance.html#ui_2","text":"If started, shut down the virtual machine. In the VMware main window, select the imported virtual machine. Click Virtual Machine \u2192 Settings\u2026 . Click Network Adapter . In the Bridged Networking section, select Autodetect . Close the settings window.","title":"UI"},{"location":"setting-up/server/virtual-appliance.html#start-guest-and-get-ip-address","text":"","title":"Start guest and get IP address"},{"location":"setting-up/server/virtual-appliance.html#ui_3","text":"In the VMware main window, select the imported virtual machine. Click the play button or select Virtual Machine \u2192 Start Up . When the instance has booted, note the IP address in the guest console.","title":"UI"},{"location":"setting-up/server/virtual-appliance.html#cliui","text":"Start the virtual machine in GUI mode. (There\u2019s no way to redirect a VMware VM\u2019s console to the host.) vmrun -gu root -gp percona start \\ pmm-server.vmx gui When the instance has booted, note the IP address in the guest console. (Optional) Stop and restart the instance in headless mode. vmrun stop pmm-server.vmx vmrun -gu root -gp percona start \\ pmm-server.vmx nogui","title":"CLI/UI"},{"location":"setting-up/server/virtual-appliance.html#virtualbox","text":"","title":"VirtualBox"},{"location":"setting-up/server/virtual-appliance.html#import_1","text":"","title":"Import"},{"location":"setting-up/server/virtual-appliance.html#ui_4","text":"Select File \u2192 Import appliance\u2026 . In the File field, type the path to the downloaded .ova file, or click the folder icon to navigate and open it. Click Continue . On the Appliance settings page, review the settings and click Import . Click Start . When the guest has booted, note the IP address in the guest console.","title":"UI"},{"location":"setting-up/server/virtual-appliance.html#cli_3","text":"Open a terminal and change directory to where the downloaded .ova file is. (Optional) Do a \u2018dry run\u2019 import to see what values will be used. VBoxManage import pmm-server-2.31.0.ova --dry-run Import the image. Choose one of: With the default settings. VBoxManage import pmm-server-2.31.0.ova With custom settings (in this example, Name: \u201cPMM Server\u201d, CPUs: 2, RAM: 8192 MB). VBoxManage import --vsys 0 --vmname \"PMM Server\" \\ --cpus 2 --memory 8192 pmm-server-2.31.0.ova","title":"CLI"},{"location":"setting-up/server/virtual-appliance.html#interface","text":"","title":"Interface"},{"location":"setting-up/server/virtual-appliance.html#ui_5","text":"Click Settings . Click Network . In the Adapter 1 field, click Attached to and change to Bridged Adapter . In the Name field, select your host\u2019s active network interface (e.g. en0: Wi-Fi (Wireless) ). Click OK .","title":"UI"},{"location":"setting-up/server/virtual-appliance.html#cli_4","text":"Show the list of available bridge interfaces. VBoxManage list bridgedifs Find the name of the active interface you want to bridge to (one with Status: Up and a valid IP address). Example: en0: Wi-Fi (Wireless) Bridge the virtual machine\u2019s first interface ( nic1 ) to the host\u2019s en0 ethernet adapter. VBoxManage modifyvm 'PMM Server' \\ --nic1 bridged --bridgeadapter1 'en0: Wi-Fi (Wireless)' Redirect the console output into a host file. VBoxManage modifyvm 'PMM Server' \\ --uart1 0x3F8 4 --uartmode1 file /tmp/pmm-server-console.log","title":"CLI"},{"location":"setting-up/server/virtual-appliance.html#get-ip","text":"","title":"Get IP"},{"location":"setting-up/server/virtual-appliance.html#ui_6","text":"Select the PMM Server virtual machine in the list. Click Start . When the guest has booted, note the IP address in the guest console.","title":"UI"},{"location":"setting-up/server/virtual-appliance.html#cli_5","text":"Start the guest. VBoxManage startvm --type headless 'PMM Server' (Optional) Watch the log file. tail -f /tmp/pmm-server-console.log Wait for one minute for the server to boot up. Choose one of: Read the IP address from the tailed log file. Extract the IP address from the log file. grep -e \"^IP:\" /tmp/pmm-server-console.log | cut -f2 -d ' ' (Optional) Stop the guest: VBoxManage controlvm \"PMM Server\" poweroff","title":"CLI"},{"location":"setting-up/server/virtual-appliance.html#log-into-user-interface","text":"","title":"Log into user interface"},{"location":"setting-up/server/virtual-appliance.html#ui_7","text":"Open a web browser and visit the guest IP address. The PMM login screen appears. Enter the default username and password in the relevant fields and click Log in . username: admin password: admin (Recommended) Follow the prompts to change the default password. You also can change the default password through SSH by using the change-admin-password command. The PMM Home Dashboard appears.","title":"UI"},{"location":"setting-up/server/virtual-appliance.html#optional-change-root-password","text":"","title":"(Optional) Change root password"},{"location":"setting-up/server/virtual-appliance.html#ui_8","text":"Start the virtual machine in GUI mode. Log in with the default superuser credentials: Username: root Password: percona Follow the prompts to change the password.","title":"UI"},{"location":"setting-up/server/virtual-appliance.html#optional-set-up-ssh","text":"","title":"(Optional) Set up SSH"},{"location":"setting-up/server/virtual-appliance.html#uicli","text":"Create a key pair for the admin user. ssh-keygen -f admin Log into the PMM user interface. Select PMM \u2192 PMM Settings \u2192 SSH Key . Copy and paste the contents of the admin.pub file into the SSH Key field. Click Apply SSH Key . (This copies the public key to /home/admin/.ssh/authorized_keys in the guest). Log in via SSH ( N.N.N.N is the guest IP address). ssh -i admin admin@N.N.N.N","title":"UI/CLI"},{"location":"setting-up/server/virtual-appliance.html#optional-set-up-static-ip","text":"When the guest OS starts, it will get an IP address from the hypervisor\u2019s DHCP server. This IP can change each time the guest OS is restarted. Setting a static IP for the guest OS avoids having to check the IP address whenever the guest is restarted.","title":"(Optional) Set up static IP"},{"location":"setting-up/server/virtual-appliance.html#cli_6","text":"Start the virtual machine in non-headless (GUI) mode. Log in as root . Edit /etc/sysconfig/network-scripts/ifcfg-eth0 Change the value of BOOTPROTO : BOOTPROTO = none Add these values: IPADDR = 192.168.1.123 # replace with the desired static IP address NETMASK = 255.255.255.0 # replace with the netmask for your IP address GATEWAY = 192.168.1.1 # replace with the network gateway for your IP address PEERDNS = no DNS1 = 192.168.1.53 # replace with your DNS server IP Restart the interface. ifdown eth0 && ifup eth0 Check the IP. ip addr show eth0 8. Preserve the network configuration across reboots. echo \"network: {config: disabled}\" > /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg","title":"CLI"},{"location":"setting-up/server/virtual-appliance.html#remove","text":"","title":"Remove"},{"location":"setting-up/server/virtual-appliance.html#ui_9","text":"Stop the virtual machine: select Close \u2192 Power Off . Remove the virtual machine: select Remove \u2192 Delete all files .","title":"UI"},{"location":"using/index.html","text":"Using \u00b6 User Interface Using the web-based user interface. Finding dashboards. Rendering dashboard images. Viewing graph details. Annotating events. Integrated alerting Backup and Restore Query Analytics , a specialized dashboard for detailed query analysis. Advisors : Enabling and seeing the results of database Advisor checks. DBaaS : Configuration for DBaaS and UI for creating Database clusters. These features are currently only available for PMM Admin users: Backup; DBaaS; Integrated Alerting; Advisors. To use these features you must be logged in as a PMM Admin user and activate the features. If you are logged in as a user that has a Viewer or Editor role you\u2019ll see an \u2018insufficient access\u2019 message when trying to use these features.","title":"Using"},{"location":"using/index.html#using","text":"User Interface Using the web-based user interface. Finding dashboards. Rendering dashboard images. Viewing graph details. Annotating events. Integrated alerting Backup and Restore Query Analytics , a specialized dashboard for detailed query analysis. Advisors : Enabling and seeing the results of database Advisor checks. DBaaS : Configuration for DBaaS and UI for creating Database clusters. These features are currently only available for PMM Admin users: Backup; DBaaS; Integrated Alerting; Advisors. To use these features you must be logged in as a PMM Admin user and activate the features. If you are logged in as a user that has a Viewer or Editor role you\u2019ll see an \u2018insufficient access\u2019 message when trying to use these features.","title":"Using"},{"location":"using/alerting.html","text":"Percona Alerting \u00b6 Percona Alerting is the new Alerting feature introduced in PMM 2.31. This replaces the Integrated Alerting feature available in previous versions. Alerting notifies of important or unusual activity in your database environments so that you can identify and resolve problems quickly. When something needs your attention, PMM automatically sends you an alert through your specified contact points. Alert types \u00b6 Percona Alerting is powered by Grafana infrastructure. PMM leverages Grafana\u2019s advanced alerting capabilities and adds an extra layer of alert templates that simplifies complex alert rules. Depending on the datasources that you want to query, and the complexity of your required evaluation criteria, PMM enables you to create the following types of alerts: Percona templated alerts : alerts based on a set of default templates with common events and expressions for alerting. If you need custom expressions on which to base your alert rules, you can also create your own templates. Grafana managed alerts : alerts that handle complex conditions and can span multiple different data sources like SQL, Prometheus, InfluxDB, etc. These alerts are stored and executed by Grafana. Mimir or Loki alerts : alerts that consist of one single query, written in PromQL or LogQL. The alert rules are stored and executed on the Mimir or Loki ruler and are completely decoupled from the PMM and Grafana runtime. Mimir or Loki recording rules : precompute the result of expensive queries and execute alerts faster. With Mimir and Loki alert rules, you can run alert expressions closer to your data and at massive scale, managed by the Grafana. Alerting components \u00b6 Alerts are split into four key components: alert rules, contact points, notification policies, and silences. Alert rules \u00b6 Describe the circumstances under which you want to be alerted. The evaluation criteria that you define determine whether an alert will fire. An alert rule consists of one or more queries and expressions, a condition, the frequency of evaluation, and optionally, the duration over which the condition is met. For example, you might configure an alert to identify and notify you when MongoDB is down. Alert templates \u00b6 Provide a simplified framework for configuring complex alert rules. PMM includes a set of default templates with common events and expressions for alerting. You can also create your own templates if you need custom expressions on which to base your alert rules. You can check the alert templates available for your account under Alerting > Alert rule templates tab. PMM lists here the following types of templates: Built-in templates, available out-of-the-box with PMM. Alert templates fetched from Percona Platform, according to the entitlements available for your Percona Account. Custom templates created or uploaded on the Alerting page > Alert Templates Tab. Custom template files available in your yaml srv/alerting/templates directory. PMM loads them during startup. Silences \u00b6 Silences specify periods of time to suppress notifications. During a silence, PMM continues to track metrics and trigger alerts but does not send notifications to the specified contact points. Once the specified silence expires, notifications are resumed. For example, you can create a silence to suppress trivial notifications during weekends. Contact points \u00b6 Contact points specify how PMM should deliver Grafana-managed alerts. When an alert fires, a notification is sent to the specified contact points. Depending on the severity of an alert, you might want to send different alerts to different channels. For example, you can deliver common alerts via Slack channel, but send an email notification for potentially critical issues. You can choose from a variety of contact points, including Slack, email, webhooks, PagerDuty, and more. Notification policies \u00b6 Notification policies determine how Grafana alerts are routed to contact points by setting where, when, and how to send notifications. For example, you might specify a limit for the number of times a notification is sent during a certain period. This helps ensure that you don\u2019t spam your Slack channel with too many notifications about the same issue. Create a Percona templated alert \u00b6 This topic focuses on creating an alert rule based on PMM templates. For information on working with the other alert types, check the Grafana documentation on Grafana Labs . Provision alert resources \u00b6 Before creating PMM alert rules, configure the required alert resources: Go to Configuration > PMM Settings and ensure that the Alerting option is enabled. This is enabled by default starting with PMM 2.31. However, if you have disabled it, the Alerting page displays only Grafana-managed alert rules. This means that you will not be able to create alerts based on PMM templates. Go to Dashboards > Browse and check the folders available for storing alert rules. If none of the available folders are relevant for your future alert rules, click New > New Folder and create a custom one. Go to Alerting > Alert Rule Templates and check the default PMM templates. If none of the templates include a relevant expression for the type of alerts that you want to create, click Add to create a custom template instead. Configure alert templates \u00b6 Alerts templates are YAML files that provide the source framework for alert rules. Alert templates contain general template details and an alert expression defined in MetricsQL . This query language is backward compatible with Prometheus QL. Create custom templates \u00b6 If none of the default PMM templates contain a relevant expression for the alert rule that you need, you can create a custom template instead. You can base multiple alert rules on the same template. For example, you can create a pmm_node_high_cpu_load template that can be used as the source for alert rules for production versus staging, warning versus critical, etc. Template format \u00b6 When creating custom templates, make sure to use the required template format below: name (required field): uniquely identifies template. Spaces and special characters are not allowed. version (required): defines the template format version. summary (required field): a template description. expr (required field): a MetricsQL query string with parameter placeholders. params : contains parameter definitions required for the query. Each parameter has a name, type, and summary. It also may have a unit, available range, and default value. name (required): the name of the parameter. Spaces and special characters are not allowed. summary (required): a short description of what this parameter represents. unit (optional): PMM currently supports either s (seconds) or % (percentage). type (required):PMM currently supports the float type. string, bool, and other types will be available in a future release. range (optional): defines the boundaries for the value of a float parameter. value (optional): default parameter value. Value strings must not include any of these special characters: < > ! @ # $ % ^ & * ( ) _ / \u2018 + - = (space) for (required): specifies the duration of time that the expression must be met before the alert will be fired. severity (required): specifies default alert severity level. labels (optional): are additional labels to be added to generated alerts. annotations (optional): are additional annotations to be added to generated alerts. Template example \u00b6 --- templates : - name : pmm_mongodb_high_memory_usage version : 1 summary : Memory used by MongoDB expr : |- sum by (node_name) (mongodb_ss_mem_resident * 1024 * 1024) / on (node_name) (node_memory_MemTotal_bytes) * 100 > [[ .threshold ]] params : - name : threshold summary : A percentage from configured maximum unit : \"%\" type : float range : [ 0 , 100 ] value : 80 for : 5m severity : warning labels : custom_label : demo annotations : summary : MongoDB high memory usage ({{ $labels.service_name }}) description : |- {{ $value }}% of memory (more than [[ .threshold ]]%) is used by {{ $labels.service_name }} on {{ $labels.node_name }}. Test alert expressions \u00b6 If you want to create custom templates, you can test the MetricsQ expressions for your custom template in the Explore section of PMM. Here you can also query any PMM internal database. To test expressions for custom templates: On the side menu in PMM, choose Explore > Metrics . Enter your expression in the Metrics field and click Run query . For example, to validate that a MongoDB instance is down, shut down a member of a three-node replica set, then check that the following expression returns 0 in Explore > Metrics : sh {service_type=\"mongodb\"} Add an alert rule \u00b6 After provisioning the resources required for creating Percona templated alerts, you are now ready to create your alert: Go to Alerting > Alert Rules , and click New alert rule . On the Create alert rule page, select the Percona templated alert option. If you want to learn about creating Grafana alerts instead, check our Grafana\u2019s documentation . In the Template details section, choose the template on which you want to base the new alert rule. This automatically populates the Name , Duration , and Severity fields with information from the template. You can change these values if you want to override the default specifications in the template. In the Filters field, specify if you want the alert rule to apply only to specific services or nodes. For example: service_name' , Operator: MATCH , VALUE: ps5.7 . From the Folder drop-down menu, select the location where you want to store the rule. Click Save and Exit to close the page and go to the Alert Rules tab where you can review, edit and silence your new alert. Silence alerts \u00b6 Create a silence when you want to stop notifications from one or more alerting rules. Silences stop notifications from being sent to your specified contact points. Silenced alerts are still recorded under Alerting > Fired Alerts so that you can review them later. Silenced alerts are disabled for as long as it\u2019s specified in the Silence Duration or until you remove a silence. For information on creating silences, see About alerting silences in the Grafana documentation. Deprecated alerting options \u00b6 PMM 2.31 introduced Percona Alerting which replaces the old Integrated Alerting in previous PMM versions. In addition to full feature parity, Percona Alerting includes additional benefits like Grafana-based alert rules and a unified, easy-to-use alerting command center on the Alerting page. Alerting compatibility \u00b6 Template compatibility with previous PMM versions \u00b6 If you have used Integrated Alerting in previous PMM versions, your custom alert rule templates will be automatically migrated to PMM 2.31. After upgrading to this new version, you will find all your alert templates under Alerting > Alert Templates . If you have any templates available in the /srv/ia/templates folder, make sure to transfer them to /srv/alerting/templates as PMM 2.31 and later will look for custom templates in this location. If you are upgrading from PMM 2.25 and earlier, alert templates will not be automatically migrated. This is because PMM 2.26.0 introduced significant changes to the core structure of rule templates. In this scenario, you will need to manually recreate any custom rule templates that you want to transfer to PMM 2.26.0 or later. Template compatibility with other alerting tools \u00b6 If you have existing YAML alert templates that you want to leverage in Percona Alerting: Go to Alerting > Alert Rule Templates tab and click Add at the top right-hand side of the table. Click Add and upload a local .yaml file from your computer. Migrate alert rules \u00b6 Alert rules created with Integrated Alerting in PMM 2.30 and earlier are not automatically migrated to Percona Alerting. After upgrading to PMM 2.31, make sure to manually migrate any alert rules that you want to transfer to PMM 2.31 using the Integrated Alerting Migration Script . Script commands \u00b6 The default command for migrating rules is: *python ia_migration.py -u admin -p admin* To see all the available options, check the scrip help using ia_migration.py -h Script prerequisites \u00b6 Python version 3.x, which you can download from Python Downloads centre . Requests library , which you can install with the following command: pip install requests . Important The script sets all migrated alert rules to Active. Make sure to silence any alerts that should not be firing. For more information about the script and advanced migration options, check out the help information embedded in the script. Disable Percona Alerting \u00b6 Percona Alerting is enabled by default in the PMM Settings. This feature adds the Percona templated alerts option on the Alerting page. If for some reason you want to disable PMM Alert templates and keep only Grafana-managed alerts: Go to Configuration > PMM Settings . Disable the Alerting option. The Alerting page will now display only Grafana-managed alert rules.","title":"Percona Alerting"},{"location":"using/alerting.html#percona-alerting","text":"Percona Alerting is the new Alerting feature introduced in PMM 2.31. This replaces the Integrated Alerting feature available in previous versions. Alerting notifies of important or unusual activity in your database environments so that you can identify and resolve problems quickly. When something needs your attention, PMM automatically sends you an alert through your specified contact points.","title":"Percona Alerting"},{"location":"using/alerting.html#alert-types","text":"Percona Alerting is powered by Grafana infrastructure. PMM leverages Grafana\u2019s advanced alerting capabilities and adds an extra layer of alert templates that simplifies complex alert rules. Depending on the datasources that you want to query, and the complexity of your required evaluation criteria, PMM enables you to create the following types of alerts: Percona templated alerts : alerts based on a set of default templates with common events and expressions for alerting. If you need custom expressions on which to base your alert rules, you can also create your own templates. Grafana managed alerts : alerts that handle complex conditions and can span multiple different data sources like SQL, Prometheus, InfluxDB, etc. These alerts are stored and executed by Grafana. Mimir or Loki alerts : alerts that consist of one single query, written in PromQL or LogQL. The alert rules are stored and executed on the Mimir or Loki ruler and are completely decoupled from the PMM and Grafana runtime. Mimir or Loki recording rules : precompute the result of expensive queries and execute alerts faster. With Mimir and Loki alert rules, you can run alert expressions closer to your data and at massive scale, managed by the Grafana.","title":"Alert types"},{"location":"using/alerting.html#alerting-components","text":"Alerts are split into four key components: alert rules, contact points, notification policies, and silences.","title":"Alerting components"},{"location":"using/alerting.html#alert-rules","text":"Describe the circumstances under which you want to be alerted. The evaluation criteria that you define determine whether an alert will fire. An alert rule consists of one or more queries and expressions, a condition, the frequency of evaluation, and optionally, the duration over which the condition is met. For example, you might configure an alert to identify and notify you when MongoDB is down.","title":"Alert rules"},{"location":"using/alerting.html#alert-templates","text":"Provide a simplified framework for configuring complex alert rules. PMM includes a set of default templates with common events and expressions for alerting. You can also create your own templates if you need custom expressions on which to base your alert rules. You can check the alert templates available for your account under Alerting > Alert rule templates tab. PMM lists here the following types of templates: Built-in templates, available out-of-the-box with PMM. Alert templates fetched from Percona Platform, according to the entitlements available for your Percona Account. Custom templates created or uploaded on the Alerting page > Alert Templates Tab. Custom template files available in your yaml srv/alerting/templates directory. PMM loads them during startup.","title":"Alert templates"},{"location":"using/alerting.html#silences","text":"Silences specify periods of time to suppress notifications. During a silence, PMM continues to track metrics and trigger alerts but does not send notifications to the specified contact points. Once the specified silence expires, notifications are resumed. For example, you can create a silence to suppress trivial notifications during weekends.","title":"Silences"},{"location":"using/alerting.html#contact-points","text":"Contact points specify how PMM should deliver Grafana-managed alerts. When an alert fires, a notification is sent to the specified contact points. Depending on the severity of an alert, you might want to send different alerts to different channels. For example, you can deliver common alerts via Slack channel, but send an email notification for potentially critical issues. You can choose from a variety of contact points, including Slack, email, webhooks, PagerDuty, and more.","title":"Contact points"},{"location":"using/alerting.html#notification-policies","text":"Notification policies determine how Grafana alerts are routed to contact points by setting where, when, and how to send notifications. For example, you might specify a limit for the number of times a notification is sent during a certain period. This helps ensure that you don\u2019t spam your Slack channel with too many notifications about the same issue.","title":"Notification policies"},{"location":"using/alerting.html#create-a-percona-templated-alert","text":"This topic focuses on creating an alert rule based on PMM templates. For information on working with the other alert types, check the Grafana documentation on Grafana Labs .","title":"Create a Percona templated alert"},{"location":"using/alerting.html#provision-alert-resources","text":"Before creating PMM alert rules, configure the required alert resources: Go to Configuration > PMM Settings and ensure that the Alerting option is enabled. This is enabled by default starting with PMM 2.31. However, if you have disabled it, the Alerting page displays only Grafana-managed alert rules. This means that you will not be able to create alerts based on PMM templates. Go to Dashboards > Browse and check the folders available for storing alert rules. If none of the available folders are relevant for your future alert rules, click New > New Folder and create a custom one. Go to Alerting > Alert Rule Templates and check the default PMM templates. If none of the templates include a relevant expression for the type of alerts that you want to create, click Add to create a custom template instead.","title":"Provision alert resources"},{"location":"using/alerting.html#configure-alert-templates","text":"Alerts templates are YAML files that provide the source framework for alert rules. Alert templates contain general template details and an alert expression defined in MetricsQL . This query language is backward compatible with Prometheus QL.","title":"Configure alert templates"},{"location":"using/alerting.html#create-custom-templates","text":"If none of the default PMM templates contain a relevant expression for the alert rule that you need, you can create a custom template instead. You can base multiple alert rules on the same template. For example, you can create a pmm_node_high_cpu_load template that can be used as the source for alert rules for production versus staging, warning versus critical, etc.","title":"Create custom templates"},{"location":"using/alerting.html#template-format","text":"When creating custom templates, make sure to use the required template format below: name (required field): uniquely identifies template. Spaces and special characters are not allowed. version (required): defines the template format version. summary (required field): a template description. expr (required field): a MetricsQL query string with parameter placeholders. params : contains parameter definitions required for the query. Each parameter has a name, type, and summary. It also may have a unit, available range, and default value. name (required): the name of the parameter. Spaces and special characters are not allowed. summary (required): a short description of what this parameter represents. unit (optional): PMM currently supports either s (seconds) or % (percentage). type (required):PMM currently supports the float type. string, bool, and other types will be available in a future release. range (optional): defines the boundaries for the value of a float parameter. value (optional): default parameter value. Value strings must not include any of these special characters: < > ! @ # $ % ^ & * ( ) _ / \u2018 + - = (space) for (required): specifies the duration of time that the expression must be met before the alert will be fired. severity (required): specifies default alert severity level. labels (optional): are additional labels to be added to generated alerts. annotations (optional): are additional annotations to be added to generated alerts.","title":"Template format"},{"location":"using/alerting.html#template-example","text":"--- templates : - name : pmm_mongodb_high_memory_usage version : 1 summary : Memory used by MongoDB expr : |- sum by (node_name) (mongodb_ss_mem_resident * 1024 * 1024) / on (node_name) (node_memory_MemTotal_bytes) * 100 > [[ .threshold ]] params : - name : threshold summary : A percentage from configured maximum unit : \"%\" type : float range : [ 0 , 100 ] value : 80 for : 5m severity : warning labels : custom_label : demo annotations : summary : MongoDB high memory usage ({{ $labels.service_name }}) description : |- {{ $value }}% of memory (more than [[ .threshold ]]%) is used by {{ $labels.service_name }} on {{ $labels.node_name }}.","title":"Template example"},{"location":"using/alerting.html#test-alert-expressions","text":"If you want to create custom templates, you can test the MetricsQ expressions for your custom template in the Explore section of PMM. Here you can also query any PMM internal database. To test expressions for custom templates: On the side menu in PMM, choose Explore > Metrics . Enter your expression in the Metrics field and click Run query . For example, to validate that a MongoDB instance is down, shut down a member of a three-node replica set, then check that the following expression returns 0 in Explore > Metrics : sh {service_type=\"mongodb\"}","title":"Test alert expressions"},{"location":"using/alerting.html#add-an-alert-rule","text":"After provisioning the resources required for creating Percona templated alerts, you are now ready to create your alert: Go to Alerting > Alert Rules , and click New alert rule . On the Create alert rule page, select the Percona templated alert option. If you want to learn about creating Grafana alerts instead, check our Grafana\u2019s documentation . In the Template details section, choose the template on which you want to base the new alert rule. This automatically populates the Name , Duration , and Severity fields with information from the template. You can change these values if you want to override the default specifications in the template. In the Filters field, specify if you want the alert rule to apply only to specific services or nodes. For example: service_name' , Operator: MATCH , VALUE: ps5.7 . From the Folder drop-down menu, select the location where you want to store the rule. Click Save and Exit to close the page and go to the Alert Rules tab where you can review, edit and silence your new alert.","title":"Add an alert rule"},{"location":"using/alerting.html#silence-alerts","text":"Create a silence when you want to stop notifications from one or more alerting rules. Silences stop notifications from being sent to your specified contact points. Silenced alerts are still recorded under Alerting > Fired Alerts so that you can review them later. Silenced alerts are disabled for as long as it\u2019s specified in the Silence Duration or until you remove a silence. For information on creating silences, see About alerting silences in the Grafana documentation.","title":"Silence alerts"},{"location":"using/alerting.html#deprecated-alerting-options","text":"PMM 2.31 introduced Percona Alerting which replaces the old Integrated Alerting in previous PMM versions. In addition to full feature parity, Percona Alerting includes additional benefits like Grafana-based alert rules and a unified, easy-to-use alerting command center on the Alerting page.","title":"Deprecated alerting options"},{"location":"using/alerting.html#alerting-compatibility","text":"","title":"Alerting compatibility"},{"location":"using/alerting.html#template-compatibility-with-previous-pmm-versions","text":"If you have used Integrated Alerting in previous PMM versions, your custom alert rule templates will be automatically migrated to PMM 2.31. After upgrading to this new version, you will find all your alert templates under Alerting > Alert Templates . If you have any templates available in the /srv/ia/templates folder, make sure to transfer them to /srv/alerting/templates as PMM 2.31 and later will look for custom templates in this location. If you are upgrading from PMM 2.25 and earlier, alert templates will not be automatically migrated. This is because PMM 2.26.0 introduced significant changes to the core structure of rule templates. In this scenario, you will need to manually recreate any custom rule templates that you want to transfer to PMM 2.26.0 or later.","title":"Template compatibility with previous PMM versions"},{"location":"using/alerting.html#template-compatibility-with-other-alerting-tools","text":"If you have existing YAML alert templates that you want to leverage in Percona Alerting: Go to Alerting > Alert Rule Templates tab and click Add at the top right-hand side of the table. Click Add and upload a local .yaml file from your computer.","title":"Template compatibility with other alerting tools"},{"location":"using/alerting.html#migrate-alert-rules","text":"Alert rules created with Integrated Alerting in PMM 2.30 and earlier are not automatically migrated to Percona Alerting. After upgrading to PMM 2.31, make sure to manually migrate any alert rules that you want to transfer to PMM 2.31 using the Integrated Alerting Migration Script .","title":"Migrate alert rules"},{"location":"using/alerting.html#script-commands","text":"The default command for migrating rules is: *python ia_migration.py -u admin -p admin* To see all the available options, check the scrip help using ia_migration.py -h","title":"Script commands"},{"location":"using/alerting.html#script-prerequisites","text":"Python version 3.x, which you can download from Python Downloads centre . Requests library , which you can install with the following command: pip install requests . Important The script sets all migrated alert rules to Active. Make sure to silence any alerts that should not be firing. For more information about the script and advanced migration options, check out the help information embedded in the script.","title":"Script prerequisites"},{"location":"using/alerting.html#disable-percona-alerting","text":"Percona Alerting is enabled by default in the PMM Settings. This feature adds the Percona templated alerts option on the Alerting page. If for some reason you want to disable PMM Alert templates and keep only Grafana-managed alerts: Go to Configuration > PMM Settings . Disable the Alerting option. The Alerting page will now display only Grafana-managed alert rules.","title":"Disable Percona Alerting"},{"location":"using/backup.html","text":"Backup and Restore \u00b6 Caution Backup and restore are technical preview features. Currently supported: MySQL database server or MongoDB replica set cluster, backing up to Amazon AWS S3 storage locations. Summary Enable backup features . Add a storage location . Satisfy preconditions: For MySQL : Confirm instance service parameters and storage location. Install required packages. For MongoDB : Install and run Percona Backup for MongoDB on every node in the replica set. Make a backup , or, Make or edit a scheduled backup, or, Enable MongoDB Point-In-Time-Recoverable Backups , or, Restore a backup . Delete a backup . Before you start \u00b6 You have an AWS S3 storage account and location details for it. In addition to bucket location details, you will also need to ensure proper S3 permissions. General minimum permissions are LIST/PUT/GET/DELETE. A sample IAM policy is: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : [ \"s3:ListBucket\" ], \"Resource\" : \"arn:aws:s3:::pmm-backup-testing\" }, { \"Effect\" : \"Allow\" , \"Action\" : [ \"s3:PutObject\" , \"s3:PutObjectAcl\" , \"s3:GetObject\" , \"s3:GetObjectAcl\" , \"s3:DeleteObject\" ], \"Resource\" : \"arn:aws:s3:::pmm-backup-testing/*\" } ] } Backup management has been enabled: Select Configuration \u2192 Settings \u2192 Advanced Settings . Activate Backup Management . Click Apply changes . If PMM Server runs as a Docker container, enable backup features at container creation time by adding -e ENABLE_BACKUP_MANAGEMENT=1 to your docker run command. Add a storage location \u00b6 Select \u2192 Backup . Select Storage locations . Click Add . Fill in the form fields. Name : A short name for this location. Description : A long description for this location. Type : Choose the type of storage: S3 : Use Amazon AWS S3 Endpoint : The S3 backup location endpoint (URL). Bucket Name : The bucket name. Access Key : The access key string. Secret Key : The secret key string. (Click to reveal and to hide again.) Local Client: (Not currently implemented) Local Server: (Not currently implemented) (Optional) Click Test to test the connection. Click Add to add the location. MySQL backup preconditions \u00b6 PMM Client is installed and running on the node. For MySQL 8.0+, the user that pmm-agent uses to connect to MySQL must have the BACKUP_ADMIN privilege for Xtrabackup to work. There is only one MySQL instance running on the node. MySQL is running: as a service via systemd ; with the name mysql or mysqld (to confirm, use systemctl status mysql or systemctl status mysqld respectively); from a mysql system user account. There is a mysql system group. MySQL is using the /var/lib/mysql directory for database storage. pmm-agent has read/write permissions to the /var/lib/mysql directory. The latest versions of the following packages are installed. They should be included in the $PATH environment variable: xtrabackup , which includes: xbcloud ; xbstream ; qpress . Important The versions of each must be compatible with the installed version of MySQL. MongoDB backup preconditions \u00b6 Percona Backup for MongoDB is installed and pbm-agent is running on all MongoDB nodes in the replica set. MongoDB is a member of a replica set. Make a backup \u00b6 Make a single on-demand backup. Add a storage location . Select \u2192 Backup . Select Backup Inventory . Click Add . In the Backup on demand dialog, enter values for: Service name : Choose from the menu the service to back up. Vendor : A value is automatically selected based on the service type. Backup name : Enter a unique name for this backup. Description : (Optional) Enter a long description for this backup. Location : Choose from the menu the storage location. Click Backup . In the Backup Inventory pane, watch the Status column. An animated ellipsis indicator shows activity in progress. Make a scheduled backup \u00b6 Make regular scheduled backups. Add a storage location . Select \u2192 Backup . Select Scheduled Backups . Click Add . In the Schedule backup dialog, enter values for: Service name : Choose from the menu the service to back up. Backup name : Enter a unique name for this scheduled backup. Vendor : A value is automatically selected based on the service type. Location : Choose from the menu the storage location. Data model : Select one of the options: Physical : Takes a physical backup of the database files. Logical : Takes a logical backup of data in the database. Currently not supported for MySQL. Description : (Optional) Enter a long description for this scheduled backup. Schedule : The schedule for the backup. Every : The backup interval. Choose from the menu one of: Year Month Week Day Hour Minute Depending on the interval chosen, the remaining options will be active or inactive. Month : Select one or more months. Day : Select one or more day numbers. Weekday : Select one or more week day names. Start time, h/m : The hour and minute for the backup. In the first field, select one or more hours ( 00 to 23 , 00 is midnight). In the second field, select one or more minutes ( 00 to 59 ). Retention : How many backups to keep. For unlimited, use 0 (zero). Retry mode : In case of error, either let PMM retry the backup again (\u201cAuto\u201d) or do it again yourself (\u201cManual\u201d). Retry times and interval : If \u201cAuto\u201d retry mode is selected, the maximum number of retries - up to 10 - and the interval between them - up to 8 hours - can be set here. Enable : Deselect to define the scheduled backup without enabling it. For this release (2.31.0), times are UTC. Click Schedule . A new entry will appear in the list. Edit a scheduled backup \u00b6 Select \u2192 Backup . Select Scheduled Backups . In the Actions column: Click the switch to enable or disable the backup. Click to edit the backup schedule. Click to delete the backup schedule. Click to create a (by default, disabled) copy of the backup schedule. MongoDB Point-In-Time-Recoverable Backups \u00b6 Caution MongoDB Point-In-Time-Recoverable Backups is part of Backup Management which is a technical preview feature. What is it? \u00b6 Better described by our team mates that develop Percona Backup for MongoDB: Point-in-Time Recovery is restoring a database up to a specific moment. Point-in-Time Recovery includes restoring the data from a backup snapshot and replaying all events that occurred to this data up to a specified moment from oplog slices . Point-in-Time Recovery helps you prevent data loss during a disaster such as crashed database, accidental data deletion or drop of tables, unwanted update of multiple fields instead of a single one. Point-In-Time-Recovery (PITR) Backups for MongoDB is new functionality available with PMM 2.23.0 as part of the larger Backup Management feature. This implementation in PMM uses Percona Backup for MongoDB pbm >= 1.6.0 behind the scenes. Percona Backup for MongoDB is a distributed, low-impact solution for achieving consistent backups of MongoDB sharded clusters and replica sets. Percona Backup for MongoDB supports Percona Server for MongoDB and MongoDB Community v3.6 or higher with MongoDB Replication enabled. Learn more about Percona Backup for MongoDB . How does it work? \u00b6 Enabling PITR \u00b6 The very first thing you want to do is to enable PITR. Here\u2019s how: Go to Backup Management . Select Scheduled Backups . Click on Add to create a new scheduled backup. Click on the PITR button to enable Point-In-Time-Recovery. Once you\u2019ve enabled PITR, head to the list of Scheduled Backups to confirm PITR is enabled. To disable PITR use the corresponding switch available on the list. PITR Artifacts \u00b6 The PITR artifacts will be available once your PITR job has run for the first time. Go to Backup Inventory to see the corresponding PITR artifact. PITR and Other Scheduled Backups \u00b6 It is important to notice that enabling PITR requires any other scheduled backup jobs to be disabled. If you try to enable PITR while other scheduled backup jobs are active, you will be shown an error message as seen in the image below. Go ahead to manually disable the existing scheduled jobs, then you\u2019ll be able to enable PITR. The above constraint applies at the service level. That said, you can still have PITR enabled for one service while having regular scheduled backup jobs for other services. Restore a backup \u00b6 MySQL backups can be restored to the same service it was created from, or to a compatible one. MongoDB backups can only be restored to the same service it was created from. Select \u2192 Backup \u2192 Backup Inventory . Find the row with the backup you want to restore. In the Actions column for that row, click Restore from backup . In the Restore from backup dialog: Select Same service to restore to a service with identical properties. Select the service in the Service name menu. Select Compatible services to restore to a compatible service. Select the compatible service in the Service name menu. Check the values and click Restore . Navigate to the Restore History tab to check the status of the restored backup. Delete a backup \u00b6 Select \u2192 Backup \u2192 Backup Inventory . Find the row with the backup you want to delete. In the Actions column for that row, click Delete backup . (Optional) Check Delete from storage to also delete the actual backup content besides just the backup register. Click Delete to proceed.","title":"Backup and Restore"},{"location":"using/backup.html#backup-and-restore","text":"Caution Backup and restore are technical preview features. Currently supported: MySQL database server or MongoDB replica set cluster, backing up to Amazon AWS S3 storage locations. Summary Enable backup features . Add a storage location . Satisfy preconditions: For MySQL : Confirm instance service parameters and storage location. Install required packages. For MongoDB : Install and run Percona Backup for MongoDB on every node in the replica set. Make a backup , or, Make or edit a scheduled backup, or, Enable MongoDB Point-In-Time-Recoverable Backups , or, Restore a backup . Delete a backup .","title":"Backup and Restore"},{"location":"using/backup.html#before-you-start","text":"You have an AWS S3 storage account and location details for it. In addition to bucket location details, you will also need to ensure proper S3 permissions. General minimum permissions are LIST/PUT/GET/DELETE. A sample IAM policy is: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : [ \"s3:ListBucket\" ], \"Resource\" : \"arn:aws:s3:::pmm-backup-testing\" }, { \"Effect\" : \"Allow\" , \"Action\" : [ \"s3:PutObject\" , \"s3:PutObjectAcl\" , \"s3:GetObject\" , \"s3:GetObjectAcl\" , \"s3:DeleteObject\" ], \"Resource\" : \"arn:aws:s3:::pmm-backup-testing/*\" } ] } Backup management has been enabled: Select Configuration \u2192 Settings \u2192 Advanced Settings . Activate Backup Management . Click Apply changes . If PMM Server runs as a Docker container, enable backup features at container creation time by adding -e ENABLE_BACKUP_MANAGEMENT=1 to your docker run command.","title":"Before you start"},{"location":"using/backup.html#add-a-storage-location","text":"Select \u2192 Backup . Select Storage locations . Click Add . Fill in the form fields. Name : A short name for this location. Description : A long description for this location. Type : Choose the type of storage: S3 : Use Amazon AWS S3 Endpoint : The S3 backup location endpoint (URL). Bucket Name : The bucket name. Access Key : The access key string. Secret Key : The secret key string. (Click to reveal and to hide again.) Local Client: (Not currently implemented) Local Server: (Not currently implemented) (Optional) Click Test to test the connection. Click Add to add the location.","title":"Add a storage location"},{"location":"using/backup.html#mysql-backup-preconditions","text":"PMM Client is installed and running on the node. For MySQL 8.0+, the user that pmm-agent uses to connect to MySQL must have the BACKUP_ADMIN privilege for Xtrabackup to work. There is only one MySQL instance running on the node. MySQL is running: as a service via systemd ; with the name mysql or mysqld (to confirm, use systemctl status mysql or systemctl status mysqld respectively); from a mysql system user account. There is a mysql system group. MySQL is using the /var/lib/mysql directory for database storage. pmm-agent has read/write permissions to the /var/lib/mysql directory. The latest versions of the following packages are installed. They should be included in the $PATH environment variable: xtrabackup , which includes: xbcloud ; xbstream ; qpress . Important The versions of each must be compatible with the installed version of MySQL.","title":"MySQL backup preconditions"},{"location":"using/backup.html#mongodb-backup-preconditions","text":"Percona Backup for MongoDB is installed and pbm-agent is running on all MongoDB nodes in the replica set. MongoDB is a member of a replica set.","title":"MongoDB backup preconditions"},{"location":"using/backup.html#make-a-backup","text":"Make a single on-demand backup. Add a storage location . Select \u2192 Backup . Select Backup Inventory . Click Add . In the Backup on demand dialog, enter values for: Service name : Choose from the menu the service to back up. Vendor : A value is automatically selected based on the service type. Backup name : Enter a unique name for this backup. Description : (Optional) Enter a long description for this backup. Location : Choose from the menu the storage location. Click Backup . In the Backup Inventory pane, watch the Status column. An animated ellipsis indicator shows activity in progress.","title":"Make a backup"},{"location":"using/backup.html#make-a-scheduled-backup","text":"Make regular scheduled backups. Add a storage location . Select \u2192 Backup . Select Scheduled Backups . Click Add . In the Schedule backup dialog, enter values for: Service name : Choose from the menu the service to back up. Backup name : Enter a unique name for this scheduled backup. Vendor : A value is automatically selected based on the service type. Location : Choose from the menu the storage location. Data model : Select one of the options: Physical : Takes a physical backup of the database files. Logical : Takes a logical backup of data in the database. Currently not supported for MySQL. Description : (Optional) Enter a long description for this scheduled backup. Schedule : The schedule for the backup. Every : The backup interval. Choose from the menu one of: Year Month Week Day Hour Minute Depending on the interval chosen, the remaining options will be active or inactive. Month : Select one or more months. Day : Select one or more day numbers. Weekday : Select one or more week day names. Start time, h/m : The hour and minute for the backup. In the first field, select one or more hours ( 00 to 23 , 00 is midnight). In the second field, select one or more minutes ( 00 to 59 ). Retention : How many backups to keep. For unlimited, use 0 (zero). Retry mode : In case of error, either let PMM retry the backup again (\u201cAuto\u201d) or do it again yourself (\u201cManual\u201d). Retry times and interval : If \u201cAuto\u201d retry mode is selected, the maximum number of retries - up to 10 - and the interval between them - up to 8 hours - can be set here. Enable : Deselect to define the scheduled backup without enabling it. For this release (2.31.0), times are UTC. Click Schedule . A new entry will appear in the list.","title":"Make a scheduled backup"},{"location":"using/backup.html#edit-a-scheduled-backup","text":"Select \u2192 Backup . Select Scheduled Backups . In the Actions column: Click the switch to enable or disable the backup. Click to edit the backup schedule. Click to delete the backup schedule. Click to create a (by default, disabled) copy of the backup schedule.","title":"Edit a scheduled backup"},{"location":"using/backup.html#mongodb-point-in-time-recoverable-backups","text":"Caution MongoDB Point-In-Time-Recoverable Backups is part of Backup Management which is a technical preview feature.","title":"MongoDB Point-In-Time-Recoverable Backups"},{"location":"using/backup.html#what-is-it","text":"Better described by our team mates that develop Percona Backup for MongoDB: Point-in-Time Recovery is restoring a database up to a specific moment. Point-in-Time Recovery includes restoring the data from a backup snapshot and replaying all events that occurred to this data up to a specified moment from oplog slices . Point-in-Time Recovery helps you prevent data loss during a disaster such as crashed database, accidental data deletion or drop of tables, unwanted update of multiple fields instead of a single one. Point-In-Time-Recovery (PITR) Backups for MongoDB is new functionality available with PMM 2.23.0 as part of the larger Backup Management feature. This implementation in PMM uses Percona Backup for MongoDB pbm >= 1.6.0 behind the scenes. Percona Backup for MongoDB is a distributed, low-impact solution for achieving consistent backups of MongoDB sharded clusters and replica sets. Percona Backup for MongoDB supports Percona Server for MongoDB and MongoDB Community v3.6 or higher with MongoDB Replication enabled. Learn more about Percona Backup for MongoDB .","title":"What is it?"},{"location":"using/backup.html#how-does-it-work","text":"","title":"How does it work?"},{"location":"using/backup.html#enabling-pitr","text":"The very first thing you want to do is to enable PITR. Here\u2019s how: Go to Backup Management . Select Scheduled Backups . Click on Add to create a new scheduled backup. Click on the PITR button to enable Point-In-Time-Recovery. Once you\u2019ve enabled PITR, head to the list of Scheduled Backups to confirm PITR is enabled. To disable PITR use the corresponding switch available on the list.","title":"Enabling PITR"},{"location":"using/backup.html#pitr-artifacts","text":"The PITR artifacts will be available once your PITR job has run for the first time. Go to Backup Inventory to see the corresponding PITR artifact.","title":"PITR Artifacts"},{"location":"using/backup.html#pitr-and-other-scheduled-backups","text":"It is important to notice that enabling PITR requires any other scheduled backup jobs to be disabled. If you try to enable PITR while other scheduled backup jobs are active, you will be shown an error message as seen in the image below. Go ahead to manually disable the existing scheduled jobs, then you\u2019ll be able to enable PITR. The above constraint applies at the service level. That said, you can still have PITR enabled for one service while having regular scheduled backup jobs for other services.","title":"PITR and Other Scheduled Backups"},{"location":"using/backup.html#restore-a-backup","text":"MySQL backups can be restored to the same service it was created from, or to a compatible one. MongoDB backups can only be restored to the same service it was created from. Select \u2192 Backup \u2192 Backup Inventory . Find the row with the backup you want to restore. In the Actions column for that row, click Restore from backup . In the Restore from backup dialog: Select Same service to restore to a service with identical properties. Select the service in the Service name menu. Select Compatible services to restore to a compatible service. Select the compatible service in the Service name menu. Check the values and click Restore . Navigate to the Restore History tab to check the status of the restored backup.","title":"Restore a backup"},{"location":"using/backup.html#delete-a-backup","text":"Select \u2192 Backup \u2192 Backup Inventory . Find the row with the backup you want to delete. In the Actions column for that row, click Delete backup . (Optional) Check Delete from storage to also delete the actual backup content besides just the backup register. Click Delete to proceed.","title":"Delete a backup"},{"location":"using/dbaas.html","text":"DBaaS \u00b6 Caution DBaaS functionality is currently in technical preview and is subject to change. The DBaaS dashboard is where you add, remove, and operate on Kubernetes and database clusters. Activate DBaaS \u00b6 The DBaaS feature is turned off by default. To turn it on: Go to Configuration \u2192 Settings \u2192 Advanced Settings . Click the toggle in the Technical preview features section of the page. Open the DBaaS dashboard \u00b6 From the left menu, select DBaaS . Kubernetes clusters \u00b6 Add a Kubernetes cluster \u00b6 Caution Ensure that you set PMM Public Address under Configuration \u2192 Settings \u2192 Advanced Settings before creating a Kubernetes cluster. Otherwise, PMM would not monitor the Kubernetes cluster along with the associated database clusters. PXC and PSMDB operators are installed as part of the Kubernetes cluster registration process. It enables you to deploy database clusters into the Kubernetes cluster. If a public address is set VM Operator is also installed as part of the Kubernetes cluster registration process. It lets you monitor a kubernetes cluster via PMM. Click Register new Kubernetes Cluster . Copy the value of Kubeconfig file and click Paste from clipboard to copy the content of the kubeconfig file in the corresponding field. The value of Kubernetes Cluster Name gets auto-populated from the contents of the kubeconfig file . Availability This feature is available starting with PMM 2.30.0. This feature is available only in secure contexts (HTTPS) and some supporting browsers . For a Kubernetes cluster, when using Amazon Elastic Kubernetes Service (EKS) and the kubeconfig file does not contain the AWS access key ID and AWS secret access key. Select the Using Amazon Elastic Kubernetes Service (EKS) checkbox and enter the access key ID and secret access key in the respective fields. For information on obtaining these, see the AWS documentation . Click Register . A message will momentarily display telling you whether the registration was successful or not. Unregister a Kubernetes cluster \u00b6 Important You can\u2019t unregister a Kubernetes cluster if there DB clusters associated with it. Click Unregister . Confirm the action by clicking Proceed , or abandon by clicking Cancel . View a Kubernetes cluster\u2019s configuration \u00b6 Find the row with the Kubernetes cluster you want to see. In the Actions column, open the menu and click Show configuration . Manage allowed component versions \u00b6 Administrators can select allowed and default versions of components versions for each cluster. Find the row with the Kubernetes cluster you want to manage. In the Actions column, open the menu and click Manage versions . Select an Operator and Component from the drop-down menus. Activate or deactivate allowed versions, and select a default in the Default menu. Click Save . Kubernetes operator status \u00b6 The Kubernetes Cluster tab shows the status of operators. Kubernetes operator update \u00b6 When a new version of the operator is available the Operators column shows a message with this information. Click the message to go to the operator release notes to find out more about the update. To update the cluster: Find the row with the operator you want to update. Click the Update button in front of the operator. Confirm the action by clicking Update , or abandon by clicking Cancel . DB clusters \u00b6 Add a DB Cluster \u00b6 You must create at least one Kubernetes cluster to create a DB cluster. To monitor a DB cluster, set up a public address for PMM Server first. Add a one-click DB cluster \u00b6 Availability This feature is available starting with PMM 2.30.0. You can create a DB cluster literally at the click of a button. All the fields will be automatically populated with the default values. To create a DB cluster, do the following: Select the DB Cluster tab. Click Create DB Cluster . Click Create Cluster to create your Cluster. Add a DB cluster with custom values \u00b6 Select the DB Cluster tab. Click Create DB Cluster . In section 1, Basic Options : Enter a value for Cluster name . A cluster name: must begin with a lowercase letter; can comprise lowercase letters, numbers and dashes; must end with an alphanumeric character. Select a cluster from the Kubernetes Cluster menu. Select a database type from the Database Type menu. Expand section 2, Advanced Options . Select Topology , either Cluster or Single Node . Select the number of nodes. (The lower limit is 3.) Select External Access if you want to make your DB cluster available outside of Kubernetes cluster. By default, only internal access is provided. External Access can\u2019t be granted for local Kubernetes clusters (e.g. minikube). Select a preset for Resources per Node . Small , Medium and Large are fixed preset values for Memory , CPU , and Disk . Values for the Custom preset can be edited. Beside each resource type is an estimate of the required and available resources represented numerically in absolute and percentage values, and graphically as a colored, segmented bar showing the projected ratio of used to available resources. A red warning triangle is shown if the requested resources exceed those available. When both Basic Options and Advanced Options section icons are green, the Create Cluster button becomes active. (If inactive, check the values for fields in sections whose icon is red.) Click Create Cluster to create your cluster. A row appears with information on your cluster: Name : The cluster name. Database : The cluster database type and version. Connection : Host : The hostname. Port : The port number. Username : The connection username. Password : The connection password (click the eye icon to reveal). DB Cluster Parameters : K8s cluster name : The Kubernetes cluster name. CPU : The number of CPUs allocated to the cluster. Memory : The amount of memory allocated to the cluster. Disk : The amount of disk space allocated to the cluster. Cluster Status : PENDING : The cluster is being created. ACTIVE : The cluster is active. FAILED : The cluster could not be created. DELETING : The cluster is being deleted. UPDATING : The cluster is being updated. Delete a DB Cluster \u00b6 Find the row with the database cluster you want to delete. In the Actions column, open the menu and click Delete . Confirm the action by clicking Proceed , or abandon by clicking Cancel . Danger Deleting a cluster in this way also deletes any attached volumes. Edit a DB Cluster \u00b6 Select the DB Cluster tab. Find the row with the database cluster you want to change. In the Actions column, open the menu and click Edit . A paused cluster can\u2019t be edited. Restart a DB Cluster \u00b6 Select the DB Cluster tab. Identify the database cluster to be changed. In the Actions column, open the menu and click Restart . Suspend or resume a DB Cluster \u00b6 Select the DB Cluster tab. Identify the DB cluster to suspend or resume. In the Actions column, open the menu and click the required action: For active clusters, click Suspend . For paused clusters, click Resume . Update a DB Cluster \u00b6 Select the DB Cluster tab. Identify the DB cluster to update. In the Actions column, open the menu and click Update : Confirm the update by clicking on Update , or abandon by clicking Cancel . See also Setting up a development environment for DBaaS","title":"DBaaS"},{"location":"using/dbaas.html#dbaas","text":"Caution DBaaS functionality is currently in technical preview and is subject to change. The DBaaS dashboard is where you add, remove, and operate on Kubernetes and database clusters.","title":"DBaaS"},{"location":"using/dbaas.html#activate-dbaas","text":"The DBaaS feature is turned off by default. To turn it on: Go to Configuration \u2192 Settings \u2192 Advanced Settings . Click the toggle in the Technical preview features section of the page.","title":"Activate DBaaS"},{"location":"using/dbaas.html#open-the-dbaas-dashboard","text":"From the left menu, select DBaaS .","title":"Open the DBaaS dashboard"},{"location":"using/dbaas.html#kubernetes-clusters","text":"","title":"Kubernetes clusters"},{"location":"using/dbaas.html#add-a-kubernetes-cluster","text":"Caution Ensure that you set PMM Public Address under Configuration \u2192 Settings \u2192 Advanced Settings before creating a Kubernetes cluster. Otherwise, PMM would not monitor the Kubernetes cluster along with the associated database clusters. PXC and PSMDB operators are installed as part of the Kubernetes cluster registration process. It enables you to deploy database clusters into the Kubernetes cluster. If a public address is set VM Operator is also installed as part of the Kubernetes cluster registration process. It lets you monitor a kubernetes cluster via PMM. Click Register new Kubernetes Cluster . Copy the value of Kubeconfig file and click Paste from clipboard to copy the content of the kubeconfig file in the corresponding field. The value of Kubernetes Cluster Name gets auto-populated from the contents of the kubeconfig file . Availability This feature is available starting with PMM 2.30.0. This feature is available only in secure contexts (HTTPS) and some supporting browsers . For a Kubernetes cluster, when using Amazon Elastic Kubernetes Service (EKS) and the kubeconfig file does not contain the AWS access key ID and AWS secret access key. Select the Using Amazon Elastic Kubernetes Service (EKS) checkbox and enter the access key ID and secret access key in the respective fields. For information on obtaining these, see the AWS documentation . Click Register . A message will momentarily display telling you whether the registration was successful or not.","title":"Add a Kubernetes cluster"},{"location":"using/dbaas.html#unregister-a-kubernetes-cluster","text":"Important You can\u2019t unregister a Kubernetes cluster if there DB clusters associated with it. Click Unregister . Confirm the action by clicking Proceed , or abandon by clicking Cancel .","title":"Unregister a Kubernetes cluster"},{"location":"using/dbaas.html#view-a-kubernetes-clusters-configuration","text":"Find the row with the Kubernetes cluster you want to see. In the Actions column, open the menu and click Show configuration .","title":"View a Kubernetes cluster's configuration"},{"location":"using/dbaas.html#manage-allowed-component-versions","text":"Administrators can select allowed and default versions of components versions for each cluster. Find the row with the Kubernetes cluster you want to manage. In the Actions column, open the menu and click Manage versions . Select an Operator and Component from the drop-down menus. Activate or deactivate allowed versions, and select a default in the Default menu. Click Save .","title":"Manage allowed component versions"},{"location":"using/dbaas.html#kubernetes-operator-status","text":"The Kubernetes Cluster tab shows the status of operators.","title":"Kubernetes operator status"},{"location":"using/dbaas.html#kubernetes-operator-update","text":"When a new version of the operator is available the Operators column shows a message with this information. Click the message to go to the operator release notes to find out more about the update. To update the cluster: Find the row with the operator you want to update. Click the Update button in front of the operator. Confirm the action by clicking Update , or abandon by clicking Cancel .","title":"Kubernetes operator update"},{"location":"using/dbaas.html#db-clusters","text":"","title":"DB clusters"},{"location":"using/dbaas.html#add-a-db-cluster","text":"You must create at least one Kubernetes cluster to create a DB cluster. To monitor a DB cluster, set up a public address for PMM Server first.","title":"Add a DB Cluster"},{"location":"using/dbaas.html#add-a-one-click-db-cluster","text":"Availability This feature is available starting with PMM 2.30.0. You can create a DB cluster literally at the click of a button. All the fields will be automatically populated with the default values. To create a DB cluster, do the following: Select the DB Cluster tab. Click Create DB Cluster . Click Create Cluster to create your Cluster.","title":"Add a one-click DB cluster"},{"location":"using/dbaas.html#add-a-db-cluster-with-custom-values","text":"Select the DB Cluster tab. Click Create DB Cluster . In section 1, Basic Options : Enter a value for Cluster name . A cluster name: must begin with a lowercase letter; can comprise lowercase letters, numbers and dashes; must end with an alphanumeric character. Select a cluster from the Kubernetes Cluster menu. Select a database type from the Database Type menu. Expand section 2, Advanced Options . Select Topology , either Cluster or Single Node . Select the number of nodes. (The lower limit is 3.) Select External Access if you want to make your DB cluster available outside of Kubernetes cluster. By default, only internal access is provided. External Access can\u2019t be granted for local Kubernetes clusters (e.g. minikube). Select a preset for Resources per Node . Small , Medium and Large are fixed preset values for Memory , CPU , and Disk . Values for the Custom preset can be edited. Beside each resource type is an estimate of the required and available resources represented numerically in absolute and percentage values, and graphically as a colored, segmented bar showing the projected ratio of used to available resources. A red warning triangle is shown if the requested resources exceed those available. When both Basic Options and Advanced Options section icons are green, the Create Cluster button becomes active. (If inactive, check the values for fields in sections whose icon is red.) Click Create Cluster to create your cluster. A row appears with information on your cluster: Name : The cluster name. Database : The cluster database type and version. Connection : Host : The hostname. Port : The port number. Username : The connection username. Password : The connection password (click the eye icon to reveal). DB Cluster Parameters : K8s cluster name : The Kubernetes cluster name. CPU : The number of CPUs allocated to the cluster. Memory : The amount of memory allocated to the cluster. Disk : The amount of disk space allocated to the cluster. Cluster Status : PENDING : The cluster is being created. ACTIVE : The cluster is active. FAILED : The cluster could not be created. DELETING : The cluster is being deleted. UPDATING : The cluster is being updated.","title":"Add a DB cluster with custom values"},{"location":"using/dbaas.html#delete-a-db-cluster","text":"Find the row with the database cluster you want to delete. In the Actions column, open the menu and click Delete . Confirm the action by clicking Proceed , or abandon by clicking Cancel . Danger Deleting a cluster in this way also deletes any attached volumes.","title":"Delete a DB Cluster"},{"location":"using/dbaas.html#edit-a-db-cluster","text":"Select the DB Cluster tab. Find the row with the database cluster you want to change. In the Actions column, open the menu and click Edit . A paused cluster can\u2019t be edited.","title":"Edit a DB Cluster"},{"location":"using/dbaas.html#restart-a-db-cluster","text":"Select the DB Cluster tab. Identify the database cluster to be changed. In the Actions column, open the menu and click Restart .","title":"Restart a DB Cluster"},{"location":"using/dbaas.html#suspend-or-resume-a-db-cluster","text":"Select the DB Cluster tab. Identify the DB cluster to suspend or resume. In the Actions column, open the menu and click the required action: For active clusters, click Suspend . For paused clusters, click Resume .","title":"Suspend or resume a DB Cluster"},{"location":"using/dbaas.html#update-a-db-cluster","text":"Select the DB Cluster tab. Identify the DB cluster to update. In the Actions column, open the menu and click Update : Confirm the update by clicking on Update , or abandon by clicking Cancel . See also Setting up a development environment for DBaaS","title":"Update a DB Cluster"},{"location":"using/interface.html","text":"User Interface \u00b6 How to log in, how the user interface is laid out, and what the controls do. PMM\u2019s user interface is a browser application based on Grafana . Logging in \u00b6 Start a web browser and in the address bar enter the server name or IP address of the PMM server host. The page loads showing the PMM login screen. Enter the username and password given to you by your system administrator. The defaults are: Username: admin Password: admin Click Log in . If this is your first time logging in, you\u2019ll be asked to set a new password. (We recommend you do.) Either enter a new password in both fields and click Submit , or, click Skip to use the default password. The PMM Home dashboard loads. Dashboards \u00b6 The interface is a collection of web pages called dashboards . Dashboards are grouped into folders . You can customize these, by renaming them or creating new ones. The area inside dashboards is populated by panels . Some are in collapsible panel groups. A panel can show a value, a graph, a chart, or a visual representation of a set. Controls \u00b6 These menus and controls appear on all dashboards: Main menu (also Grafana menu , side menu ). Navigation bar. View controls. View selectors (with dynamic contents). Shortcut menu (with dynamic contents). (For details see UI Components .) Navigation \u00b6 Search for a dashboard by name \u00b6 There are two ways to open the dashboard search page. (Each takes you to the same search screen.) Click the icon in the main menu. Click the dashboard name in the navigation bar (top row, to the right of the icon). (To search within the current folder, click the folder name instead of the dashboard name.) Click Search dashboards by name and begin typing any part of the dashboard name (in this example, \u201c Instances \u201d). Click one of the search results to go to that dashboard. Change the search text to refine the list. To abandon the search, click the icon at the end of the search bar. Open a dashboard with the menu \u00b6 In the main menu , the PMM Dashboards icon reveals a submenu containing links to all PMM dashboards grouped by service type. (This menu will replace the shortcut menu which has links to commonly-used dashboards.) Panels \u00b6 Charts, graphs and set-based panels reveal extra information when the mouse is moved over them. Some panels have an information icon in the top left corner. Mouse over this to reveal panel information. Panel menu \u00b6 At the top of each panel and to the right of the panel name is the panel menu . Tip The menu is hidden until you mouse over it. Look for the symbol in the title bar of a panel. Item Description View Open the panel in full window mode. Share Share the panel\u2019s link or image . Explore Run PromQL queries. Inspect See the panel\u2019s data or definition. More (Only charts and graphs) Additional options. View \u00b6 The View menu items opens panels in full-window mode. This is useful for graphs with several metrics. Exit a panel\u2019s full window mode by pressing Escape or clicking the left arrow next to the dashboard name. See also How to render dashboard images How to annotate special events Timezones \u00b6 By default Grafana uses the timezone from your web browser. Howewer you can change this setting. Set user timezone \u00b6 On the left menu, hover your cursor over your avatar and then click Preferences . Click to select an option in the Timezone list . Click Save .","title":"User Interface"},{"location":"using/interface.html#user-interface","text":"How to log in, how the user interface is laid out, and what the controls do. PMM\u2019s user interface is a browser application based on Grafana .","title":"User Interface"},{"location":"using/interface.html#logging-in","text":"Start a web browser and in the address bar enter the server name or IP address of the PMM server host. The page loads showing the PMM login screen. Enter the username and password given to you by your system administrator. The defaults are: Username: admin Password: admin Click Log in . If this is your first time logging in, you\u2019ll be asked to set a new password. (We recommend you do.) Either enter a new password in both fields and click Submit , or, click Skip to use the default password. The PMM Home dashboard loads.","title":"Logging in"},{"location":"using/interface.html#dashboards","text":"The interface is a collection of web pages called dashboards . Dashboards are grouped into folders . You can customize these, by renaming them or creating new ones. The area inside dashboards is populated by panels . Some are in collapsible panel groups. A panel can show a value, a graph, a chart, or a visual representation of a set.","title":"Dashboards"},{"location":"using/interface.html#controls","text":"These menus and controls appear on all dashboards: Main menu (also Grafana menu , side menu ). Navigation bar. View controls. View selectors (with dynamic contents). Shortcut menu (with dynamic contents). (For details see UI Components .)","title":"Controls"},{"location":"using/interface.html#navigation","text":"","title":"Navigation"},{"location":"using/interface.html#search-for-a-dashboard-by-name","text":"There are two ways to open the dashboard search page. (Each takes you to the same search screen.) Click the icon in the main menu. Click the dashboard name in the navigation bar (top row, to the right of the icon). (To search within the current folder, click the folder name instead of the dashboard name.) Click Search dashboards by name and begin typing any part of the dashboard name (in this example, \u201c Instances \u201d). Click one of the search results to go to that dashboard. Change the search text to refine the list. To abandon the search, click the icon at the end of the search bar.","title":"Search for a dashboard by name"},{"location":"using/interface.html#open-a-dashboard-with-the-menu","text":"In the main menu , the PMM Dashboards icon reveals a submenu containing links to all PMM dashboards grouped by service type. (This menu will replace the shortcut menu which has links to commonly-used dashboards.)","title":"Open a dashboard with the menu"},{"location":"using/interface.html#panels","text":"Charts, graphs and set-based panels reveal extra information when the mouse is moved over them. Some panels have an information icon in the top left corner. Mouse over this to reveal panel information.","title":"Panels"},{"location":"using/interface.html#panel-menu","text":"At the top of each panel and to the right of the panel name is the panel menu . Tip The menu is hidden until you mouse over it. Look for the symbol in the title bar of a panel. Item Description View Open the panel in full window mode. Share Share the panel\u2019s link or image . Explore Run PromQL queries. Inspect See the panel\u2019s data or definition. More (Only charts and graphs) Additional options.","title":"Panel menu"},{"location":"using/interface.html#view","text":"The View menu items opens panels in full-window mode. This is useful for graphs with several metrics. Exit a panel\u2019s full window mode by pressing Escape or clicking the left arrow next to the dashboard name. See also How to render dashboard images How to annotate special events","title":"View"},{"location":"using/interface.html#timezones","text":"By default Grafana uses the timezone from your web browser. Howewer you can change this setting.","title":"Timezones"},{"location":"using/interface.html#set-user-timezone","text":"On the left menu, hover your cursor over your avatar and then click Preferences . Click to select an option in the Timezone list . Click Save .","title":"Set user timezone"},{"location":"using/query-analytics.html","text":"Query Analytics \u00b6 The Query Analytics dashboard shows how queries are executed and where they spend their time. It helps you analyze database queries over time, optimize database performance, and find and remedy the source of problems. Query Analytics supports MySQL, MongoDB and PostgreSQL. The minimum requirements for MySQL are: MySQL 5.1 or later (if using the slow query log). MySQL 5.6.9 or later (if using Performance Schema). Query Analytics displays metrics in both visual and numeric form. Performance-related characteristics appear as plotted graphics with summaries. The dashboard contains three panels: the Filters Panel ; the Overview Panel ; the Details Panel . Query Analytics data retrieval is not instantaneous and can be delayed due to network conditions. In such situations no data is reported and a gap appears in the sparkline. Filters Panel \u00b6 The Filter panel on the left hand side of the dashboard lists the filters grouped by category. It also shows the percentage of the main metrics (explained below). If you select a different metric, the percentages on the left panel will change as per this metric. When you select a metric, it reduces the overview list as per the matching filter. The first five of each category are shown. If there are more, the list is expanded by clicking Show all beside the category name, and collapsed again with Show top 5 . Applying a filter may make other filters inapplicable. These become grayed out and inactive. Click the chart symbol to navigate directly to an item\u2019s associated dashboard. Separately, the global Time range setting filters results by time, either your choice of Absolute time range , or one of the predefined Relative time ranges . Overview Panel \u00b6 To the right of the Filters panel and occupying the upper part of the dashboard is the Overview panel. Each row of the table represents the metrics for a chosen object type, one of: Query; Service Name; Database; Schema; User Name; Client Host. At the top of the second column is the dimension menu. Use this to choose the object type. On the right side of the dimension column is the Dimension Search bar. Enter a string and press Enter to limit the view to queries containing only the specified keywords. Delete the search text and press Enter to see the full list again. Columns \u00b6 The first column is the object\u2019s identifier. For Query , it is the query\u2019s Fingerprint . The second column is the Main metric , containing a reduced graphical representation of the metric over time, called a sparkline , and a horizontal meter, filled to reflect a percentage of the total value. Additional values are revealed as mouse-over tool-tips. Tool-tips \u00b6 For the Query dimension, hovering over the information icon reveals the query ID and its example. Hovering on a column header reveals an informative tool-tip for that column. Hovering on the main metric sparkline highlights the data point and a tooltip shows the data value under the cursor. Hovering on the main metric meter reveals the percentage of the total, and other details specific to the main metric. Hovering on column values reveals more details on the value. The contents depends on the type of value. Adding and removing columns \u00b6 Metrics columns are added with the Add column button. When clicked, a text field and list of available metrics are revealed. Select a metric or enter a search string to reduce the list. Selecting a metric adds it to the panel. A metric column is removed by clicking on the column heading and selecting Remove column . The value plotted in the main metric column can be changed by clicking a metric column heading and selecting Swap with main metric . Sorting \u00b6 The entire list is sorted by one of the columns. Click either the up or down caret to sort the list by that column\u2019s ascending or descending values. Pagination \u00b6 The pagination device lets you move forwards or backwards through pages, jump to a specific page, and choose how many items are listed per page. Queries are grouped into pages of 25, 50 or 100 items. Details Panel \u00b6 Selecting an item in the Overview panel opens the Details panel with a Details Tab . If the dimension is Query , the panel also contains the Examples Tab , Explain Tab , and Tables Tab . Details Tab \u00b6 The Details tab contains a Query time distribution bar (only for MySQL databases) and a set of Metrics in collapsible subpanels. The Query time distribution bar shows a query\u2019s total time made up of colored segments, each segment representing the proportion of time spent on a named activity. query_time : Statement execution time. lock_time : Time to acquire locks. blk_read_time : Total time the statement spent reading blocks (if track_io_timing is enabled, otherwise zero). blk_write_time : Total time the statement spent writing blocks (if track_io_timing is enabled, otherwise zero). innodb_io_r_wait : Time for InnoDB to read the data from storage. innodb_queue_wait : Time the query spent either waiting to enter the InnoDB queue, or in it pending execution. innodb_rec_lock_wait : Time the query waited for row locks. other : Remaining uncategorized query time. Metrics is a table with headings: Metric : The Metric name, with a question-mark tool-tip that reveals a description of the metric on mouse-over; Rate/Second : A sparkline chart of real-time values per unit time; Sum : A summation of the metric for the selected query, and the percentage of the total; Per Query Stats : The value of the metric per query. Each row in the table is a metric. The contents depends on the chosen dimension. For PostgreSQL queries (when using pg_stat_monitor ) the top query will also be shown in the details section if the query was called by an outer query. Other usefull metrics (when using pg_stat_monitor ) to monitor PostgreSQL Server performance are Histograms . Histograms provide more explicit information about number of queries for fingerprint (queryid). Ranges are from 0 seconds up to 100 seconds. Ranges (numbers are in miliseconds): 0 - 3 3 - 10 10 - 31 31 - 100 100 - 316 316 - 1000 1000 - 3162 3162 - 10000 10000 - 31622 31622 - 100000 Here is picture of histogram in graph: Examples Tab \u00b6 (For Query dimension.) The Examples tab shows an example of the selected query\u2019s fingerprint or table element. Query example and fingerprint can be truncated to 1024 long to reduce space usage. In this case, the query explains section will not work. Explain Tab \u00b6 (For Query dimension.) The Explain tab shows the explain output for the selected query, in Classic or JSON formats. MySQL: Classic and JSON. MongoDB: JSON only. PostgreSQL: Not supported. \u2018Explain\u2019 for MongoDB To run Explain you need the same permissions as for executing the original query. For example, to run explain on updates you need update permissions. Example: Grant the explainRole with update permissions. db.grantPrivilegesToRole( \"explainRole\", [ { resource: { db: \"\", collection: \"\" }, actions: [ \"update\" ] } ]) Tables Tab \u00b6 (For Query dimension.) The Tables tab shows information on the tables and indexes involved in the selected query. Plan Tab \u00b6 (For Query dimension.) The Plan tab shows the plan for PostgreSQL queries (only available when using pg_stat_monitor ). Query Analytics for MongoDB \u00b6 MongoDB is conceptually different from relational database management systems, such as MySQL and MariaDB. Relational database management systems store data in tables that represent single entities. Complex objects are represented by linking tables. In contrast, MongoDB uses the concept of a document where all essential information for a complex object is stored in one place. Query Analytics can monitor MongoDB queries. Although MongoDB is not a relational database management system, you analyze its databases and collections in the same interface using the same tools. Sharing a link for Query Analytics \u00b6 To share a link for Query Analytics, use Copy Link . It copies the link to the clipboard with all the relevant information such as selected query, table page, selected filters, details tab, and time range. Thus, when you open the link, it will display the exact information. Ensure that you use Copy Link to copy the link instead of using the browser address bar or the standard Grafana functionality (to share a dashboard). Otherwise, Query Analytics might not display the exact information that existed while sharing the link. By default, Grafana uses a relative time range and not an absolute range, so it will have a different timestamp when this link is opened.","title":"Query Analytics"},{"location":"using/query-analytics.html#query-analytics","text":"The Query Analytics dashboard shows how queries are executed and where they spend their time. It helps you analyze database queries over time, optimize database performance, and find and remedy the source of problems. Query Analytics supports MySQL, MongoDB and PostgreSQL. The minimum requirements for MySQL are: MySQL 5.1 or later (if using the slow query log). MySQL 5.6.9 or later (if using Performance Schema). Query Analytics displays metrics in both visual and numeric form. Performance-related characteristics appear as plotted graphics with summaries. The dashboard contains three panels: the Filters Panel ; the Overview Panel ; the Details Panel . Query Analytics data retrieval is not instantaneous and can be delayed due to network conditions. In such situations no data is reported and a gap appears in the sparkline.","title":"Query Analytics"},{"location":"using/query-analytics.html#filters-panel","text":"The Filter panel on the left hand side of the dashboard lists the filters grouped by category. It also shows the percentage of the main metrics (explained below). If you select a different metric, the percentages on the left panel will change as per this metric. When you select a metric, it reduces the overview list as per the matching filter. The first five of each category are shown. If there are more, the list is expanded by clicking Show all beside the category name, and collapsed again with Show top 5 . Applying a filter may make other filters inapplicable. These become grayed out and inactive. Click the chart symbol to navigate directly to an item\u2019s associated dashboard. Separately, the global Time range setting filters results by time, either your choice of Absolute time range , or one of the predefined Relative time ranges .","title":"Filters Panel"},{"location":"using/query-analytics.html#overview-panel","text":"To the right of the Filters panel and occupying the upper part of the dashboard is the Overview panel. Each row of the table represents the metrics for a chosen object type, one of: Query; Service Name; Database; Schema; User Name; Client Host. At the top of the second column is the dimension menu. Use this to choose the object type. On the right side of the dimension column is the Dimension Search bar. Enter a string and press Enter to limit the view to queries containing only the specified keywords. Delete the search text and press Enter to see the full list again.","title":"Overview Panel"},{"location":"using/query-analytics.html#columns","text":"The first column is the object\u2019s identifier. For Query , it is the query\u2019s Fingerprint . The second column is the Main metric , containing a reduced graphical representation of the metric over time, called a sparkline , and a horizontal meter, filled to reflect a percentage of the total value. Additional values are revealed as mouse-over tool-tips.","title":"Columns"},{"location":"using/query-analytics.html#tool-tips","text":"For the Query dimension, hovering over the information icon reveals the query ID and its example. Hovering on a column header reveals an informative tool-tip for that column. Hovering on the main metric sparkline highlights the data point and a tooltip shows the data value under the cursor. Hovering on the main metric meter reveals the percentage of the total, and other details specific to the main metric. Hovering on column values reveals more details on the value. The contents depends on the type of value.","title":"Tool-tips"},{"location":"using/query-analytics.html#adding-and-removing-columns","text":"Metrics columns are added with the Add column button. When clicked, a text field and list of available metrics are revealed. Select a metric or enter a search string to reduce the list. Selecting a metric adds it to the panel. A metric column is removed by clicking on the column heading and selecting Remove column . The value plotted in the main metric column can be changed by clicking a metric column heading and selecting Swap with main metric .","title":"Adding and removing columns"},{"location":"using/query-analytics.html#sorting","text":"The entire list is sorted by one of the columns. Click either the up or down caret to sort the list by that column\u2019s ascending or descending values.","title":"Sorting"},{"location":"using/query-analytics.html#pagination","text":"The pagination device lets you move forwards or backwards through pages, jump to a specific page, and choose how many items are listed per page. Queries are grouped into pages of 25, 50 or 100 items.","title":"Pagination"},{"location":"using/query-analytics.html#details-panel","text":"Selecting an item in the Overview panel opens the Details panel with a Details Tab . If the dimension is Query , the panel also contains the Examples Tab , Explain Tab , and Tables Tab .","title":"Details Panel"},{"location":"using/query-analytics.html#details-tab","text":"The Details tab contains a Query time distribution bar (only for MySQL databases) and a set of Metrics in collapsible subpanels. The Query time distribution bar shows a query\u2019s total time made up of colored segments, each segment representing the proportion of time spent on a named activity. query_time : Statement execution time. lock_time : Time to acquire locks. blk_read_time : Total time the statement spent reading blocks (if track_io_timing is enabled, otherwise zero). blk_write_time : Total time the statement spent writing blocks (if track_io_timing is enabled, otherwise zero). innodb_io_r_wait : Time for InnoDB to read the data from storage. innodb_queue_wait : Time the query spent either waiting to enter the InnoDB queue, or in it pending execution. innodb_rec_lock_wait : Time the query waited for row locks. other : Remaining uncategorized query time. Metrics is a table with headings: Metric : The Metric name, with a question-mark tool-tip that reveals a description of the metric on mouse-over; Rate/Second : A sparkline chart of real-time values per unit time; Sum : A summation of the metric for the selected query, and the percentage of the total; Per Query Stats : The value of the metric per query. Each row in the table is a metric. The contents depends on the chosen dimension. For PostgreSQL queries (when using pg_stat_monitor ) the top query will also be shown in the details section if the query was called by an outer query. Other usefull metrics (when using pg_stat_monitor ) to monitor PostgreSQL Server performance are Histograms . Histograms provide more explicit information about number of queries for fingerprint (queryid). Ranges are from 0 seconds up to 100 seconds. Ranges (numbers are in miliseconds): 0 - 3 3 - 10 10 - 31 31 - 100 100 - 316 316 - 1000 1000 - 3162 3162 - 10000 10000 - 31622 31622 - 100000 Here is picture of histogram in graph:","title":"Details Tab"},{"location":"using/query-analytics.html#examples-tab","text":"(For Query dimension.) The Examples tab shows an example of the selected query\u2019s fingerprint or table element. Query example and fingerprint can be truncated to 1024 long to reduce space usage. In this case, the query explains section will not work.","title":"Examples Tab"},{"location":"using/query-analytics.html#explain-tab","text":"(For Query dimension.) The Explain tab shows the explain output for the selected query, in Classic or JSON formats. MySQL: Classic and JSON. MongoDB: JSON only. PostgreSQL: Not supported. \u2018Explain\u2019 for MongoDB To run Explain you need the same permissions as for executing the original query. For example, to run explain on updates you need update permissions. Example: Grant the explainRole with update permissions. db.grantPrivilegesToRole( \"explainRole\", [ { resource: { db: \"\", collection: \"\" }, actions: [ \"update\" ] } ])","title":"Explain Tab"},{"location":"using/query-analytics.html#tables-tab","text":"(For Query dimension.) The Tables tab shows information on the tables and indexes involved in the selected query.","title":"Tables Tab"},{"location":"using/query-analytics.html#plan-tab","text":"(For Query dimension.) The Plan tab shows the plan for PostgreSQL queries (only available when using pg_stat_monitor ).","title":"Plan Tab"},{"location":"using/query-analytics.html#query-analytics-for-mongodb","text":"MongoDB is conceptually different from relational database management systems, such as MySQL and MariaDB. Relational database management systems store data in tables that represent single entities. Complex objects are represented by linking tables. In contrast, MongoDB uses the concept of a document where all essential information for a complex object is stored in one place. Query Analytics can monitor MongoDB queries. Although MongoDB is not a relational database management system, you analyze its databases and collections in the same interface using the same tools.","title":"Query Analytics for MongoDB"},{"location":"using/query-analytics.html#sharing-a-link-for-query-analytics","text":"To share a link for Query Analytics, use Copy Link . It copies the link to the clipboard with all the relevant information such as selected query, table page, selected filters, details tab, and time range. Thus, when you open the link, it will display the exact information. Ensure that you use Copy Link to copy the link instead of using the browser address bar or the standard Grafana functionality (to share a dashboard). Otherwise, Query Analytics might not display the exact information that existed while sharing the link. By default, Grafana uses a relative time range and not an absolute range, so it will have a different timestamp when this link is opened.","title":"Sharing a link for Query Analytics"}]}